#! /usr/bin/env bash
# Patch: -pro_arm_davinci_dm355_dma
# Date: Mon Jan 21 23:06:09 2008
# Source: MontaVista Software, Inc.
# MR:    25657
# Type:  Enhancement 
# Disposition: local
# Signed-off-by: Jerry Alexander <jalexander@mvista.com>
# Description:
# 
# 	Consolidated patch for DM355 and DM644x DMA
# 	This is a new DMA driver provided by TI and
# 	augmented with DM355 and DM644x specific params
# 	and array values.
# 

PATCHNUM=1678
LSPINFO=include/linux/lsppatchlevel.h
TMPFILE=/tmp/mvl_patch_$$

function dopatch() {
    patch $* >${TMPFILE} 2>&1 <<"EOF"
Source: MontaVista Software, Inc.
MR:    25657
Type:  Enhancement 
Disposition: local
Signed-off-by: Jerry Alexander <jalexander@mvista.com>
Description:

	Consolidated patch for DM355 and DM644x DMA
	This is a new DMA driver provided by TI and
	augmented with DM355 and DM644x specific params
	and array values.

Index: linux-2.6.10/arch/arm/mach-davinci/dma.c
===================================================================
--- linux-2.6.10.orig/arch/arm/mach-davinci/dma.c
+++ linux-2.6.10/arch/arm/mach-davinci/dma.c
@@ -3,7 +3,7 @@
  *
  * TI DaVinci DMA file
  *
- * Copyright (C) 2006 Texas Instruments.
+ * Copyright (C) 2006 Texas Instruments
  *
  * ----------------------------------------------------------------------------
  *
@@ -23,678 +23,1348 @@
  * ----------------------------------------------------------------------------
  *
  */
+
 #include <linux/sched.h>
 #include <linux/init.h>
 #include <linux/module.h>
 #include <linux/interrupt.h>
-#include <linux/device.h>
 #include <linux/spinlock.h>
+#include <linux/kernel.h>
+
 #include <asm/io.h>
+
 #include <asm/arch/memory.h>
-#include <linux/kernel.h>
 #include <asm/arch/hardware.h>
 #include <asm/arch/irqs.h>
-
 #include <asm/arch/edma.h>
+#include <asm/arch/cpu.h>
 
-static spinlock_t dma_chan_lock;
-static struct device_driver edma_driver;
-static struct platform_device edma_dev;
+#ifdef DEBUG
+#define DMA_PRINTK(ARGS...)  printk(KERN_INFO "<%s><%d>: ",__FUNCTION__,__LINE__);printk(ARGS)
+#define DMA_FN_IN printk(KERN_INFO "[%s]: start\n", __FUNCTION__)
+#define DMA_FN_OUT printk(KERN_INFO "[%s]: end\n",__FUNCTION__)
+#else
+#define DMA_PRINTK( x... )
+#define DMA_FN_IN
+#define DMA_FN_OUT
+#endif
+
+
+struct edma_map {
+	int param1;
+	int param2;
+};
 
-#define LOCK_INIT     spin_lock_init(&dma_chan_lock)
-#define LOCK          spin_lock(&dma_chan_lock)
-#define UNLOCK        spin_unlock(&dma_chan_lock)
+static unsigned int *edma_channels_arm;
+static unsigned char *qdma_channels_arm;
+static unsigned int *param_entry_arm;
+static unsigned int *tcc_arm;
+static unsigned int *param_entry_reserved;
+
+const unsigned int davinci_qdma_ch_map[] = {
+	EDMA_DM644X_NUM_PARAMENTRY,
+	EDMA_DM646X_NUM_PARAMENTRY,
+	EDMA_DM355_NUM_PARAMENTRY,
+};
 
-typedef void (*intr_callback) (void);
-static int register_dma_interrupts(intr_callback, intr_callback, intr_callback,
-				   intr_callback);
+/* SoC specific EDMA3 hardware information, should be provided for a new SoC */
+/* DaVinci specific EDMA3 information */
+/*
+  Each bit field of the elements below indicate the corresponding DMA channel
+  availability on EDMA_MASTER_SHADOW_REGION side events
+*/
+static unsigned int dm644x_edma_channels_arm[EDMA_NUM_DMA_CHAN_DWRDS] = {
+	0xFFFFFFFFu,  0xFFFFFFFFu
+};
 
-#define DAVINCI_DMA_REGISTER_BASE DAVINCI_DMA_3PCC_BASE
+/*
+  Each bit field of the elements below indicate the corresponding QDMA channel
+  availability on EDMA_MASTER_SHADOW_REGION side events
+*/
+static unsigned char dm644x_qdma_channels_arm[EDMA_NUM_QDMA_CHAN_DWRDS] = {
+	0x00000010u
+};
 
-static edmacc_regs *get_edma_base(void)
-{
-	return ((edmacc_regs *) IO_ADDRESS(DAVINCI_DMA_REGISTER_BASE));
-}
+/*
+   Each bit field of the elements below indicate corresponding PARAM entry
+   availability on EDMA_MASTER_SHADOW_REGION side events
+*/
+static unsigned int dm644x_param_entry_arm[] = {
+	0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu
+};
+
+/*
+   Each bit field of the elements below indicate corresponding TCC
+   availability on EDMA_MASTER_SHADOW_REGION side events
+*/
+static unsigned int dm644x_tcc_arm[EDMA_NUM_DMA_CHAN_DWRDS] = {
+	0xFFFFFFFFu, 0xFFFFFFFFu
+};
+
+/*
+   Each bit field of the elements below indicate whether the corresponding
+    PARAM entry is available for ANY DMA channel or not.
+    1- reserved, 0 - not
+    (First 64 PaRAM Sets are reserved for 64 DMA Channels)
+*/
+static unsigned int dm644x_param_entry_reserved[] = {
+	0xFFFFFFFFu, 0xFFFFFFFFu, 0x0u, 0x0u
+};
+
+static struct edma_map dm644x_queue_priority_mapping[EDMA_DM644X_NUM_EVQUE] = {
+	/* {Event Queue No, Priority} */
+	{0, 0},
+	{1, 1}
+};
+
+static struct edma_map dm644x_queue_watermark_level[EDMA_DM644X_NUM_EVQUE] = {
+	/* {Event Queue No, Watermark Level} */
+	{0, 16},
+	{1, 16}
+};
+
+static struct edma_map dm644x_queue_tc_mapping[EDMA_DM644X_NUM_EVQUE] = {
+	/* {Event Queue No, TC no} */
+	{0, 0},
+	{1, 1}
+};
 
-static intr_callback cb[4];
+/* DaVinci-HD specific EDMA3 information */
+/*
+  Each bit field of the elements below indicate the corresponding DMA channel
+  availability on EDMA_MASTER_SHADOW_REGION side events
+*/
+static unsigned int dm646x_edma_channels_arm[EDMA_NUM_DMA_CHAN_DWRDS] = {
+	0x30FF1FF0u,  0x00C007FFu
+};
 
-/* Structure containing the dma channel parameters */
-static struct davinci_dma_lch {
-	int dev_id;
-	int in_use;		/* 1-used 0-unused */
-	int link_lch;
-	int dma_running;
-	int param_no;
-	int tcc;
-} dma_chan[DAVINCI_EDMA_NUM_PARAMENTRY];
+/*
+  Each bit field of the elements below indicate the corresponding QDMA channel
+  availability on EDMA_MASTER_SHADOW_REGION side events
+*/
+static unsigned char dm646x_qdma_channels_arm[EDMA_NUM_QDMA_CHAN_DWRDS] = {
+	0x00000080
+};
+
+/*
+   Each bit field of the elements below indicate corresponding PARAM entry
+   availability on EDMA_MASTER_SHADOW_REGION side events
+*/
+static unsigned int dm646x_param_entry_arm[] = {
+	0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu,
+	0x0u, 0x0u, 0x0u, 0x0u,
+	0x0u, 0x0u, 0x0u, 0x0u,
+	0x0u, 0x0u, 0x0u, 0x0u
+};
+
+/*
+   Each bit field of the elements below indicate corresponding TCC
+   availability on EDMA_MASTER_SHADOW_REGION side events
+*/
+static unsigned int dm646x_tcc_arm[EDMA_NUM_DMA_CHAN_DWRDS] = {
+	0x30FF1FF0u, 0x00C007FFu
+};
+
+/*
+   Each bit field of the elements below indicate whether the corresponding
+    PARAM entry is available for ANY DMA channel or not.
+    1- reserved, 0 - not
+    (First 64 PaRAM Sets are reserved for 64 DMA Channels)
+*/
+static unsigned int dm646x_param_entry_reserved[] = {
+	0xFFFFFFFFu, 0xFFFFFFFFu, 0x0u, 0x0u,
+	0x0u, 0x0u, 0x0u, 0x0u,
+	0x0u, 0x0u, 0x0u, 0x0u,
+	0x0u, 0x0u, 0x0u, 0x0u
+};
 
+static struct edma_map dm646x_queue_priority_mapping[EDMA_DM646X_NUM_EVQUE] = {
+	/* {Event Queue No, Priority} */
+	{0, 0},
+	{1, 1},
+	{2, 2},
+	{3, 3}
+};
+
+static struct edma_map dm646x_queue_watermark_level[EDMA_DM646X_NUM_EVQUE] = {
+	/* {Event Queue No, Watermark Level} */
+	{0, 16},
+	{1, 16},
+	{2, 16},
+	{3, 16}
+};
+
+static struct edma_map dm646x_queue_tc_mapping[EDMA_DM646X_NUM_EVQUE] = {
+	/* {Event Queue No, TC no} */
+	{0, 0},
+	{1, 1},
+	{2, 2},
+	{3, 3},
+};
+
+
+/* DM355 specific EDMA3 information */
+/*
+  Each bit field of the elements below indicate the corresponding DMA channel
+  availability on EDMA_MASTER_SHADOW_REGION side events
+*/
+static unsigned int dm355_edma_channels_arm[] = {
+	0xFFFFFFFFu,
+	0x00000000u,
+};
+
+/*
+  Each bit field of the elements below indicate the corresponding QDMA channel
+  availability on EDMA_MASTER_SHADOW_REGION side events
+*/
+static unsigned char dm355_qdma_channels_arm[EDMA_NUM_QDMA_CHAN_DWRDS] = {
+	0x000000FFu
+};
+
+/*
+   Each bit field of the elements below indicate corresponding PARAM entry
+   availability on EDMA_MASTER_SHADOW_REGION side events
+*/
+static unsigned int dm355_param_entry_arm[] = {
+	0xFFFFFFFFu, 0x00000000u, 0x00000000u, 0xFFFFFFC0u,
+	0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu,
+	0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu,
+	0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu,
+};
+
+/*
+   Each bit field of the elements below indicate corresponding TCC
+   availability on EDMA_MASTER_SHADOW_REGION side events
+*/
+static unsigned int dm355_tcc_arm[EDMA_NUM_DMA_CHAN_DWRDS] = {
+	0xFFFFFFFFu, 0xFFFFFFFFu
+};
+
+/*
+   Each bit field of the elements below indicate whether the corresponding
+    PARAM entry is available for ANY DMA channel or not.
+    1- reserved, 0 - not
+    (First 64 PaRAM Sets are reserved for 64 DMA Channels)
+*/
+static unsigned int dm355_param_entry_reserved[] = {
+	0xFFFFFFFFu, 0xFFFFFFFFu, 0x0u, 0x0u
+};
+
+static struct edma_map dm355_queue_priority_mapping[] = {
+	/* {Event Queue No, Priority} */
+	{0, 0},
+	{1, 1},
+	{2, 1},
+	{3, 1},
+	{4, 1},
+	{5, 1},
+	{6, 1},
+	{7, 1},
+};
+
+static struct edma_map dm355_queue_watermark_level[] = {
+	/* {Event Queue No, Watermark Level} */
+	{0, 16},
+	{1, 16},
+	{2, 16},
+	{3, 16},
+	{4, 16},
+	{5, 16},
+	{6, 16},
+	{7, 16},
+};
+
+static struct edma_map dm355_queue_tc_mapping[] = {
+	/* {Event Queue No, TC no} */
+	{0, 0},
+	{1, 1},
+	{2, 2},
+	{3, 3},
+	{4, 4},
+	{5, 5},
+	{6, 6},
+	{7, 7},
+};
+
+static spinlock_t dma_chan_lock;
+
+/**************************************************************************\
+* Edma Driver Internal Data Structures
+\**************************************************************************/
+
+/*
+ * Array to maintain the Callback details registered
+ * against a particular TCC. Used to call the callback
+ * functions linked to the particular channel.
+ */
 static struct dma_interrupt_data {
 	void (*callback) (int lch, unsigned short ch_status, void *data);
 	void *data;
-} intr_data[64];
+} dma_interrupt_param[EDMA_NUM_TCC];
+
 
 /*
-  Each bit field of the elements bellow indicate the corresponding EDMA channel
-  availability  on arm side events
-*/
-static unsigned long edma_channels_arm[] = {
-	0xffffffff,
-	0xffffffff
+ * Resources bound to a Logical Channel (DMA/QDMA/LINK)
+ *
+ * When a request for a channel is made, the resources PaRAM Set and TCC
+ * get bound to that channel. This information is needed internally by the
+ * driver when a request is made to free the channel (Since it is the
+ * responsibility of the driver to free up the channel-associated resources
+ * from the Resource Manager layer).
+ */
+struct edma3_ch_bound_res {
+	/** PaRAM Set number associated with the particular channel */
+	unsigned int param_id;
+	/** TCC associated with the particular channel */
+	unsigned int tcc;
 };
 
+static struct edma3_ch_bound_res *dma_ch_bound_res;
+static int edma_max_logical_ch;
+static unsigned int davinci_edma_num_evtq;
+static unsigned int davinci_edma_chmap_exist;
+static unsigned int davinci_edma_num_tc;
+static unsigned int davinci_edma_num_param;
+static unsigned int *davinci_edmatc_base_addrs;
+static unsigned int *davinci_dma_ch_hw_event_map;
+
 /*
-  Each bit field of the elements bellow indicate the corresponding QDMA channel
-  availability  on arm side events
+* Mapping of DMA channels to Hardware Events from
+* various peripherals, which use EDMA for data transfer.
+* All channels need not be mapped, some can be free also.
 */
-static unsigned char qdma_channels_arm[] = {
-	0x00
+static unsigned int dm644x_dma_ch_hw_event_map[EDMA_NUM_DMA_CHAN_DWRDS] = {
+	EDMA_DM644X_CHANNEL_TO_EVENT_MAPPING_0,
+	EDMA_DM644X_CHANNEL_TO_EVENT_MAPPING_1
+};
+
+static unsigned int dm355_dma_ch_hw_event_map[EDMA_NUM_DMA_CHAN_DWRDS] = {
+	EDMA_DM355_CHANNEL_TO_EVENT_MAPPING_0,
+	EDMA_DM355_CHANNEL_TO_EVENT_MAPPING_1
+};
+
+static unsigned int dm646x_dma_ch_hw_event_map[EDMA_NUM_DMA_CHAN_DWRDS] = {
+	EDMA_DM646X_CHANNEL_TO_EVENT_MAPPING_0,
+	EDMA_DM646X_CHANNEL_TO_EVENT_MAPPING_1
 };
 
 /*
-   Each bit field of the elements bellow indicate corresponding PARAM entry
-   availibility on arm side events
+   Each bit field of the elements below indicate whether a DMA Channel
+   is free or in use
+   1 - free
+   0 - in use
 */
-static unsigned long param_entry_arm[] = {
-	0xffffffff, 0xffffffff, 0x0000ffff, 0xffffffff,
-	0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff,
-	0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff,
-	0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff
+static unsigned int dma_ch_use_status[EDMA_NUM_DMA_CHAN_DWRDS] = {
+	0xFFFFFFFFu,
+	0xFFFFFFFFu
 };
 
 /*
-   Each bit field of the elements bellow indicate whether a PARAM entry
+   Each bit field of the elements below indicate whether a intrerrupt
    is free or in use
    1 - free
    0 - in use
 */
-static unsigned long param_entry_use_status[] = {
-	0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff,
-	0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff,
-	0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff,
-	0xffffffff
+static unsigned char qdma_ch_use_status[EDMA_NUM_QDMA_CHAN_DWRDS] = {
+	0xFFu
 };
 
 /*
-   Each bit field of the elements bellow indicate whether a intrerrupt
+   Each bit field of the elements below indicate whether a PARAM entry
    is free or in use
    1 - free
    0 - in use
 */
-static unsigned long dma_intr_use_status[] = {
-	0xffffffff,
-	0xffffffff
+static unsigned int param_entry_use_status[EDMA_MAX_PARAM_SET/32u] = {
+	0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu,
+	0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu,
+	0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu,
+	0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu
 };
 
 /*
-    This lists the DMA channel numbers which does not have any events
-    associated with it
+   Each bit field of the elements below indicate whether a intrerrupt
+   is free or in use
+   1 - free
+   0 - in use
 */
-static int dma_chan_no_event[] = {
-	0, 1, 12, 13, 14, 15, 25, 30, 31, 45, 46, 47, 55, 56, 57, 58, 59, 60,
-	61, 62, 63, -1
+static unsigned long tcc_use_status[EDMA_NUM_DMA_CHAN_DWRDS] = {
+	0xFFFFFFFFu,
+	0xFFFFFFFFu
 };
 
-static int channel_queue_mapping[][2] = {
-/* {channel no, event queue no } */
-	{0, 0}, {1, 1}, {2, 0}, {3, 1}, {4, 0}, {5, 1}, {6, 0}, {7, 1},
-	{8, 0}, {9, 1}, {10, 0}, {11, 1}, {12, 0}, {13, 1}, {14, 0},
-	{15, 1}, {16, 0}, {17, 1}, {18, 0}, {19, 1}, {20, 0}, {21, 1},
-	{22, 0}, {23, 1}, {24, 0}, {25, 1}, {26, 0}, {27, 1}, {28, 0},
-	{29, 1}, {30, 0}, {31, 1}, {32, 0}, {33, 1}, {34, 0}, {35, 1},
-	{36, 0}, {37, 1}, {38, 0}, {39, 1}, {40, 0}, {41, 1}, {42, 0},
-	{43, 1}, {44, 0}, {45, 1}, {46, 0}, {47, 1}, {48, 0}, {49, 1},
-	{50, 0}, {51, 1}, {52, 0}, {53, 1}, {54, 0}, {55, 1}, {56, 0},
-	{57, 1}, {58, 0}, {59, 1}, {60, 0}, {61, 1}, {62, 0}, {63, 1},
-	{64, 0}, {65, 1}, {66, 0}, {67, 1}, {68, 0}, {69, 1}, {70, 0},
-	{71, 1}, {-1, -1}
+
+
+/*
+   Global Array to store the mapping between DMA channels and Interrupt
+   channels i.e. TCCs.
+   DMA channel X can use any TCC Y. Transfer completion
+   interrupt will occur on the TCC Y (IPR/IPRH Register, bit Y), but error
+   interrupt will occur on DMA channel X (EMR/EMRH register, bit X). In that
+   scenario, this DMA channel <-> TCC mapping will be used to point to
+   the correct callback function.
+*/
+static unsigned int edma_dma_ch_tcc_mapping [EDMA_NUM_DMACH];
+
+
+/**
+    Global Array to store the mapping between QDMA channels and Interrupt
+    channels i.e. TCCs.
+    QDMA channel X can use any TCC Y. Transfer completion
+    interrupt will occur on the TCC Y (IPR/IPRH Register, bit Y), but error
+    interrupt will occur on QDMA channel X (QEMR register, bit X). In that
+    scenario, this QDMA channel <-> TCC mapping will be used to point to
+    the correct callback function.
+*/
+static unsigned int edma_qdma_ch_tcc_mapping [EDMA_NUM_QDMACH];
+
+
+/**
+ * The list of Interrupt Channels which get allocated while requesting the
+ * TCC. It will be used while checking the IPR/IPRH bits in the RM ISR.
+ */
+static unsigned int allocated_tccs[2u] = {0u, 0u};
+
+
+/* Pointer to CC Registers */
+volatile edmacc_regs *ptr_edmacc_regs = NULL;
+
+/* Array containing physical addresses of all the TCs present */
+u32 dm644x_edmatc_base_addrs[EDMA_MAX_TC] = {
+	(u32)DAVINCI_DMA_3PTC0_BASE,
+	(u32)DAVINCI_DMA_3PTC1_BASE,
+	(u32)NULL,
+	(u32)NULL,
+	(u32)NULL,
+	(u32)NULL,
+	(u32)NULL,
+	(u32)NULL,
+};
+u32 dm646x_edmatc_base_addrs[EDMA_MAX_TC] = {
+	(u32)DAVINCI_DMA_3PTC0_BASE,
+	(u32)DAVINCI_DMA_3PTC1_BASE,
+	(u32)DAVINCI_DM646X_DMA_3PTC2_BASE,
+	(u32)DAVINCI_DM646X_DMA_3PTC3_BASE,
+	(u32)NULL,
+	(u32)NULL,
+	(u32)NULL,
+	(u32)NULL,
+};
+u32 dm355_edmatc_base_addrs[EDMA_MAX_TC] = {
+	(u32)DAVINCI_DMA_3PTC0_BASE,
+	(u32)DAVINCI_DMA_3PTC1_BASE,
+	(u32)NULL,
+	(u32)NULL,
+	(u32)NULL,
+	(u32)NULL,
+	(u32)NULL,
+	(u32)NULL,
 };
 
-static int queue_tc_mapping[DAVINCI_EDMA_NUM_EVQUE + 1][2] = {
-/* {event queue no, TC no} */
-	{0, 0},
-	{1, 1},
-	{-1, -1}
+/* Array containing the virtual addresses of all the TCs present */
+volatile edmatc_regs *ptr_edmatc_regs[EDMA_MAX_TC] = {NULL};
+
+/* Pointer to CC Shadow Region Specific Registers */
+volatile edmacc_shadow_regs *ptr_edmacc_shadow_regs = NULL;
+
+/**
+ * Variable which will be used internally for referring transfer controllers'
+ * error interrupts.
+ */
+unsigned int dm644x_tc_error_int[EDMA_MAX_TC] = {
+	IRQ_TCERRINT0, IRQ_TCERRINT,
+	0, 0, 0, 0, 0, 0,
+};
+unsigned int dm646x_tc_error_int[EDMA_MAX_TC] = {
+	IRQ_TCERRINT0, IRQ_TCERRINT,
+	IRQ_DM646X_TCERRINT2, IRQ_DM646X_TCERRINT3,
+	0, 0, 0, 0,
+};
+unsigned int dm355_tc_error_int[EDMA_MAX_TC] = {
+	IRQ_TCERRINT0, IRQ_TCERRINT,
+	0, 0, 0, 0, 0, 0,
 };
 
-static int queue_priority_mapping[DAVINCI_EDMA_NUM_EVQUE + 1][2] = {
-	/* {event queue no, Priority} */
-	{0, 0},
-	{1, 1},
-	{-1, -1}
+/**************************************************************************\
+* Edma Driver Internal Functions
+\**************************************************************************/
+
+/**  EDMA3 TC0 Error Interrupt Handler ISR Routine */
+
+static irqreturn_t dma_tc0_err_handler(int irq, void *dev_id,
+					struct pt_regs *data);
+/**  EDMA3 TC1 Error Interrupt Handler ISR Routine */
+static irqreturn_t dma_tc1_err_handler(int irq, void *dev_id,
+					struct pt_regs *data);
+/**  EDMA3 TC2 Error Interrupt Handler ISR Routine */
+static irqreturn_t dma_tc2_err_handler(int irq, void *dev_id,
+					struct pt_regs *data);
+/**  EDMA3 TC3 Error Interrupt Handler ISR Routine */
+static irqreturn_t dma_tc3_err_handler(int irq, void *dev_id,
+					struct pt_regs *data);
+/**  EDMA3 TC4 Error Interrupt Handler ISR Routine */
+static irqreturn_t dma_tc4_err_handler(int irq, void *dev_id,
+					struct pt_regs *data);
+/**  EDMA3 TC5 Error Interrupt Handler ISR Routine */
+static irqreturn_t dma_tc5_err_handler(int irq, void *dev_id,
+					struct pt_regs *data);
+/**  EDMA3 TC6 Error Interrupt Handler ISR Routine */
+static irqreturn_t dma_tc6_err_handler(int irq, void *dev_id,
+					struct pt_regs *data);
+/**  EDMA3 TC7 Error Interrupt Handler ISR Routine */
+static irqreturn_t dma_tc7_err_handler(int irq, void *dev_id,
+					struct pt_regs *data);
+
+
+/**
+  * EDMA3 TC ISRs which need to be registered with the underlying OS by the user
+  * (Not all TC error ISRs need to be registered, register only for the
+  * available Transfer Controllers).
+  */
+irqreturn_t (*ptr_edmatc_isrs[EDMA_MAX_TC])(int irq, void *dev_id, 
+		struct pt_regs *data) = {
+	&dma_tc0_err_handler,
+	&dma_tc1_err_handler,
+	&dma_tc2_err_handler,
+	&dma_tc3_err_handler,
+	&dma_tc4_err_handler,
+	&dma_tc5_err_handler,
+	&dma_tc6_err_handler,
+	&dma_tc7_err_handler,
 };
 
-static int qdam_to_param_mapping[8] = { 0 };
+/* Function registering different ISRs with the OS */
+static int register_dma_interrupts(void);
 
-volatile edmacc_regs *ptr_edmacc_regs = NULL;
 
-/*****************************************************************************/
 
-static void map_dmach_queue(int ch_no, int queue_no)
+/*******************************************************************************/
+static void map_dma_ch_evt_queue (unsigned int dma_ch, unsigned int evt_queue)
 {
-	if (ch_no < DAVINCI_EDMA_NUM_DMACH) {
-		int bit_start = (ch_no % 8) * 4;
-		ptr_edmacc_regs->dmaqnum[ch_no >> 3] &= (~(0x7 << bit_start));
-		ptr_edmacc_regs->dmaqnum[ch_no >> 3] |=
-		    ((queue_no & 0x7) << bit_start);
-	} else if (ch_no >= DAVINCI_EDMA_NUM_DMACH
-		   &&
-		   ch_no < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		int bit_start = (ch_no - DAVINCI_EDMA_NUM_DMACH) * 4;
-		ptr_edmacc_regs->qdmaqnum &= (~(0x7 << bit_start));
-		ptr_edmacc_regs->qdmaqnum |= ((queue_no & 0x7) << bit_start);
-	}
+    ptr_edmacc_regs->dmaqnum[dma_ch >> 3] &= DMAQNUM_CLR_MASK(dma_ch);
+    ptr_edmacc_regs->dmaqnum[dma_ch >> 3] |= DMAQNUM_SET_MASK(dma_ch, evt_queue);
 }
 
-/* For Davinci this Macro supports mapping only for QDMA channels and PaRam
-   entry */
-static void map_dmach_param(int ch_no, int param_no)
-{
-	if (ch_no >= DAVINCI_EDMA_NUM_DMACH
-	    && ch_no < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		ptr_edmacc_regs->qchmap[ch_no - DAVINCI_EDMA_NUM_DMACH] &=
-		    ~(PAENTRY | TRWORD);
-		ptr_edmacc_regs->qchmap[ch_no - DAVINCI_EDMA_NUM_DMACH] |=
-		    (((param_no & 0x1ff) << 5) | (QDMA_TRWORD << 2));
-	}
+
+
+static void map_qdma_ch_evt_queue (unsigned int qdma_ch, unsigned int evt_queue)
+{
+    /* Map QDMA channel to event queue*/
+    ptr_edmacc_regs->qdmaqnum &= QDMAQNUM_CLR_MASK(qdma_ch);
+    ptr_edmacc_regs->qdmaqnum |= QDMAQNUM_SET_MASK(qdma_ch, evt_queue);
 }
 
-static void map_queue_tc(int queue_no, int tc_no)
+
+static void map_dma_ch_param_set (unsigned int dma_ch, unsigned int param_set)
 {
-	int bit_start = queue_no * 4;
-	ptr_edmacc_regs->quetcmap &= ~(0x7 << bit_start);
-	ptr_edmacc_regs->quetcmap |= ((tc_no & 0x7) << bit_start);
+
+    if (davinci_edma_chmap_exist == 1)  {
+    /* Map PaRAM Set Number for specified dma_ch */
+        ptr_edmacc_regs->dchmap[dma_ch] &= DMACH_PARAM_CLR_MASK;
+        ptr_edmacc_regs->dchmap[dma_ch] |=
+                    DMACH_PARAM_SET_MASK(param_set);
+    }
 }
 
-static void assign_priority_to_queue(int queue_no, int priority)
+
+static void map_qdma_ch_param_set (unsigned int qdma_ch, unsigned int param_set)
 {
-	int bit_start = queue_no * 4;
-	ptr_edmacc_regs->quepri &= ~(0x7 << bit_start);
-	ptr_edmacc_regs->quepri |= ((priority & 0x7) << bit_start);
+    /* Map PaRAM Set Number for specified qdma_ch */
+    ptr_edmacc_regs->qchmap[qdma_ch] &= QDMACH_PARAM_CLR_MASK;
+    ptr_edmacc_regs->qchmap[qdma_ch] |= QDMACH_PARAM_SET_MASK(param_set);
+
+    /* Set CCNT as default Trigger Word */
+    ptr_edmacc_regs->qchmap[qdma_ch] &= QDMACH_TRWORD_CLR_MASK;
+    ptr_edmacc_regs->qchmap[qdma_ch] |= QDMACH_TRWORD_SET_MASK(QDMA_DEF_TRIG_WORD);
 }
 
-/******************************************************************************
- *
- * DMA Param entry requests: Requests for the param structure entry for the dma
- *                          channel passed
- * Arguments:
- *      lch  - logical channel for which param entry is being requested.
- *
- * Return: param number on success, or negative error number on failure
- *
- *****************************************************************************/
-static int request_param(int lch, int dev_id)
+
+
+static void register_callback(unsigned int tcc,
+                        void (*callback) (int lch, unsigned short ch_status,
+                                            void *data),
+                        void *data)
 {
-	int i = 0, j = 0, is_break = 0;
-	if (lch >= 0 && lch < DAVINCI_EDMA_NUM_DMACH) {
-		/*
-		   In davinci there is 1:1 mapping between edma channels
-		   and param sets
-		 */
-		LOCK;
-		/* It maintains param entry availability bitmap which
-		   could be updated by several thread  same channel
-		   and so requires protection
-		 */
-		param_entry_use_status[lch / 32] &= (~(1 << (lch % 32)));
-		UNLOCK;
-		return lch;
-	} else {
-		if (dev_id >= DAVINCI_DMA_QDMA0 &&
-		    dev_id <= DAVINCI_DMA_QDMA7) {
-			i = 0;
-		} else if (dev_id == DAVINCI_EDMA_PARAM_ANY) {
-			i = DAVINCI_EDMA_NUM_DMACH;
-		}
+    /* If callback function is not NULL */
+    if (callback) {
+        if (tcc < 32) {
+            ptr_edmacc_shadow_regs->iesr |= (1UL << tcc);
+
+             DMA_PRINTK ("ier = %x \r\n",
+                                ptr_edmacc_shadow_regs->ier);
 
-		/* This allocation alogrithm requires complete lock because
-		   availabilty of param entry is checked from structure
-		   param_entry_use_status and same struct is updated back also
-		   once allocated
-		 */
-
-		LOCK;
-		while (i < DAVINCI_EDMA_NUM_PARAMENTRY) {
-			j = 0, is_break = 1;
-			if ((param_entry_arm[i / 32] & (1 << (i % 32))) &&
-			    (param_entry_use_status[i / 32] & (1 << (i % 32))))
-			{
-				if (dev_id != DAVINCI_EDMA_PARAM_ANY) {
-					while (dma_chan_no_event[j] != -1) {
-						if (dma_chan_no_event[j] == i) {
-							is_break = 0;
-						}
-						j++;
-					}
-					if (!is_break) {
-						break;
-					}
-				} else {
-					break;
-				}
-				i++;
-			} else {
-				i++;
-			}
+        } else if (tcc < EDMA_NUM_TCC) {
+            ptr_edmacc_shadow_regs->iesrh |=
+                                    (1UL << (tcc - 32));
+
+             DMA_PRINTK ("ierh = %x \r\n",
+                                ptr_edmacc_shadow_regs->ierh);
+        } else {
+		printk ("WARNING: dma register callback failed - "
+			"invalid tcc %d\n", tcc);
+		return;
+	}
+
+        /* Save the callback function also */
+        dma_interrupt_param[tcc].callback = callback;
+        dma_interrupt_param[tcc].data = data;
+    }
+}
+
+
+
+static void unregister_callback(unsigned int lch, enum resource_type ch_type)
+{
+    unsigned int tcc;
+
+    DMA_FN_IN;
+    DMA_PRINTK("lch = %d\n", lch);
+
+    switch (ch_type)   {
+        case RES_DMA_CHANNEL:
+            tcc = edma_dma_ch_tcc_mapping[lch];
+            DMA_PRINTK("mapped tcc for DMA channel = %d\n", tcc);
+            /* reset */
+            edma_dma_ch_tcc_mapping[lch] = EDMA_NUM_TCC;
+            break;
+
+        case RES_QDMA_CHANNEL:
+            tcc = edma_qdma_ch_tcc_mapping[lch-EDMA_QDMA_CHANNEL_0];
+            DMA_PRINTK("mapped tcc for QDMA channel = %d\n", tcc);
+            /* reset */
+            edma_qdma_ch_tcc_mapping[lch-EDMA_QDMA_CHANNEL_0]=EDMA_NUM_TCC;
+            break;
+
+        default:
+            return;
+    }
+
+
+    /* Remove the callback function and disable the interrupts */
+    if (tcc < 32) {
+	ptr_edmacc_shadow_regs->iecr |= (1UL << tcc);
+    } else if (tcc < EDMA_NUM_TCC) {
+	ptr_edmacc_shadow_regs->iecrh |= (1UL << (tcc - 32));
+    } else {
+	printk ("WARNING: dma unregister callback failed - invalid "
+		"tcc %d on lch %d\n", tcc, lch);
+	return;
+    }
+
+    if (tcc < EDMA_NUM_TCC)   {
+	dma_interrupt_param[tcc].callback = 0;
+	dma_interrupt_param[tcc].data = 0;
+    }
+
+    DMA_FN_OUT;
+}
+
+
+
+static int alloc_resource(unsigned int res_id,
+                            enum resource_type res_type)
+{
+    unsigned int avlbl_id = 0;
+    int result = -1;
+    unsigned int res_id_clear = 0x0;
+    unsigned int res_id_set = 0x0;
+
+    res_id_clear = (unsigned int)(~(1u << (res_id % 32u)));
+    res_id_set = (1u << (res_id % 32u));
+
+    spin_lock(&dma_chan_lock);
+
+    switch (res_type)   {
+        case RES_DMA_CHANNEL :
+        {
+            if (res_id == EDMA_DMA_CHANNEL_ANY) {
+                for (avlbl_id = 0; avlbl_id < EDMA_NUM_DMACH; ++avlbl_id)   {
+                    if (((edma_channels_arm[avlbl_id/32]) &
+                        (dma_ch_use_status[avlbl_id/32u]) &
+                        ~(davinci_dma_ch_hw_event_map[avlbl_id/32u]) &
+                        (1u << (avlbl_id % 32u))) != 0)   {
+                        /* DMA Channel Available, mark it as unavailable */
+                        DMA_PRINTK ("avlbl dma = %x\n", avlbl_id);
+                        result = avlbl_id;
+                        dma_ch_use_status[avlbl_id/32u] &= (~(1u << (avlbl_id%32u)));
+
+                        /* Enable the DMA channel in the DRAE/DRAEH registers */
+                        if (avlbl_id < 32u) {
+                            ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].drae
+                                |= (0x1u << avlbl_id);
+                            DMA_PRINTK ("drae = %x\n",
+                                ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].drae);
+                        }   else    {
+                            ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].draeh
+                                |= (0x1u << (avlbl_id - 32u));
+                            DMA_PRINTK ("draeh = %x\n",
+                                ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].draeh);
+                        }
+
+                        break;
+                    }
+                }
+            }
+            else if (res_id < EDMA_NUM_DMACH)   {
+                if (((edma_channels_arm[res_id/32]) & (res_id_set)) != 0)  {
+                    if (((dma_ch_use_status[res_id/32u]) & (res_id_set)) != 0)  {
+                        /* Mark it as non-available now */
+                        dma_ch_use_status[res_id/32u] &= res_id_clear;
+
+                        if (res_id < 32u)   {
+                            /* Enable the DMA channel in the DRAE register */
+                            ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].drae
+                                |= (0x1u << res_id);
+                            DMA_PRINTK ("drae = %x\n",
+                                ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].drae);
+
+                            ptr_edmacc_shadow_regs->eecr |= (1 << res_id);
+                        } else {
+                            /* Enable the DMA channel in the DRAEH register */
+                            ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].draeh
+                                |= (0x1u << (res_id - 32u));
+                            DMA_PRINTK ("draeh = %x\n",
+                                ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].draeh);
+
+                            ptr_edmacc_shadow_regs->eecrh
+                                                    |= (1 << (res_id - 32u));
+                        }
+                        result = res_id;
+                    }
+                }
+            }
+        }
+        break;
+
+    case RES_QDMA_CHANNEL:
+        {
+            if (res_id == EDMA_QDMA_CHANNEL_ANY) {
+                for (avlbl_id = 0; avlbl_id < EDMA_NUM_QDMACH; ++avlbl_id)  {
+                    if (((qdma_channels_arm[0]) &
+                        (qdma_ch_use_status[0]) &
+                        (1u << (avlbl_id % 32u))) != 0)   {
+                        /* QDMA Channel Available, mark it as unavailable */
+                        DMA_PRINTK ("avlbl qdma = %x\n", avlbl_id);
+                        result = avlbl_id;
+                        qdma_ch_use_status[0] &= (~(1u << (avlbl_id%32u)));
+
+                        /* Enable the QDMA channel in the QRAE registers */
+                        ptr_edmacc_regs->qrae[EDMA_MASTER_SHADOW_REGION]
+                            |= (0x1u << avlbl_id);
+                        DMA_PRINTK ("qrae = %x\n",
+                            ptr_edmacc_regs->qrae[EDMA_MASTER_SHADOW_REGION]);
+
+                        break;
+                    }
+                }
+            }
+            else if (res_id < EDMA_NUM_QDMACH)  {
+                if (((qdma_channels_arm[0]) & (res_id_set)) != 0)  {
+                    if (((qdma_ch_use_status[0]) & (res_id_set)) != 0)  {
+                        /* QDMA Channel Available, mark it as unavailable */
+                        qdma_ch_use_status[0] &= res_id_clear;
+
+                        /* Enable the QDMA channel in the QRAE registers */
+                        ptr_edmacc_regs->qrae[EDMA_MASTER_SHADOW_REGION]
+                            |= (0x1u << res_id);
+                        DMA_PRINTK ("qrae = %x\n",
+                            ptr_edmacc_regs->qrae[EDMA_MASTER_SHADOW_REGION]);
+
+                        result = res_id;
+                    }
+                }
+            }
+        }
+        break;
+
+    case RES_TCC:
+        {
+            if (res_id == EDMA_TCC_ANY) {
+                for (avlbl_id = 0; avlbl_id < EDMA_NUM_TCC; ++avlbl_id) {
+                    if (((tcc_arm[avlbl_id/32]) &
+                        (tcc_use_status[avlbl_id/32u]) &
+                        ~(davinci_dma_ch_hw_event_map[avlbl_id/32u]) &
+                        (1u << (avlbl_id % 32u))) != 0)   {
+                        /* TCC Available, mark it as unavailable */
+                        DMA_PRINTK ("avlbl tcc = %x\n", avlbl_id);
+                        result = avlbl_id;
+                        tcc_use_status[avlbl_id/32u] &= (~(1u << (avlbl_id%32u)));
+
+                        /* Enable the TCC in the DRAE/DRAEH registers */
+                        if (avlbl_id < 32u) {
+                            ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].drae
+                                |= (0x1u << avlbl_id);
+                            DMA_PRINTK ("drae = %x\n",
+                                ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].drae);
+
+                            /* Add it to the Allocated TCCs list */
+                            allocated_tccs[0u] |= (0x1u << avlbl_id);
+                        }   else    {
+                            ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].draeh
+                                |= (0x1u << (avlbl_id - 32u));
+                            DMA_PRINTK ("draeh = %x\n",
+                                ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].draeh);
+
+                            /* Add it to the Allocated TCCs list */
+                            allocated_tccs[1u] |= (0x1u << (avlbl_id - 32u));
+                        }
+
+                        break;
+                    }
+                }
+            }
+            else if (res_id < EDMA_NUM_TCC) {
+                if (((tcc_arm[res_id/32]) & (1u << (res_id%32u))) != 0)   {
+                    if (((tcc_use_status[res_id/32u]) & (res_id_set)) != 0) {
+                        /* Mark it as non-available now */
+                        tcc_use_status[res_id/32u] &= res_id_clear;
+
+                        /* Enable the TCC in the DRAE/DRAEH registers */
+                        if (res_id < 32u) {
+                            ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].drae
+                                |= (0x1u << res_id);
+
+                            allocated_tccs[0u] |= (0x1u << res_id);
+                        }   else    {
+                            ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].draeh
+                                |= (0x1u << (res_id - 32u));
+
+                            allocated_tccs[1u] |= (0x1u << (res_id - 32u));
+                        }
+
+                        result = res_id;
+                    }
+                }
+            }
+        }
+        break;
+
+    case RES_PARAM_SET:
+        {
+            if (res_id == DAVINCI_EDMA_PARAM_ANY)   {
+                for (avlbl_id = 0; avlbl_id < davinci_edma_num_param;
+			++avlbl_id) {
+                    if (((param_entry_arm[avlbl_id/32]) &
+                        (param_entry_use_status[avlbl_id/32u]) &
+                        ~(param_entry_reserved[avlbl_id/32u]) &
+                        (1u << (avlbl_id%32u))) != 0)   {
+                        /* PARAM Set Available, mark it as unavailable */
+                        DMA_PRINTK ("avlbl param = %x\n", avlbl_id);
+                        result = avlbl_id;
+                        param_entry_use_status[avlbl_id/32u]
+                                                &= (~(1u << (avlbl_id%32u)));
+
+                        /* Also, make the actual PARAM Set NULL */
+                        memset((void *)&(ptr_edmacc_regs->paramentry[avlbl_id]),
+                                    0x00u,
+                                    sizeof(ptr_edmacc_regs->paramentry[avlbl_id]));
+
+                        break;
+                    }
+                }
+            }
+            else if (res_id < davinci_edma_num_param)  {
+                if (((param_entry_arm[res_id/32])&(1u << (res_id%32u))) != 0) {
+                    if (((param_entry_use_status[res_id/32u]) & (res_id_set)) != 0) {
+                        /* Mark it as non-available now */
+                        param_entry_use_status[res_id/32u] &= res_id_clear;
+                        result = res_id;
+
+                        /* Also, make the actual PARAM Set NULL */
+                        memset((void *)&(ptr_edmacc_regs->paramentry[res_id]),
+                                    0x00u,
+                                    sizeof(ptr_edmacc_regs->paramentry[res_id]));
+                    }
+                }
+            }
+        }
+        break;
+    }
+
+    spin_unlock(&dma_chan_lock);
+
+    return result;
+}
+
+static void free_resource(unsigned int res_id,
+                            enum resource_type res_type)
+{
+    unsigned int res_id_set = 0x0;
+
+    res_id_set = (1u << (res_id%32u));
+
+    spin_lock(&dma_chan_lock);
+
+    switch (res_type)   {
+        case RES_DMA_CHANNEL :
+        {
+            if (res_id < EDMA_NUM_DMACH)    {
+                if (((edma_channels_arm[res_id/32]) & (res_id_set)) != 0)  {
+                    if ((~(dma_ch_use_status[res_id/32u]) & (res_id_set)) != 0) {
+                        /* Make it as available */
+                        dma_ch_use_status[res_id/32u] |= res_id_set;
+
+                        /* Reset the DRAE/DRAEH bit also */
+                        if (res_id < 32u) {
+                            ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].drae
+                                &= (~(0x1u << res_id));
+                        }   else    {
+                            ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].draeh
+                                &= (~(0x1u << (res_id - 32u)));
+                        }
+                    }
+                }
+            }
+        }
+        break;
+
+    case RES_QDMA_CHANNEL:
+        {
+            if (res_id < EDMA_NUM_QDMACH)   {
+                if (((qdma_channels_arm[0]) & (res_id_set)) != 0)  {
+                    if ((~(qdma_ch_use_status[0]) & (res_id_set)) != 0) {
+                        /* Make it as available */
+                        qdma_ch_use_status[0] |= res_id_set;
+
+                        /* Reset the DRAE/DRAEH bit also */
+                        ptr_edmacc_regs->qrae[EDMA_MASTER_SHADOW_REGION]
+                            &= (~(0x1u << res_id));
+                    }
+                }
+            }
+        }
+        break;
+
+    case RES_TCC:
+        {
+            if (res_id < EDMA_NUM_TCC)  {
+                if (((tcc_arm[res_id/32]) & (res_id_set)) != 0)  {
+                    if ((~(tcc_use_status[res_id/32u]) & (res_id_set)) != 0)   {
+                        /* Make it as available */
+                        tcc_use_status[res_id/32u] |= res_id_set;
+
+                        /* Reset the DRAE/DRAEH bit also */
+                        if (res_id < 32u) {
+                            ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].drae
+                                &= (~(0x1u << res_id));
+
+                            /* Remove it from the Allocated TCCs list */
+                            allocated_tccs[0u] &= (~(0x1u << res_id));
+                        }   else    {
+                            ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].draeh
+                                &= (~(0x1u << (res_id - 32u)));
+
+                            /* Remove it from the Allocated TCCs list */
+                            allocated_tccs[1u] &= (~(0x1u << (res_id - 32u)));
+                        }
+                    }
+                }
+            }
+        }
+        break;
+
+    case RES_PARAM_SET:
+        {
+            if (res_id < davinci_edma_num_param)   {
+                if (((param_entry_arm[res_id/32])&(1u << (res_id%32u))) != 0) {
+                    if ((~(param_entry_use_status[res_id/32u]) & (res_id_set)) 
+			!= 0) {
+                        /* Make it as available */
+                        param_entry_use_status[res_id/32u] |= res_id_set;
+                    }
+                }
+            }
+        }
+        break;
+
+    }
+
+    spin_unlock(&dma_chan_lock);
+}
+
+
+/******************************************************************************
+ *
+ * EDMA3 CC Transfer Completion Interrupt Handler
+ *
+ *****************************************************************************/
+static irqreturn_t dma_irq_handler (int irq, void *dev_id, struct pt_regs *data)
+{
+    unsigned int cnt = 0;
+    volatile unsigned int pendingIrqs = 0;
+    unsigned int indexl = 1;
+    unsigned int indexh = 1;
+
+    if((ptr_edmacc_shadow_regs->ipr !=0 ) ||
+        (ptr_edmacc_shadow_regs->iprh !=0 )) {
+        /*Loop while cnt < 10, breaks when no pending interrupt is found*/
+        while ((cnt < 10u) && ((indexl != 0u) || (indexh != 0u)))   {
+            indexl = 0;
+            pendingIrqs = ptr_edmacc_shadow_regs->ipr;
+
+            /**
+             * Choose interrupts coming from our allocated TCCs
+             * and MASK remaining ones.
+             */
+            pendingIrqs = (pendingIrqs & allocated_tccs[0u]);
+
+            while (pendingIrqs) {
+                /*Process all the pending interrupts*/
+                if((pendingIrqs & 1u) == 1)   {
+                    /**
+                     * If the user has not given any callback function
+                     * while requesting the TCC, its TCC specific bit
+                     * in the IPR register will NOT be cleared.
+                     */
+                    if (dma_interrupt_param[indexl].callback) {
+                    /* here write to ICR to clear the corresponding IPR bits*/
+                        ptr_edmacc_shadow_regs->icr |= (1u << indexl);
+
+                        /* Call the callback function now */
+                        dma_interrupt_param[indexl].callback(indexl,
+                                            DMA_COMPLETE,
+                                            dma_interrupt_param[indexl].data);
+                    }
+                }
+                ++indexl;
+                pendingIrqs >>= 1u;
+            }
+
+            indexh = 0;
+            pendingIrqs = ptr_edmacc_shadow_regs->iprh;
+
+            /**
+             * Choose interrupts coming from our allocated TCCs
+             * and MASK remaining ones.
+             */
+            pendingIrqs = (pendingIrqs & allocated_tccs[1u]);
+
+            while (pendingIrqs) {
+                /*Process all the pending interrupts*/
+                if((pendingIrqs & 1u) == 1)   {
+                    /**
+                     * If the user has not given any callback function
+                     * while requesting the TCC, its TCC specific bit
+                     * in the IPRH register will NOT be cleared.
+                     */
+                    if (dma_interrupt_param[32+indexh].callback) {
+                        /* Write to ICRH to clear the corresponding IPRH bits*/
+                        ptr_edmacc_shadow_regs->icrh |= (1u << indexh);
+
+                        /* Call the callback function now */
+                        dma_interrupt_param[32+indexh].callback(32+indexh,
+                                        DMA_COMPLETE,
+                                        dma_interrupt_param[32+indexh].data);
+                    }
+                }
+                ++indexh;
+                pendingIrqs >>= 1u;
+            }
+
+            cnt++;
+        }
+
+        ptr_edmacc_shadow_regs->ieval = 0x1;
+    }
+
+    return IRQ_HANDLED;
+}
+
+
+
+/******************************************************************************
+ *
+ * EDMA3 CC Error Interrupt Handler
+ *
+ *****************************************************************************/
+static irqreturn_t dma_ccerr_handler (int irq, void *dev_id, struct pt_regs *data)
+{
+    unsigned int cnt = 0;
+    volatile unsigned int pendingIrqs = 0;
+    unsigned int index = 1;
+    unsigned int mapped_tcc = 0;
+    unsigned int evtque_num = 0;
+
+    if(((ptr_edmacc_regs->emr != 0 )
+        || (ptr_edmacc_regs->emrh != 0 ))
+        || ((ptr_edmacc_regs->qemr != 0)
+        || (ptr_edmacc_regs->ccerr != 0))) {
+        /*Loop while Cnt < 10, breaks when no pending interrupt is found*/
+        while ((cnt < 10u) && (index != 0u))    {
+            index = 0;
+            pendingIrqs = ptr_edmacc_regs->emr;
+
+            while (pendingIrqs) {
+                /*Process all the pending interrupts*/
+                if((pendingIrqs & 1u) == 1)   {
+                    /* Write to EMCR to clear the corresponding EMR bit */
+                    ptr_edmacc_regs->emcr |= (1u << index);
+                    /*Clear any SER*/
+                    ptr_edmacc_shadow_regs->secr |= (1u << index);
+
+                    /**
+                     * Using the 'index' value (basically the DMA
+                     * channel), fetch the corresponding TCC
+                     * value, mapped to this DMA channel.
+                     */
+                    mapped_tcc = edma_dma_ch_tcc_mapping[index];
+
+                    if (dma_interrupt_param[mapped_tcc].callback) {
+                        dma_interrupt_param[mapped_tcc].callback(mapped_tcc,
+                                    DMA_EVT_MISS_ERROR,
+                                    dma_interrupt_param[mapped_tcc].data);
+                    }
+                }
+                ++index;
+                pendingIrqs >>= 1u;
+            }
+
+            index = 0;
+            pendingIrqs = ptr_edmacc_regs->emrh;
+
+            while (pendingIrqs) {
+                /*Process all the pending interrupts*/
+                if((pendingIrqs & 1u) == 1)   {
+                    /* Write to EMCRH to clear the corresponding EMRH bit */
+                    ptr_edmacc_regs->emcrh |= (1u << index);
+                    /*Clear any SERH*/
+                    ptr_edmacc_shadow_regs->secrh |= (1u << index);
+
+                    /**
+                     * Using the 'index' value (basically the DMA
+                     * channel), fetch the corresponding TCC
+                     * value, mapped to this DMA channel.
+                     */
+                    mapped_tcc = edma_dma_ch_tcc_mapping[32 + index];
+
+                    if (dma_interrupt_param[mapped_tcc].callback) {
+                        dma_interrupt_param[mapped_tcc].callback(mapped_tcc,
+                                    DMA_EVT_MISS_ERROR,
+                                    dma_interrupt_param[mapped_tcc].data);
+                    }
+                }
+                ++index;
+                pendingIrqs >>= 1u;
+            }
+
+            index = 0;
+            pendingIrqs = ptr_edmacc_regs->qemr;
+
+            while (pendingIrqs) {
+                DMA_PRINTK ("pendingIrqs = %x\n", pendingIrqs);
+
+                /*Process all the pending interrupts*/
+                if((pendingIrqs & 1u) == 1)   {
+                    /* Write to QEMCR to clear the corresponding QEMR bit */
+                    ptr_edmacc_regs->qemcr |= (1u << index);
+                    /*Clear any QSER*/
+                    ptr_edmacc_shadow_regs->qsecr |= (1u << index);
+
+                    /**
+                     * Using the 'index' value (basically the QDMA
+                     * channel), fetch the corresponding TCC
+                     * value, mapped to this QDMA channel.
+                     */
+                    mapped_tcc = edma_qdma_ch_tcc_mapping[index];
+                    DMA_PRINTK ("index = %d, mapped_tcc = %d\n", index, mapped_tcc);
+
+                    if (dma_interrupt_param[mapped_tcc].callback) {
+                        dma_interrupt_param[mapped_tcc].callback(mapped_tcc,
+                                    QDMA_EVT_MISS_ERROR,
+                                    dma_interrupt_param[mapped_tcc].data);
+                    }
+                }
+                ++index;
+                pendingIrqs >>= 1u;
+            }
+
+            pendingIrqs = ptr_edmacc_regs->ccerr;
+
+            for (evtque_num = 0; evtque_num < davinci_edma_num_evtq;
+		evtque_num++) {
+                if((pendingIrqs & (1u << evtque_num)) != 0)   {
+                    /* Clear the event queue specific error interrupt */
+                    ptr_edmacc_regs->ccerrclr |= (1u << evtque_num);
+                }
+            }
+
+            if (pendingIrqs & (1 << CCRL_CCERR_TCCERR_SHIFT))  {
+                ptr_edmacc_regs->ccerrclr |= (1 << CCRL_CCERR_TCCERR_SHIFT);
+            }
+
+        cnt++;
+        }
+
+        ptr_edmacc_regs->eeval=0x1u;
+    }
+
+    return IRQ_HANDLED;
+}
+
+
+
+/******************************************************************************
+ *
+ * EDMA3 Transfer Controller Error Interrupt Handler
+ *
+ *****************************************************************************/
+static int dma_tc_err_handler(unsigned int tc_num)
+{
+	volatile edmatc_regs *tcregs = NULL;
+
+	if (tc_num >= davinci_edma_num_tc) {
+		return -EINVAL;
+	}
+
+	tcregs = (volatile edmatc_regs *)(ptr_edmatc_regs[tc_num]);
+
+	if (tcregs != NULL) {
+	    if(tcregs->errstat) {
+		if((tcregs->errstat & (1u << EDMA_TC_ERRSTAT_BUSERR_SHIFT)) != 0) {
+			tcregs->errclr = (1u << EDMA_TC_ERRSTAT_BUSERR_SHIFT);
+		}
+
+		if((tcregs->errstat & (1 << EDMA_TC_ERRSTAT_TRERR_SHIFT)) != 0) {
+			tcregs->errclr = (1 << EDMA_TC_ERRSTAT_TRERR_SHIFT);
 		}
-		if (i < DAVINCI_EDMA_NUM_PARAMENTRY) {
-			param_entry_use_status[i / 32] &= (~(1 << (i % 32)));
-			UNLOCK;
-			dev_dbg(&edma_dev.dev, "param no=%d\r\n", i);
-			return i;
-		} else {
-			UNLOCK;
-			return -1;	/* no free param */
+
+		if((tcregs->errstat & (1 << EDMA_TC_ERRSTAT_MMRAERR_SHIFT)) != 0) {
+			tcregs->errclr = (1 << EDMA_TC_ERRSTAT_MMRAERR_SHIFT);
 		}
+
+	    }
 	}
+
+	return 0;
 }
 
+
+
 /******************************************************************************
  *
- * Free dma param entry: Freethe param entry number passed
- * Arguments:
- *      param_no - Param entry to be released or freed out
- *
- * Return: N/A
+ * EDMA3 TC0 Error Interrupt Handler
  *
  *****************************************************************************/
-static void free_param(int param_no)
+static irqreturn_t dma_tc0_err_handler(int irq, void *dev_id, struct pt_regs *data)
 {
-	if (param_no >= 0 && param_no < DAVINCI_EDMA_NUM_PARAMENTRY) {
-		LOCK;
-		/* This is global data structure and could be accessed
-		   by several thread
-		 */
-		param_entry_use_status[param_no / 32] |= (1 << (param_no % 32));
-		UNLOCK;
-	}
+	/* Invoke Error Handler ISR for TC0*/
+	dma_tc_err_handler (0u);
+
+	return IRQ_HANDLED;
 }
 
 /******************************************************************************
  *
- * DMA interrupt requests: Requests for the interrupt on the free channel
+ * EDMA3 TC1 Error Interrupt Handler
  *
- * Arguments:
- *      lch - logical channel number for which the interrupt is to be requested
- *            for the free channel.
- *      callback - callback function registered for the requested interrupt
- *                 channel
- *      data - channel private data.
- *
- * Return: free interrupt channel number on success, or negative error number
- *              on failure
- *
- *****************************************************************************/
-static int request_dma_interrupt(int lch,
-				 void (*callback) (int lch,
-						   unsigned short ch_status,
-						   void *data),
-				 void *data, int param_no, int requested_tcc)
-{
-	signed int free_intr_no = -1;
-	int i = 0, j = 0, is_break = 0;
-	/* edma channels */
-	if (lch >= 0 && lch < DAVINCI_EDMA_NUM_DMACH) {
-		/* Bitmap dma_intr_use_status is used to identify availabe tcc
-		   for interrupt purpose. This could be modified by several
-		   thread and same structure is checked availabilty as well as
-		   updated once it's found that resource is avialable */
-		LOCK;
-		if (dma_intr_use_status[lch / 32] & (1 << (lch % 32))) {
-			/* in use */
-			dma_intr_use_status[lch / 32] &= (~(1 << (lch % 32)));
-			UNLOCK;
-			free_intr_no = lch;
-			dev_dbg(&edma_dev.dev, "interrupt no=%d\r\n", free_intr_no);
-		} else {
-			UNLOCK;
-			dev_dbg(&edma_dev.dev, "EDMA:Error\r\n");
-			return -1;
-		}
-	}
+ *****************************************************************************/
+static irqreturn_t dma_tc1_err_handler(int irq, void *dev_id, struct pt_regs *data)
+{
+	/* Invoke Error Handler ISR for TC1*/
+	dma_tc_err_handler (1u);
 
-	/* qdma channels */
-	else if (lch >= DAVINCI_EDMA_NUM_DMACH
-		 && lch < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		if (requested_tcc != TCC_ANY) {
-			/* Complete allocation algo requires lock and as it's
-			   shared resources could be invoked by several thread.
-			   Structure dma_intr_use_status is used to check
-			   whether resource is availabe or not and latter marked
-			   as not available in the same structure */
-			LOCK;
-			if (dma_intr_use_status[requested_tcc / 32] &
-			    (1 << (requested_tcc % 32))) {
-				j = 0;
-				is_break = 1;
-				while (dma_chan_no_event[j] != -1) {
-					if (dma_chan_no_event[j] ==
-					    requested_tcc) {
-						is_break = 0;
-						break;
-					}
-					j++;
-				}
-				if (!is_break) {
-					dma_intr_use_status[requested_tcc / 32]
-					    &= (~(1 << (requested_tcc % 32)));
-					free_intr_no = requested_tcc;
-					dev_dbg(&edma_dev.dev,
-						"interrupt no=%d\r\n",
-						free_intr_no);
-				} else {
-					UNLOCK;
-					dev_dbg(&edma_dev.dev,
-						"Error - wrong tcc passed\r\n");
-					return -1;
-				}
-				UNLOCK;
-			} else {
-				UNLOCK;
-				dev_dbg(&edma_dev.dev,
-					"Error - wrong tcc passed\r\n");
-				return -1;
-			}
-		} else {
-			i = 0;
-			LOCK;
-			while (i < DAVINCI_EDMA_NUM_DMACH) {
-				j = 0;
-				is_break = 1;
-				if (dma_intr_use_status[i / 32] &
-				    (1 << (i % 32))) {
-					while (dma_chan_no_event[j] != -1) {
-						if (dma_chan_no_event[j] == i) {
-							is_break = 0;
-							break;
-						}
-						j++;
-					}
-					if (!is_break) {
-						dma_intr_use_status[i / 32] &=
-						    (~(1 << (i % 32)));
-						free_intr_no = i;
-
-						dev_dbg(&edma_dev.dev,
-							"interrupt no=%d\r\n",
-							free_intr_no);
-						break;
-					}
-					i++;
-				} else {
-					i++;
-				}
-			}
-			UNLOCK;
-		}
-	} else {
-		dev_dbg(&edma_dev.dev, "ERROR lch = %d\r\n", lch);
-	}
-	if (is_break) {
-		dev_dbg(&edma_dev.dev, "While allocating EDMA channel for QDMA");
-	}
-	if (lch >= DAVINCI_EDMA_NUM_DMACH && lch <
-	    (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		if (free_intr_no < 32) {
-			ptr_edmacc_regs->dra[0].drae =
-			    ptr_edmacc_regs->dra[0].drae | (1 << free_intr_no);
-		} else {
-			ptr_edmacc_regs->dra[0].draeh =
-			    ptr_edmacc_regs->dra[0].
-			    draeh | (1 << (free_intr_no - 32));
-		}
-	}
-	if (free_intr_no >= 0 && free_intr_no < 64) {
-		(free_intr_no < 32) ?
-		    (ptr_edmacc_regs->shadow[0].iesr |= (1UL << free_intr_no))
-		    : (ptr_edmacc_regs->shadow[0].iesrh |=
-		       (1UL << (free_intr_no - 32)));
-		intr_data[free_intr_no].callback = callback;
-		intr_data[free_intr_no].data = data;
-	}
-	return free_intr_no;
+	return IRQ_HANDLED;
 }
 
 /******************************************************************************
  *
- * Free the dma interrupt: Releases the dma interrupt on the channel
+ * EDMA3 TC2 Error Interrupt Handler
  *
- * Arguments:
- *      intr_no - interrupt number on the channel to be released or freed out
+ *****************************************************************************/
+static irqreturn_t dma_tc2_err_handler(int irq, void *dev_id, struct pt_regs *data)
+{
+	/* Invoke Error Handler ISR for TC2*/
+	dma_tc_err_handler (2u);
+
+	return IRQ_HANDLED;
+}
+
+/******************************************************************************
  *
- * Return: N/A
+ * EDMA3 TC3 Error Interrupt Handler
  *
  *****************************************************************************/
-static void free_dma_interrupt(int intr_no)
+static irqreturn_t dma_tc3_err_handler(int irq, void *dev_id, struct pt_regs *data)
 {
-	if (intr_no >= 0 && intr_no < 64) {
-		(intr_no < 32) ? (ptr_edmacc_regs->shadow[0].icr |=
-				  (1UL << (intr_no))) : (ptr_edmacc_regs->
-							 shadow[0].icrh |=
-							 (1UL <<
-							  (intr_no - 32)));
-		LOCK;
-		/* Global structure and could be modified by several task */
-		dma_intr_use_status[intr_no / 32] |= (1 << (intr_no % 32));
-		UNLOCK;
-		intr_data[intr_no].callback = NULL;
-		intr_data[intr_no].data = NULL;
+	/* Invoke Error Handler ISR for TC3*/
+	dma_tc_err_handler (3u);
 
-	}
+	return IRQ_HANDLED;
 }
 
 /******************************************************************************
  *
- * DMA interrupt handler
+ * EDMA3 TC4 Error Interrupt Handler
  *
  *****************************************************************************/
-static void dma_irq_handler(void)
+static irqreturn_t dma_tc4_err_handler(int irq, void *dev_id, struct pt_regs *data)
 {
-	int i;
-	unsigned int cnt;
-	cnt = 0;
-	if ((ptr_edmacc_regs->shadow[0].ipr == 0)
-	    && (ptr_edmacc_regs->shadow[0].iprh == 0))
-		return;
-	while (1) {
-		if (ptr_edmacc_regs->shadow[0].ipr) {
-			dev_dbg(&edma_dev.dev, "IPR =%d\r\n",
-				ptr_edmacc_regs->shadow[0].ipr);
-			for (i = 0; i < 32; i++) {
-				if (ptr_edmacc_regs->shadow[0].ipr & (1 << i)) {
-					/* Clear the corresponding IPR bits */
-					ptr_edmacc_regs->shadow[0].icr |=
-					    (1 << i);
-					if (intr_data[i].callback) {
-						intr_data[i].callback(i,
-								      DMA_COMPLETE,
-								      intr_data
-								      [i].data);
-
-					}
-				}
-			}
-		} else if (ptr_edmacc_regs->shadow[0].iprh) {
-			dev_dbg(&edma_dev.dev, "IPRH =%d\r\n",
-				ptr_edmacc_regs->shadow[0].iprh);
-			for (i = 0; i < 32; i++) {
-				if (ptr_edmacc_regs->shadow[0].iprh & (1 << i)) {
-					/* Clear the corresponding IPR bits */
-					ptr_edmacc_regs->shadow[0].icrh |=
-					    (1 << i);
-					if (intr_data[32 + i].callback) {
-						intr_data[32 + i].callback(32 +
-									   i,
-									   DMA_COMPLETE,
-									   intr_data
-									   [32 +
-									    i].
-									   data);
-					}
-				}
-			}
-		}
-		if ((ptr_edmacc_regs->shadow[0].ipr == 0)
-		    && (ptr_edmacc_regs->shadow[0].iprh == 0)) {
-			break;
-		}
-		cnt++;
-		if (cnt > 10) {
-			break;
-		}
-	}
-	ptr_edmacc_regs->shadow[0].ieval = 0x1;
+	/* Invoke Error Handler ISR for TC4*/
+	dma_tc_err_handler (4u);
+
+	return IRQ_HANDLED;
 }
 
 /******************************************************************************
  *
- * DMA error interrupt handler
+ * EDMA3 TC5 Error Interrupt Handler
  *
  *****************************************************************************/
-static void dma_ccerr_handler(void)
+static irqreturn_t dma_tc5_err_handler(int irq, void *dev_id, struct pt_regs *data)
 {
-	int i;
-	unsigned int cnt;
-	cnt = 0;
-	if ((ptr_edmacc_regs->emr == 0) && (ptr_edmacc_regs->emr == 0) &&
-	    (ptr_edmacc_regs->qemr == 0) && (ptr_edmacc_regs->ccerr == 0))
-		return;
-	while (1) {
-		if (ptr_edmacc_regs->emr) {
-			dev_dbg(&edma_dev.dev, "EMR =%d\r\n", ptr_edmacc_regs->emr);
-			for (i = 0; i < 32; i++) {
-				if (ptr_edmacc_regs->emr & (1 << i)) {
-					/* Clear the corresponding EMR bits */
-					ptr_edmacc_regs->emcr |= (1 << i);
-					/* Clear any SER */
-					ptr_edmacc_regs->shadow[0].secr |=
-					    (1 << i);
-					if (intr_data[i].callback) {
-						intr_data[i].callback(i,
-								      DMA_CC_ERROR,
-								      intr_data
-								      [i].data);
-					}
-				}
-			}
-		} else if (ptr_edmacc_regs->emrh) {
-			dev_dbg(&edma_dev.dev, "EMRH =%d\r\n",
-				ptr_edmacc_regs->emrh);
-			for (i = 0; i < 32; i++) {
-				if (ptr_edmacc_regs->emrh & (1 << i)) {
-					/* Clear the corresponding IPR bits */
-					ptr_edmacc_regs->emcrh |= (1 << i);
-					/* Clear any SER */
-					ptr_edmacc_regs->shadow[0].secrh |=
-					    (1 << i);
-					if (intr_data[i].callback) {
-						intr_data[i].callback(i,
-								      DMA_CC_ERROR,
-								      intr_data
-								      [i].data);
-					}
-				}
-			}
-		} else if (ptr_edmacc_regs->qemr) {
-			dev_dbg(&edma_dev.dev, "QEMR =%d\r\n",
-				ptr_edmacc_regs->qemr);
-			for (i = 0; i < 8; i++) {
-				if (ptr_edmacc_regs->qemr & (1 << i)) {
-					/* Clear the corresponding IPR bits */
-					ptr_edmacc_regs->qemcr |= (1 << i);
-					ptr_edmacc_regs->shadow[0].qsecr |=
-					    (1 << i);
-				}
-			}
-		} else if (ptr_edmacc_regs->ccerr) {
-			dev_dbg(&edma_dev.dev, "CCERR =%d\r\n",
-				ptr_edmacc_regs->ccerr);
-			for (i = 0; i < 8; i++) {
-				if (ptr_edmacc_regs->ccerr & (1 << i)) {
-					/* Clear the corresponding IPR bits */
-					ptr_edmacc_regs->ccerrclr |= (1 << i);
-				}
-			}
-		}
-		if ((ptr_edmacc_regs->emr == 0)
-		    && (ptr_edmacc_regs->emrh == 0)
-		    && (ptr_edmacc_regs->qemr == 0)
-		    && (ptr_edmacc_regs->ccerr == 0)) {
-			break;
-		}
-		cnt++;
-		if (cnt > 10) {
-			break;
-		}
-	}
-	ptr_edmacc_regs->eeval = 0x1;
+	/* Invoke Error Handler ISR for TC5*/
+	dma_tc_err_handler (5u);
+
+	return IRQ_HANDLED;
 }
 
 /******************************************************************************
  *
- * DMA error interrupt handler
+ * EDMA3 TC6 Error Interrupt Handler
  *
  *****************************************************************************/
-static void dma_tc1err_handler(void)
+static irqreturn_t dma_tc6_err_handler(int irq, void *dev_id, struct pt_regs *data)
 {
+	/* Invoke Error Handler ISR for TC6*/
+	dma_tc_err_handler (6u);
 
+	return IRQ_HANDLED;
 }
 
 /******************************************************************************
  *
- * DMA error interrupt handler
+ * EDMA3 TC7 Error Interrupt Handler
  *
  *****************************************************************************/
-static void dma_tc2err_handler(void)
+static irqreturn_t dma_tc7_err_handler(int irq, void *dev_id, struct pt_regs *data)
 {
+	/* Invoke Error Handler ISR for TC7*/
+	dma_tc_err_handler (7u);
 
+	return IRQ_HANDLED;
 }
 
+
 /******************************************************************************
  *
- * DMA initialisation on davinci
+ * davinci_get_qdma_channel: Convert qdma channel to logical channel
+ * Arguments:
+ *      ch     - input qdma channel.
+ *
+ * Return: logical channel associated with qdma channel or logical channel
+ *     associated with qdma channel 0 for out of range channel input.
  *
  *****************************************************************************/
-int __init arch_dma_init(void)
+int davinci_get_qdma_channel(int ch)
 {
-	int i;
-	edma_driver.name = "edma";
-	edma_dev.name = "dma";
-	edma_dev.id = -1;
-	edma_dev.dev.driver = &edma_driver;
-
-	ptr_edmacc_regs = get_edma_base();
-	dev_dbg(&edma_dev.dev, "DMA REG BASE ADDR=%x\n", ptr_edmacc_regs);
-	memset(dma_chan, 0x00, sizeof(dma_chan));
-	memset((void *)&(ptr_edmacc_regs->paramentry[0]), 0x00,
-	       sizeof(ptr_edmacc_regs->paramentry));
-	i = 0;
-	/* Channel to queue mapping */
-	while (channel_queue_mapping[i][0] != -1) {
-		map_dmach_queue(channel_queue_mapping[i][0],
-				channel_queue_mapping[i][1]);
-		i++;
-	}
-	i = 0;
-	/* Event queue to TC mapping */
-	while (queue_tc_mapping[i][0] != -1) {
-		map_queue_tc(queue_tc_mapping[i][0], queue_tc_mapping[i][1]);
-		i++;
-	}
-	i = 0;
-	/* Event queue priority mapping */
-	while (queue_priority_mapping[i][0] != -1) {
-		assign_priority_to_queue(queue_priority_mapping[i][0],
-					 queue_priority_mapping[i][1]);
-		i++;
-	}
-	for (i = 0; i < DAVINCI_EDMA_NUM_REGIONS; i++) {
-		ptr_edmacc_regs->dra[i].drae = 0x0;
-		ptr_edmacc_regs->dra[i].draeh = 0x0;
-		ptr_edmacc_regs->qrae[i] = 0x0;
-	}
-	LOCK_INIT;
-	return 0;
+	if ((ch>=0) || (ch <= EDMA_MAX_CHANNEL))
+		return (davinci_qdma_ch_map[davinci_cpu_index] + ch);
+	else    /* return channel 0 for out of range values */
+		return davinci_qdma_ch_map[davinci_cpu_index];
 }
+EXPORT_SYMBOL(davinci_get_qdma_channel);
+
+
 
 /******************************************************************************
  *
@@ -706,19 +1376,18 @@ int __init arch_dma_init(void)
  *      callback   - pointer to the channel callback.
  *      Arguments:
  *          lch  - channel no, which is the IPR bit position,
- *		   indicating from which channel the interrupt arised.
+ *         indicating from which channel the interrupt arised.
  *          data - channel private data, which is received as one of the
- *		   arguments in davinci_request_dma.
+ *         arguments in davinci_request_dma.
  *      data - private data for the channel to be requested, which is used to
  *                   pass as a parameter in the callback function
- *		     in irq handler.
+ *           in irq handler.
  *      lch - contains the device id allocated
  *  tcc        - Transfer Completion Code, used to set the IPR register bit
  *                   after transfer completion on that channel.
  *  eventq_no  - Event Queue no to which the channel will be associated with
  *               (valied only if you are requesting for a DMA MasterChannel)
  *               Values : 0 to 7
- *                       -1 for Default queue
  * INPUT:   dev_id
  * OUTPUT:  *dma_ch_out
  *
@@ -726,311 +1395,388 @@ int __init arch_dma_init(void)
  *
  *****************************************************************************/
 int davinci_request_dma(int dev_id, const char *dev_name,
-			void (*callback) (int lch, unsigned short ch_status,
-					  void *data),
-			void *data, int *lch,
-			int *tcc, enum dma_event_q eventq_no)
-{
-
-	int ret_val = 0, i = 0;
-	static int req_flag = 0;
-	int temp_ch = 0;
-	/* checking the ARM side events */
-	if (dev_id >= 0 && (dev_id < DAVINCI_EDMA_NUM_DMACH)) {
-		if (!(edma_channels_arm[dev_id / 32] & (0x1 << (dev_id % 32)))) {
-			dev_dbg(&edma_dev.dev,
-				"dev_id = %d not supported on ARM side\r\n",
-				dev_id);
-			return -EINVAL;
-		}
-	} else if (dev_id >= DAVINCI_EDMA_NUM_DMACH
-		   && dev_id <=
-		   (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		if (!(qdma_channels_arm[0] &
-		      (0x1 << (dev_id - DAVINCI_EDMA_NUM_DMACH)))) {
-
-			dev_dbg(&edma_dev.dev,
-				"dev_id = %d not supported on ARM side\r\n",
-				dev_id);
-			return -EINVAL;
-		}
-	}
+            void (*callback) (int lch, unsigned short ch_status,
+                      void *data),
+            void *data, int *lch,
+            int *tcc, enum dma_event_q eventq_no)
+{
+
+    int ret_val = 0;
+    int param_id = 0;
+    int tcc_val = 0;
+
+    DMA_FN_IN;
+
+    /* Validating the arguments passed first */
+    if ((!lch) || (!tcc) || (eventq_no >= davinci_edma_num_evtq))   {
+        return -EINVAL;
+    }
+
+    if (dev_id >= 0u && dev_id < EDMA_NUM_DMACH) {
+
+        if (alloc_resource(dev_id, RES_DMA_CHANNEL) == dev_id)  {
+            *lch = dev_id;
+            DMA_PRINTK ("DMA channel %d allocated\r\n", *lch);
+
+            /*
+                Allocate PaRAM Set.
+                The 64 DMA Channels are mapped to the first 64 PARAM entries.
+            */
+            if (alloc_resource(dev_id, RES_PARAM_SET) == dev_id)    {
+                param_id = dev_id;
+                DMA_PRINTK ("PaRAM Set %d allocated\r\n", param_id);
+
+                spin_lock(&dma_chan_lock);
+                dma_ch_bound_res[dev_id].param_id = param_id;
+                spin_unlock(&dma_chan_lock);
+
+                /* Allocate TCC (1-to-1 mapped with the DMA channel) */
+                if (alloc_resource(dev_id, RES_TCC) == dev_id)  {
+                    *tcc = dev_id;
+                    DMA_PRINTK ("TCC %d allocated\r\n", *tcc);
+
+                    spin_lock(&dma_chan_lock);
+                    dma_ch_bound_res[dev_id].tcc = *tcc;
+                    spin_unlock(&dma_chan_lock);
+
+                    /* all resources allocated */
+                    /* Store the mapping b/w DMA channel and TCC first. */
+                    edma_dma_ch_tcc_mapping[*lch] = *tcc;
+
+                    /* Register callback function */
+                    register_callback((*tcc), callback, data);
+
+                    /* Map DMA channel to event queue */
+                    map_dma_ch_evt_queue(*lch, eventq_no);
+
+                    /* Map DMA channel to PaRAM Set */
+                    map_dma_ch_param_set(*lch, param_id);
+
+                    ret_val = 0;
+                } else {
+                    /* TCC allocation failed */
+                    /*free previously allocated resources*/
+                    free_resource(dev_id, RES_PARAM_SET);
+                    spin_lock(&dma_chan_lock);
+                    dma_ch_bound_res[dev_id].param_id = 0;
+                    spin_unlock(&dma_chan_lock);
+
+                    free_resource(dev_id, RES_DMA_CHANNEL);
+
+            DMA_PRINTK ("TCC allocation failed \r\n");
+                    ret_val = -EINVAL;
+                }
+            } else {
+                /* PaRAM Set allocation failed */
+                /*free previously allocated resources*/
+                free_resource(dev_id, RES_DMA_CHANNEL);
+
+                DMA_PRINTK ("PaRAM Set allocation  failed \r\n");
+                ret_val = -EINVAL;
+            }
+        } else {
+            /* Dma channel allocation failed */
+            DMA_PRINTK ("DMA channel allocation  failed \r\n");
+            ret_val = -EINVAL;
+        }
+    }   else if (dev_id >= EDMA_QDMA_CHANNEL_0
+                && dev_id <= EDMA_QDMA_CHANNEL_7) {
+            /**
+             * Allocate QDMA channel first.
+             * Modify the *lch to point it to the correct QDMA
+             * channel and then check whether the same channel
+             * has been allocated or not.
+             */
+            *lch = dev_id - EDMA_QDMA_CHANNEL_0;
+
+            if (alloc_resource((*lch), RES_QDMA_CHANNEL) == (*lch))    {
+                /* Requested Channel allocated successfully */
+                *lch = dev_id;
+                DMA_PRINTK ("QDMA channel %d allocated\r\n", (*lch));
+
+                /* Allocate param set */
+                param_id = alloc_resource (DAVINCI_EDMA_PARAM_ANY, RES_PARAM_SET);
+
+                if (param_id != -1) {
+                    DMA_PRINTK ("PaRAM Set %d allocated\r\n", param_id);
+
+                    spin_lock(&dma_chan_lock);
+                    dma_ch_bound_res[*lch].param_id = param_id;
+                    spin_unlock(&dma_chan_lock);
+
+                    /* allocate tcc */
+                    tcc_val = alloc_resource(*tcc, RES_TCC);
+
+                    if (tcc_val != -1) {
+                        DMA_PRINTK("TCC %d allocated\n", tcc_val);
+                        *tcc = tcc_val;
+
+                        spin_lock(&dma_chan_lock);
+                        dma_ch_bound_res[*lch].tcc = *tcc;
+                        spin_unlock(&dma_chan_lock);
+
+                        /* all resources allocated */
+                        /* Store the mapping b/w QDMA channel and TCC first. */
+                        edma_qdma_ch_tcc_mapping[(*lch) - EDMA_QDMA_CHANNEL_0]
+                                                        = *tcc;
+
+                        /* Register callback function */
+                        register_callback((*tcc), callback, data);
+
+                        /* Map QDMA channel to event queue */
+                        map_qdma_ch_evt_queue((*lch) - EDMA_QDMA_CHANNEL_0,
+                                                eventq_no);
+
+                        /* Map QDMA channel to PaRAM Set */
+                        map_qdma_ch_param_set((*lch) - EDMA_QDMA_CHANNEL_0,
+                                                param_id);
+
+                        ret_val = 0;
+                    } else {
+                        /* TCC allocation failed */
+                        /*free previously allocated resources*/
+                        free_resource(param_id, RES_PARAM_SET);
+                        spin_lock(&dma_chan_lock);
+                        dma_ch_bound_res[dev_id].param_id = 0;
+                        spin_unlock(&dma_chan_lock);
+
+                        free_resource((dev_id-EDMA_QDMA_CHANNEL_0),
+                                                        RES_QDMA_CHANNEL);
+
+                        DMA_PRINTK ("TCC channel allocation  failed \r\n");
+                        ret_val = -EINVAL;
+                    }
+                } else {
+                    /* PaRAM Set allocation failed. */
+                    /*free previously allocated resources*/
+                    free_resource((dev_id - EDMA_QDMA_CHANNEL_0),
+                                            RES_QDMA_CHANNEL);
+
+                    DMA_PRINTK ("PaRAM channel allocation  failed \r\n");
+                    ret_val = -EINVAL;
+                }
+            } else {
+                /* QDMA Channel allocation failed */
+                DMA_PRINTK ("QDMA channel allocation  failed \r\n");
+                ret_val = -EINVAL;
+            }
+    } else if (dev_id == EDMA_DMA_CHANNEL_ANY) {
+        *lch = alloc_resource(EDMA_DMA_CHANNEL_ANY, RES_DMA_CHANNEL);
+
+        if ((*lch) != -1)   {
+            DMA_PRINTK ("EDMA_DMA_CHANNEL_ANY::channel %d allocated\n", (*lch));
+
+            /* Allocate param set tied to the DMA channel (1-to-1 mapping) */
+            param_id = alloc_resource ((*lch), RES_PARAM_SET);
+
+            if (param_id != -1) {
+                DMA_PRINTK ("EDMA_DMA_CHANNEL_ANY::param %d allocated\n", param_id);
+
+                spin_lock(&dma_chan_lock);
+                dma_ch_bound_res[*lch].param_id = param_id;
+                spin_unlock(&dma_chan_lock);
+
+                /* allocate tcc */
+                *tcc = alloc_resource(*tcc, RES_TCC);
+
+                if (*tcc != -1) {
+                    DMA_PRINTK ("EDMA_DMA_CHANNEL_ANY:: tcc %d allocated\n", (*tcc));
+
+                    spin_lock(&dma_chan_lock);
+                    dma_ch_bound_res[*lch].tcc = *tcc;
+                    spin_unlock(&dma_chan_lock);
+
+                    /* all resources allocated */
+                    /* Store the mapping b/w DMA channel and TCC first. */
+                    edma_dma_ch_tcc_mapping[*lch] = *tcc;
+
+                    /* Register callback function */
+                    register_callback((*tcc), callback, data);
+
+                    /* Map DMA channel to event queue */
+                    map_dma_ch_evt_queue(*lch, eventq_no);
+
+                    /* Map DMA channel to PaRAM Set */
+                    map_dma_ch_param_set(*lch, param_id);
+
+                    ret_val = 0;
+                } else {
+                    /* free previously allocated resources */
+                    free_resource(param_id, RES_PARAM_SET);
+                    spin_lock(&dma_chan_lock);
+                    dma_ch_bound_res[*lch].param_id = 0;
+                    spin_unlock(&dma_chan_lock);
+
+                    free_resource((*lch), RES_DMA_CHANNEL);
+
+                    DMA_PRINTK ("free resource \r\n");
+                    ret_val = -EINVAL;
+                }
+            } else {
+                /*
+                    PaRAM Set allocation failed, free previously allocated
+                    resources.
+                */
+                DMA_PRINTK ("PaRAM Set allocation failed \r\n");
+                free_resource((*lch), RES_DMA_CHANNEL);
+                ret_val = -EINVAL;
+            }
+        } else {
+            DMA_PRINTK ("EINVAL \r\n");
+            ret_val = -EINVAL;
+        }
+    } else if (dev_id == EDMA_QDMA_CHANNEL_ANY) {
+        *lch = alloc_resource(dev_id, RES_QDMA_CHANNEL);
+
+        if ((*lch) != -1)   {
+            /* Channel allocated successfully */
+            *lch = ((*lch) + EDMA_QDMA_CHANNEL_0);
+
+            DMA_PRINTK ("EDMA_QDMA_CHANNEL_ANY::channel %d allocated\n", (*lch));
+
+            /* Allocate param set */
+            param_id = alloc_resource (DAVINCI_EDMA_PARAM_ANY, RES_PARAM_SET);
+
+            if (param_id != -1) {
+                DMA_PRINTK ("EDMA_QDMA_CHANNEL_ANY::param %d allocated \r\n",
+                                                    param_id);
+
+                spin_lock(&dma_chan_lock);
+                dma_ch_bound_res[*lch].param_id = param_id;
+                spin_unlock(&dma_chan_lock);
+
+                /* allocate tcc */
+                tcc_val = alloc_resource(*tcc, RES_TCC);
+
+                if (tcc_val != -1) {
+                    DMA_PRINTK ("EDMA_QDMA_CHANNEL_ANY:: tcc %d allocated\n",
+                                                                    tcc_val);
+                    *tcc = tcc_val;
+
+                    spin_lock(&dma_chan_lock);
+                    dma_ch_bound_res[*lch].tcc = *tcc;
+                    spin_unlock(&dma_chan_lock);
+
+                    /* all resources allocated */
+                    /* Store the mapping b/w QDMA channel and TCC first. */
+                    edma_qdma_ch_tcc_mapping[(*lch) - EDMA_QDMA_CHANNEL_0]
+                                                             = *tcc;
+
+                    /* Register callback function */
+                    register_callback((*tcc), callback, data);
+
+                    /* Map QDMA channel to event queue */
+                    map_qdma_ch_evt_queue((*lch) - EDMA_QDMA_CHANNEL_0,
+                                            eventq_no);
+
+                    /* Map QDMA channel to PaRAM Set */
+                    map_qdma_ch_param_set((*lch) - EDMA_QDMA_CHANNEL_0,
+                                            param_id);
+
+                    ret_val = 0;
+                } else {
+                    /* free previously allocated resources */
+                    free_resource(param_id, RES_PARAM_SET);
+                    spin_lock(&dma_chan_lock);
+                    dma_ch_bound_res[*lch].param_id = 0;
+                    spin_unlock(&dma_chan_lock);
+
+                    free_resource((dev_id-EDMA_QDMA_CHANNEL_0),
+                                            RES_QDMA_CHANNEL);
+
+                    ret_val = -EINVAL;
+                }
+            } else {
+                /*
+                PaRAM Set allocation failed, free previously allocated
+                resources.
+                */
+                free_resource((dev_id - EDMA_QDMA_CHANNEL_0), RES_QDMA_CHANNEL);
+                ret_val = -EINVAL;
+            }
+        } else {
+            /* QDMA Channel allocation failed */
+            ret_val = -EINVAL;
+        }
+    } else if (dev_id == DAVINCI_EDMA_PARAM_ANY) {
+        /* Allocate a PaRAM Set */
+        *lch = alloc_resource(dev_id, RES_PARAM_SET);
+        if ((*lch) != -1) {
+            DMA_PRINTK ("DAVINCI_EDMA_PARAM_ANY:: link channel %d allocated\n", (*lch));
+
+            /* link channel allocated */
+            spin_lock(&dma_chan_lock);
+            dma_ch_bound_res[*lch].param_id = *lch;
+            spin_unlock(&dma_chan_lock);
+
+            /* assign the link field to NO link. i.e 0xFFFF */
+            ptr_edmacc_regs->paramentry[*lch].link_bcntrld |= 0xFFFFu;
+
+            /*
+                Check whether user has passed a NULL TCC or not.
+                If it is not NULL, use that value to set the OPT.TCC field
+                of the link channel and enable the interrupts also.
+                Otherwise, disable the interrupts.
+            */
+            if (*tcc != -1) {
+                /* Check for the valid TCC */
+                if ((*tcc) < EDMA_NUM_TCC)   {
+                    /* Set the OPT.TCC field */
+                    ptr_edmacc_regs->paramentry[*lch].opt &= (~TCC);
+                    ptr_edmacc_regs->paramentry[*lch].opt |=
+                                            ((0x3F & (*tcc)) << 12);
+
+                    /* set TCINTEN bit in PARAM entry */
+                    ptr_edmacc_regs->paramentry[*lch].opt |= TCINTEN;
+
+                    /* Store the TCC also */
+                    spin_lock(&dma_chan_lock);
+                    dma_ch_bound_res[*lch].tcc = *tcc;
+                    spin_unlock(&dma_chan_lock);
+                } else {
+                    /* Invalid TCC passed. */
+                    ret_val = -EINVAL;
+                    return ret_val;
+                }
+            } else {
+                ptr_edmacc_regs->paramentry[*lch].opt &= ~TCINTEN;
+            }
+
+            ret_val = 0;
+            return ret_val;
+        } else {
+            ret_val = -EINVAL;
+        }
+    } else {
+	ret_val = -EINVAL;
+    }
+
+    if (!ret_val) {
+        if (callback) {
+            ptr_edmacc_regs->paramentry[param_id].opt &= (~TCC);
+            ptr_edmacc_regs->paramentry[param_id].opt |=
+                                    ((0x3F & (*tcc)) << 12);
+
+            /* set TCINTEN bit in PARAM entry */
+            ptr_edmacc_regs->paramentry[param_id].opt |= TCINTEN;
+        } else {
+            ptr_edmacc_regs->paramentry[param_id].opt &= ~TCINTEN;
+        }
+
+        /* assign the link field to NO link. i.e 0xFFFF */
+        ptr_edmacc_regs->paramentry[param_id].link_bcntrld |= 0xFFFF;
+    }
 
-	if ((dev_id != DAVINCI_DMA_CHANNEL_ANY)
-	    && (dev_id != DAVINCI_EDMA_PARAM_ANY)) {
-		if (dev_id >= DAVINCI_EDMA_NUM_DMACH
-		    &&
-		    dev_id < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)
-		    ) {
-			ptr_edmacc_regs->qrae[0] =
-			    ptr_edmacc_regs->qrae[0] |
-			    (1 << (dev_id - DAVINCI_EDMA_NUM_DMACH));
-		} else {
-			if (dev_id < 32) {
-				ptr_edmacc_regs->dra[0].drae =
-				    ptr_edmacc_regs->dra[0].drae |
-				    (1 << dev_id);
-			} else {
-				ptr_edmacc_regs->dra[0].draeh =
-				    ptr_edmacc_regs->dra[0].draeh |
-				    (1 << (dev_id - 32));
-			}
-		}
-	}
-
-	if (!req_flag) {
-		if (register_dma_interrupts
-		    (dma_irq_handler, dma_ccerr_handler,
-		     dma_tc1err_handler, dma_tc2err_handler)) {
-			dev_dbg(&edma_dev.dev,
-				"register_dma_interrupts failed\r\n");
-			return -EINVAL;
-		} else
-			req_flag = 1;
-	}
+    DMA_FN_OUT;
 
-	if (dev_id >= 0 && dev_id < (DAVINCI_EDMA_NUM_DMACH)) {
-		/* The 64 Channels are mapped to the first 64 PARAM entries */
-		if (!dma_chan[dev_id].in_use) {
-			*lch = dev_id;
-			dma_chan[*lch].param_no = request_param(*lch, dev_id);
-			if (dma_chan[*lch].param_no == -1) {
-				return -EINVAL;
-			} else
-				dev_dbg(&edma_dev.dev, "param_no=%d\r\n",
-					dma_chan[*lch].param_no);
-			if (callback) {
-				dma_chan[*lch].tcc =
-				    request_dma_interrupt(*lch, callback, data,
-							  dma_chan[*lch].
-							  param_no, *tcc);
-				if (dma_chan[*lch].tcc == -1) {
-					return -EINVAL;
-				} else {
-					*tcc = dma_chan[*lch].tcc;
-					dev_dbg(&edma_dev.dev, "tcc_no=%d\r\n",
-						dma_chan[*lch].tcc);
-				}
-			} else
-				dma_chan[*lch].tcc = -1;
-
-			map_dmach_queue(dev_id, eventq_no);
-			ret_val = 0;
-		} else
-			ret_val = -EINVAL;
-	}
-
-	else if (dev_id >= DAVINCI_EDMA_NUM_DMACH && dev_id <
-		 (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		if ((qdam_to_param_mapping[dev_id - DAVINCI_EDMA_NUM_DMACH] !=
-		     -1)
-		    &&
-		    (dma_chan
-		     [qdam_to_param_mapping[dev_id - DAVINCI_EDMA_NUM_DMACH]].
-		     in_use)
-		    ) {
-			ret_val = -EINVAL;
-		} else {
-			*lch = dev_id;
-			dma_chan[*lch].param_no = request_param(*lch, dev_id);
-			if (dma_chan[*lch].param_no == -1) {
-				dev_dbg(&edma_dev.dev, "request_param failed\r\n");
-				return -EINVAL;
-			} else {
-				dev_dbg(&edma_dev.dev, "param_no=%d\r\n",
-					dma_chan[*lch].param_no);
-				map_dmach_param(*lch, dma_chan[*lch].param_no);
-			}
-			if (callback) {
-				dma_chan[*lch].tcc =
-				    request_dma_interrupt(*lch, callback, data,
-							  dma_chan[*lch].
-							  param_no, *tcc);
-				if (dma_chan[*lch].tcc == -1) {
-					return -EINVAL;
-				} else {
-					*tcc = dma_chan[*lch].tcc;
-					dev_dbg(&edma_dev.dev, "tcc_no=%d\r\n",
-						dma_chan[*lch].tcc);
-				}
-			} else
-				dma_chan[*lch].tcc = -1;
-			map_dmach_queue(dev_id, eventq_no);
-			ret_val = 0;
-		}
-	} else if (dev_id == DAVINCI_DMA_CHANNEL_ANY) {
-		i = 0;
-		ret_val = 0;
-		while (dma_chan_no_event[i] != -1) {
-			if (!dma_chan[dma_chan_no_event[i]].in_use) {
-				*lch = dma_chan_no_event[i];
-				dma_chan[*lch].param_no =
-				    request_param(*lch, dev_id);
-				if (dma_chan[*lch].param_no == -1) {
-					return -EINVAL;
-				}
-				dev_dbg(&edma_dev.dev, "param_no=%d\r\n",
-					dma_chan[*lch].param_no);
-				if (dma_chan[*lch].param_no >=
-				    DAVINCI_EDMA_NUM_DMACH
-				    &&
-				    dma_chan[*lch].param_no <
-				    (DAVINCI_EDMA_NUM_DMACH +
-				     DAVINCI_EDMA_NUM_QDMACH)
-				    ) {
-
-					ptr_edmacc_regs->qrae[0] =
-					    ptr_edmacc_regs->qrae[0] |
-					    (1 << (dma_chan[*lch].param_no -
-						   DAVINCI_EDMA_NUM_DMACH));
-
-				} else {
-					if (dma_chan[*lch].param_no < 32) {
-						ptr_edmacc_regs->dra[0].drae =
-						    ptr_edmacc_regs->dra[0].drae
-						    |
-						    (1 << dma_chan[*lch].
-						     param_no);
-					} else {
-						ptr_edmacc_regs->dra[0].draeh =
-						    ptr_edmacc_regs->dra[0].
-						    draeh | (1 <<
-							     (dma_chan[*lch].
-							      param_no - 32));
-					}
-				}
-				if (callback) {
-					dma_chan[*lch].tcc =
-					    request_dma_interrupt(*lch,
-								  callback,
-								  data,
-								  dma_chan
-								  [*lch].
-								  param_no,
-								  *tcc);
-					if (dma_chan[*lch].tcc == -1) {
-						return -EINVAL;
-					} else {
-						*tcc = dma_chan[*lch].tcc;
-					}
-				} else {
-					dma_chan[*lch].tcc = -1;
-				}
-				map_dmach_queue(dev_id, eventq_no);
-				ret_val = 0;
-				break;
-			}
-			i++;
-		}
-	}
-
-	else if (dev_id == DAVINCI_EDMA_PARAM_ANY) {
-		ret_val = 0;
-		for (i = (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH);
-		     i < DAVINCI_EDMA_NUM_PARAMENTRY; i++) {
-			if (!dma_chan[i].in_use) {
-				dev_dbg(&edma_dev.dev, "any link = %d\r\n", i);
-				*lch = i;
-				dma_chan[*lch].param_no =
-				    request_param(*lch, dev_id);
-				if (dma_chan[*lch].param_no == -1) {
-					dev_dbg(&edma_dev.dev,
-						"request_param failed\r\n");
-					return -EINVAL;
-				} else {
-					dev_dbg(&edma_dev.dev, "param_no=%d\r\n",
-						dma_chan[*lch].param_no);
-				}
-				if (*tcc != -1)
-					dma_chan[*lch].tcc = *tcc;
-				else
-					dma_chan[*lch].tcc = -1;
-				ret_val = 0;
-				break;
-			}
-		}
-	} else {
-		ret_val = -EINVAL;
-	}
-	if (!ret_val) {
-		if (dev_id >= DAVINCI_EDMA_NUM_DMACH && dev_id <
-		    (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-			/* Master Channel */
-			qdam_to_param_mapping[dev_id -
-					      DAVINCI_EDMA_NUM_DMACH] =
-			    dma_chan[*lch].param_no;
-			LOCK;
-			/* It's used global data structure and used to find out
-			   whether channel is available or not */
-			dma_chan[qdam_to_param_mapping
-				 [dev_id - DAVINCI_EDMA_NUM_DMACH]].in_use = 1;
-			UNLOCK;
-			dma_chan[qdam_to_param_mapping
-				 [dev_id - DAVINCI_EDMA_NUM_DMACH]].dev_id =
-			    *lch;
-			dma_chan[qdam_to_param_mapping
-				 [dev_id - DAVINCI_EDMA_NUM_DMACH]].tcc =
-			    dma_chan[*lch].tcc;
-			temp_ch =
-			    qdam_to_param_mapping[dev_id -
-						  DAVINCI_EDMA_NUM_DMACH];
-			dma_chan[temp_ch].param_no = dma_chan[*lch].param_no;
-			if (dma_chan[*lch].tcc != -1) {
-				ptr_edmacc_regs->paramentry[dma_chan[temp_ch].
-							    param_no].opt &=
-				    (~TCC);
-				ptr_edmacc_regs->paramentry[dma_chan[temp_ch].
-							    param_no].opt |=
-				    ((0x3f & dma_chan[*lch].tcc) << 12);
-				/* set TCINTEN bit in PARAM entry */
-				ptr_edmacc_regs->
-				    paramentry[dma_chan[temp_ch].param_no].
-				    opt |= TCINTEN;
-			} else {
-				ptr_edmacc_regs->paramentry[dma_chan[temp_ch].
-							    param_no].opt &=
-				    ~TCINTEN;
-			}
-			/* assign the link field to no link. i.e 0xffff */
-			ptr_edmacc_regs->paramentry[dma_chan[temp_ch].
-						    param_no].
-			    link_bcntrld |= 0xffff;
-		} else {
-			/* Slave Channel */
-			LOCK;
-			/* Global structure to identify whether resoures is
-			   available or not */
-			dma_chan[*lch].in_use = 1;
-			UNLOCK;
-			dma_chan[*lch].dev_id = *lch;
-			if (dma_chan[*lch].tcc != -1) {
-				ptr_edmacc_regs->paramentry[dma_chan[*lch].
-							    param_no].opt &=
-				    (~TCC);
-				ptr_edmacc_regs->paramentry[dma_chan[*lch].
-							    param_no].opt |=
-				    ((0x3f & dma_chan[*lch].tcc) << 12);
-				/* set TCINTEN bit in PARAM entry */
-				ptr_edmacc_regs->paramentry[dma_chan[*lch].
-							    param_no].opt |=
-				    TCINTEN;
-			} else {
-				ptr_edmacc_regs->paramentry[dma_chan[*lch].
-							    param_no].opt &=
-				    ~TCINTEN;
-			}
-			/* assign the link field to no link. i.e 0xffff */
-			ptr_edmacc_regs->paramentry[dma_chan[*lch].
-						    param_no].
-			    link_bcntrld |= 0xffff;
-		}
-	}
-	return ret_val;
+    return ret_val;
 }
+EXPORT_SYMBOL(davinci_request_dma);
+
 
 /******************************************************************************
  *
- * DMA channel free: Free dma channle
+ * DMA channel free: Free dma channel
  * Arguments:
  *      dev_id     - request for the param entry device id
  *
@@ -1039,22 +1785,83 @@ int davinci_request_dma(int dev_id, cons
  *****************************************************************************/
 void davinci_free_dma(int lch)
 {
-	int temp_ch = 0;
-	if (lch >= DAVINCI_EDMA_NUM_DMACH && lch <
-	    (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch = qdam_to_param_mapping[lch - DAVINCI_EDMA_NUM_DMACH];
-		lch = temp_ch;
-	}
-	LOCK;
-	dma_chan[lch].in_use = 0;
-	UNLOCK;
-	free_param(dma_chan[lch].param_no);
-
-	if (lch >= 0
-	    && lch < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		free_dma_interrupt(dma_chan[lch].tcc);
-	}
+    int param_id=0;
+    int tcc=0;
+
+    DMA_FN_IN;
+    DMA_PRINTK("lch = %d\n", lch);
+
+    if (lch < EDMA_NUM_DMACH)   {
+        /* Disable any ongoing transfer first */
+        davinci_stop_dma(lch);
+
+        /* Un-register the callback function */
+        unregister_callback(lch, RES_DMA_CHANNEL);
+
+        /* Remove DMA channel to PaRAM Set mapping */
+        if (davinci_edma_chmap_exist == 1)  {
+            ptr_edmacc_regs->dchmap[lch] &= DMACH_PARAM_CLR_MASK;
+        }
+
+        param_id = dma_ch_bound_res[lch].param_id;
+        tcc = dma_ch_bound_res[lch].tcc;
+
+        DMA_PRINTK("Free ParamSet %d\n",param_id);
+        free_resource(param_id, RES_PARAM_SET);
+        spin_lock(&dma_chan_lock);
+        dma_ch_bound_res[lch].param_id=0;
+        spin_unlock(&dma_chan_lock);
+
+        DMA_PRINTK("Free TCC %d\n",tcc);
+        free_resource(tcc, RES_TCC);
+        spin_lock(&dma_chan_lock);
+        dma_ch_bound_res[lch].tcc=0;
+        spin_unlock(&dma_chan_lock);
+
+        DMA_PRINTK("Free DMA channel %d\n",lch);
+        free_resource(lch, RES_DMA_CHANNEL);
+    } else  if (lch >= EDMA_NUM_DMACH && lch < davinci_edma_num_param) {
+        param_id = dma_ch_bound_res[lch].param_id;
+
+        DMA_PRINTK("Free LINK channel %d\n", param_id);
+        free_resource(param_id, RES_PARAM_SET);
+        spin_lock(&dma_chan_lock);
+        dma_ch_bound_res[lch].param_id=0;
+        spin_unlock(&dma_chan_lock);
+    } else if (lch >= EDMA_QDMA_CHANNEL_0 && lch <= EDMA_QDMA_CHANNEL_7) {
+        /* Disable any ongoing transfer first */
+        davinci_stop_dma(lch);
+
+        /* Un-register the callback function */
+        unregister_callback(lch, RES_QDMA_CHANNEL);
+
+        /* Remove QDMA channel to PaRAM Set mapping */
+        ptr_edmacc_regs->qchmap[lch-EDMA_QDMA_CHANNEL_0] &= QDMACH_PARAM_CLR_MASK;
+        /* Reset trigger word */
+        ptr_edmacc_regs->qchmap[lch-EDMA_QDMA_CHANNEL_0] &= QDMACH_TRWORD_CLR_MASK;
+
+        param_id = dma_ch_bound_res[lch].param_id;
+        tcc = dma_ch_bound_res[lch].tcc;
+
+        DMA_PRINTK("Free ParamSet %d\n",param_id);
+        free_resource(param_id, RES_PARAM_SET);
+        spin_lock(&dma_chan_lock);
+        dma_ch_bound_res[lch].param_id=0;
+        spin_unlock(&dma_chan_lock);
+
+        DMA_PRINTK("Free TCC %d\n",tcc);
+        free_resource(tcc, RES_TCC);
+        spin_lock(&dma_chan_lock);
+        dma_ch_bound_res[lch].tcc=0;
+        spin_unlock(&dma_chan_lock);
+
+        DMA_PRINTK("Free QDMA channel %d\n",lch);
+        free_resource(lch - EDMA_QDMA_CHANNEL_0, RES_QDMA_CHANNEL);
+    }
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_free_dma);
+
 
 /******************************************************************************
  *
@@ -1063,32 +1870,41 @@ void davinci_free_dma(int lch)
  *      lch         - channel for which the source parameters to be configured
  *      src_port    - Source port address
  *      addressMode - indicates wether addressing mode is fifo.
+ *                      (In FIFO mode, address should be 32bytes aligned)
  *
  *****************************************************************************/
 void davinci_set_dma_src_params(int lch, unsigned long src_port,
-				enum address_mode mode, enum fifo_width width)
+                enum address_mode mode, enum fifo_width width)
 {
-	int temp_ch = 0;
-	if (lch >= DAVINCI_EDMA_NUM_DMACH && lch <
-	    (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch = qdam_to_param_mapping[lch - DAVINCI_EDMA_NUM_DMACH];
-		lch = temp_ch;
-	}
-	if (lch >= 0 && lch < DAVINCI_EDMA_NUM_PARAMENTRY) {
-		/* set the source port address
-		   in source register of param structure */
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].src =
-		    src_port;
-		/* set the fifo addressing mode */
-		if (mode) {	/* reset SAM and FWID */
-			ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].opt
-			    &= (~(SAM | EDMA_FWID));
-			/* set SAM and program FWID */
-			ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].opt
-			    |= (mode | ((width & 0x7) << 8));
-		}
-	}
+    DMA_FN_IN;
+
+    if (lch < edma_max_logical_ch)   {
+        int param_id = 0;
+        param_id = dma_ch_bound_res[lch].param_id;
+
+        if ((mode) && ((src_port & 0x1Fu) != 0))   {
+            /* Address in FIFO mode not 32 bytes aligned */
+            return;
+        }
+
+        /* set the source port address in source register of param structure */
+        ptr_edmacc_regs->paramentry[param_id].src = src_port;
+
+        /* set the fifo addressing mode */
+        if (mode) {
+            /* reset SAM and FWID */
+            ptr_edmacc_regs->paramentry[param_id].opt &= (~(SAM | EDMA_FWID));
+            /* set SAM and program FWID */
+            ptr_edmacc_regs->paramentry[param_id].opt
+                                    |= (SAM | ((width & 0x7) << 8));
+        }
+    }
+
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_set_dma_src_params);
+
+
 
 /******************************************************************************
  *
@@ -1097,33 +1913,41 @@ void davinci_set_dma_src_params(int lch,
  *    lch - channel or param device for destination parameters to be configured
  *    dest_port    - Destination port address
  *    addressMode  - indicates wether addressing mode is fifo.
+ *                      (In FIFO mode, address should be 32bytes aligned)
  *
  *****************************************************************************/
 void davinci_set_dma_dest_params(int lch, unsigned long dest_port,
-				 enum address_mode mode, enum fifo_width width)
+                 enum address_mode mode, enum fifo_width width)
 {
-	int temp_ch = 0;
-	if (lch >= DAVINCI_EDMA_NUM_DMACH && lch <
-	    (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch = qdam_to_param_mapping[lch - DAVINCI_EDMA_NUM_DMACH];
-		lch = temp_ch;
-	}
-	if (lch >= 0 && lch < DAVINCI_EDMA_NUM_PARAMENTRY) {
-		/* set the destination port address
-		   in dest register of param structure */
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].dst =
-		    dest_port;
-		/* set the fifo addressing mode */
-		if (mode) {	/* reset DAM and FWID */
-
-			ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].opt
-			    &= (~(DAM | EDMA_FWID));
-			/* set DAM and program FWID */
-			ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].opt
-			    |= ((mode << 1) | ((width & 0x7) << 8));
-		}
-	}
+    DMA_FN_IN;
+
+    if (lch < edma_max_logical_ch)   {
+        int param_id = 0;
+        param_id=dma_ch_bound_res[lch].param_id;
+
+        if ((mode) && ((dest_port & 0x1Fu) != 0))   {
+            /* Address in FIFO mode not 32 bytes aligned */
+            return;
+        }
+
+        /* set the dest port address in dest register of param structure */
+        ptr_edmacc_regs->paramentry[param_id].dst = dest_port;
+
+        /* set the fifo addressing mode */
+        if (mode) {
+            /* reset DAM and FWID */
+            ptr_edmacc_regs->paramentry[param_id].opt &= (~(DAM | EDMA_FWID));
+            /* set DAM and program FWID */
+            ptr_edmacc_regs->paramentry[param_id].opt
+                                    |= (DAM | ((width & 0x7) << 8));
+        }
+    }
+
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_set_dma_dest_params);
+
+
 
 /******************************************************************************
  *
@@ -1136,24 +1960,31 @@ void davinci_set_dma_dest_params(int lch
  *****************************************************************************/
 void davinci_set_dma_src_index(int lch, short src_bidx, short src_cidx)
 {
-	int temp_ch = 0;
-	if (lch >= DAVINCI_EDMA_NUM_DMACH && lch <
-	    (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch = qdam_to_param_mapping[lch - DAVINCI_EDMA_NUM_DMACH];
-		lch = temp_ch;
-	}
+    DMA_FN_IN;
 
-	if (lch >= 0 && lch < DAVINCI_EDMA_NUM_PARAMENTRY) {
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].src_dst_bidx
-		    &= 0xffff0000;
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].src_dst_bidx
-		    |= src_bidx;
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].src_dst_cidx
-		    &= 0xffff0000;
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].src_dst_cidx
-		    |= src_cidx;
-	}
+    if (lch < edma_max_logical_ch) {
+        int param_id = 0;
+
+        param_id = dma_ch_bound_res[lch].param_id;
+
+        DMA_PRINTK("lch = %d, param_id = %d\n", lch, param_id);
+
+        ptr_edmacc_regs->paramentry[param_id].src_dst_bidx
+                                        &= 0xffff0000;
+        ptr_edmacc_regs->paramentry[param_id].src_dst_bidx
+                                        |= (src_bidx & 0xFFFFu);
+
+        ptr_edmacc_regs->paramentry[param_id].src_dst_cidx
+                                        &= 0xffff0000;
+        ptr_edmacc_regs->paramentry[param_id].src_dst_cidx
+                                        |= (src_cidx & 0xFFFFu);
+    }
+
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_set_dma_src_index);
+
+
 
 /******************************************************************************
  *
@@ -1166,23 +1997,31 @@ void davinci_set_dma_src_index(int lch, 
  *****************************************************************************/
 void davinci_set_dma_dest_index(int lch, short dest_bidx, short dest_cidx)
 {
-	int temp_ch = 0;
-	if (lch >= DAVINCI_EDMA_NUM_DMACH && lch <
-	    (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch = qdam_to_param_mapping[lch - DAVINCI_EDMA_NUM_DMACH];
-		lch = temp_ch;
-	}
-	if (lch >= 0 && lch < DAVINCI_EDMA_NUM_PARAMENTRY) {
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].src_dst_bidx
-		    &= 0x0000ffff;
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].src_dst_bidx
-		    |= ((unsigned long)dest_bidx << 16);
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].src_dst_cidx
-		    &= 0x0000ffff;
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].src_dst_cidx
-		    |= ((unsigned long)dest_cidx << 16);
-	}
+    DMA_FN_IN;
+
+    if (lch < edma_max_logical_ch) {
+        int param_id = 0;
+
+        param_id = dma_ch_bound_res[lch].param_id;
+
+        DMA_PRINTK("lch = %d, param_id = %d\n", lch, param_id);
+
+        ptr_edmacc_regs->paramentry[param_id].src_dst_bidx
+                                        &= 0x0000ffff;
+        ptr_edmacc_regs->paramentry[param_id].src_dst_bidx
+                                        |= ((unsigned long)dest_bidx << 16);
+
+        ptr_edmacc_regs->paramentry[param_id].src_dst_cidx
+                                        &= 0x0000ffff;
+        ptr_edmacc_regs->paramentry[param_id].src_dst_cidx
+                                        |= ((unsigned long)dest_cidx << 16);
+    }
+
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_set_dma_dest_index);
+
+
 
 /******************************************************************************
  *
@@ -1196,33 +2035,43 @@ void davinci_set_dma_dest_index(int lch,
  *
  *****************************************************************************/
 void davinci_set_dma_transfer_params(int lch, unsigned short acnt,
-				     unsigned short bcnt, unsigned short ccnt,
-				     unsigned short bcntrld,
-				     enum sync_dimension sync_mode)
-{
-	int temp_ch = 0;
-	if (lch >= DAVINCI_EDMA_NUM_DMACH && lch <
-	    (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch = qdam_to_param_mapping[lch - DAVINCI_EDMA_NUM_DMACH];
-		lch = temp_ch;
-	}
-	if (lch >= 0 && lch < DAVINCI_EDMA_NUM_PARAMENTRY) {
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].link_bcntrld
-		    &= 0x0000ffff;
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].link_bcntrld
-		    |= (bcntrld << 16);
-		if (sync_mode == ASYNC)
-			ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].opt
-			    &= (~SYNCDIM);
-		else
-			ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].opt
-			    |= SYNCDIM;
-		/* Set the acount, bcount, ccount registers */
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].a_b_cnt =
-		    (bcnt << 16) | acnt;
-		ptr_edmacc_regs->paramentry[dma_chan[lch].param_no].ccnt = ccnt;
-	}
+                     unsigned short bcnt, unsigned short ccnt,
+                     unsigned short bcntrld,
+                     enum sync_dimension sync_mode)
+{
+        int param_id = 0;
+    DMA_FN_IN;
+
+    if (lch < edma_max_logical_ch) {
+
+        param_id = dma_ch_bound_res[lch].param_id;
+
+        DMA_PRINTK("lch = %d, param_id = %d\n", lch, param_id);
+
+        ptr_edmacc_regs->paramentry[param_id].link_bcntrld
+                                        &= 0x0000ffff;
+        ptr_edmacc_regs->paramentry[param_id].link_bcntrld
+                                        |= ((bcntrld & 0xFFFFu) << 16);
+
+        if (sync_mode == ASYNC)     {
+            ptr_edmacc_regs->paramentry[param_id].opt
+                &= (~SYNCDIM);
+        } else {
+            ptr_edmacc_regs->paramentry[param_id].opt
+                |= SYNCDIM;
+        }
+
+        /* Set the acount, bcount, ccount registers */
+        ptr_edmacc_regs->paramentry[param_id].a_b_cnt =
+            (((bcnt & 0xFFFFu) << 16) | acnt);
+        ptr_edmacc_regs->paramentry[param_id].ccnt = ccnt;
+    }
+
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_set_dma_transfer_params);
+
+
 
 /******************************************************************************
  *
@@ -1231,21 +2080,26 @@ void davinci_set_dma_transfer_params(int
  *      lch - logical channel number
  *
  *****************************************************************************/
-void davinci_set_dma_params(int lch, edmacc_paramentry_regs * temp)
+void davinci_set_dma_params(int lch, edmacc_paramentry_regs *new_param)
 {
-	int temp_ch = 0;
-	if (lch >= DAVINCI_EDMA_NUM_DMACH && lch <
-	    (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch = qdam_to_param_mapping[lch - DAVINCI_EDMA_NUM_DMACH];
-		lch = temp_ch;
-	}
-	if (lch >= 0 && lch < DAVINCI_EDMA_NUM_PARAMENTRY) {
-		memcpy((void *)
-		       &(ptr_edmacc_regs->
-			 paramentry[dma_chan[lch].param_no].opt),
-		       (void *)temp, sizeof(edmacc_paramentry_regs));
-	}
+            int param_id = 0;
+    DMA_FN_IN;
+
+    if (new_param)   {
+        if (lch < edma_max_logical_ch) {
+
+            param_id = dma_ch_bound_res[lch].param_id;
+
+            memcpy((void *)(&(ptr_edmacc_regs->paramentry[param_id].opt)),
+                   (void *)new_param, sizeof(edmacc_paramentry_regs));
+        }
+    }
+
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_set_dma_params);
+
+
 
 /******************************************************************************
  *
@@ -1254,90 +2108,102 @@ void davinci_set_dma_params(int lch, edm
  *      lch - logical channel number
  *
  *****************************************************************************/
-void davinci_get_dma_params(int lch, edmacc_paramentry_regs * temp)
+void davinci_get_dma_params(int lch, edmacc_paramentry_regs *curr_param)
 {
-	int temp_ch = 0;
-	if (lch >= DAVINCI_EDMA_NUM_DMACH && lch <
-	    (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch = qdam_to_param_mapping[lch - DAVINCI_EDMA_NUM_DMACH];
-		lch = temp_ch;
-	}
-	if (lch >= 0 && lch < DAVINCI_EDMA_NUM_PARAMENTRY) {
-		memcpy((void *)temp,
-		       (void *)&(ptr_edmacc_regs->
-				 paramentry[dma_chan[lch].param_no].opt),
-		       sizeof(edmacc_paramentry_regs));
-	}
+    DMA_FN_IN;
+
+    if (curr_param)  {
+        if (lch < edma_max_logical_ch) {
+            int param_id = 0;
+
+            param_id = dma_ch_bound_res[lch].param_id;
+
+
+            memcpy((void *)curr_param,
+                    (void *)(&(ptr_edmacc_regs->paramentry[param_id].opt)),
+                        sizeof(edmacc_paramentry_regs));
+        }
+    }
+
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_get_dma_params);
+
+
 
 /******************************************************************************
  *
- * DMA Strat - Starts the dma on the channel passed
+ * DMA Start - Starts the dma on the channel passed
  * ARGUMENTS:
  *      lch - logical channel number
  *
  *****************************************************************************/
 int davinci_start_dma(int lch)
 {
-	int ret_val;
-	if (lch >= 0 && (lch < DAVINCI_EDMA_NUM_DMACH)) {
-		int i = 0;
-		int flag = 0;
-		/* If the dma start request is for the unused events */
-		while (dma_chan_no_event[i] != -1) {
-			if (dma_chan_no_event[i] == lch) {
-				/* EDMA channels without event association */
-				dev_dbg(&edma_dev.dev, "ESR=%x\r\n",
-					ptr_edmacc_regs->shadow[0].esr);
-
-				(lch < 32) ?
-				    (ptr_edmacc_regs->shadow[0].esr |=
-				     (1UL << lch)) : (ptr_edmacc_regs->
-						      shadow[0].esrh |=
-						      (1UL << (lch - 32)));
-				flag = 1;
-				ret_val = 0;
-				break;
-			}
-			i++;
-		}
-		if (!flag) {
-			/* EDMA channel with event association */
-			dev_dbg(&edma_dev.dev, "ER=%d\r\n",
-				ptr_edmacc_regs->shadow[0].er);
-			/* Clear any pedning error */
-			(lch < 32) ?
-			    (ptr_edmacc_regs->emcr |=
-			     (1UL << lch)) :
-			    (ptr_edmacc_regs->emcrh |= (1UL << (lch - 32)));
-			/* Clear any SER */
-			(lch < 32) ?
-			    (ptr_edmacc_regs->shadow[0].secr |=
-			     (1UL << lch)) :
-			    (ptr_edmacc_regs->shadow[0].secrh |=
-			     (1UL << (lch - 32)));
-
-			(lch < 32) ?
-			    (ptr_edmacc_regs->shadow[0].eesr |=
-			     (1UL << lch)) :
-			    (ptr_edmacc_regs->shadow[0].eesrh |=
-			     (1UL << (lch - 32)));
-
-			dev_dbg(&edma_dev.dev, "EER=%d\r\n",
-				ptr_edmacc_regs->shadow[0].eer);
-			ret_val = 0;
-		}
-	} else if ((lch >= DAVINCI_EDMA_NUM_DMACH)
-		   && (lch <
-		       (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH))) {
-		ptr_edmacc_regs->shadow[0].qeesr |=
-		    (1 << (lch - DAVINCI_EDMA_NUM_DMACH));
-		ret_val = 0;
-	} else {		/* for slaveChannels */
-		ret_val = EINVAL;
-	}
-	return ret_val;
+    int ret_val = 0;
+    DMA_FN_IN;
+
+    DMA_PRINTK ("lch %d edma_max_logical_ch %d\n", lch,edma_max_logical_ch);
+    if (lch < edma_max_logical_ch)   {
+        if (lch >= 0 && lch < EDMA_NUM_DMACH)   {
+            /* DMA Channel */
+                DMA_PRINTK ("davinci_dma_ch_hw_event_map 0x%8x \n",davinci_dma_ch_hw_event_map[lch/32u]);
+                DMA_PRINTK ("lch/32u  %i  lch-32u %i\n",lch/32u,(lch % 32u));
+            if (((davinci_dma_ch_hw_event_map[lch/32u]) &
+                    (1u << (lch % 32u))) != 0)    {
+               DMA_PRINTK ("Event mapped\n");
+                /* Event Mapped */
+                if (lch < 32)   {
+                    DMA_PRINTK ("ER = %x\r\n", ptr_edmacc_shadow_regs->er);
+
+                    /* Clear any SER */
+                    ptr_edmacc_shadow_regs->secr = (1UL << lch);
+
+                    /* Clear any pending error */
+                    ptr_edmacc_regs->emcr = (1UL << lch);
+
+                    /* Set EER */
+                    ptr_edmacc_shadow_regs->eesr = (1UL << lch);
+
+                    ptr_edmacc_shadow_regs->ecr = (1UL << lch);
+                } else {
+                    DMA_PRINTK ("ERH = %x\r\n", ptr_edmacc_shadow_regs->erh);
+
+                    /* Clear any pending error */
+                    ptr_edmacc_regs->emcrh = (1UL << (lch-32));
+
+                    /* Clear any SERH */
+                    ptr_edmacc_shadow_regs->secrh = (1UL << (lch-32));
+
+                    /* Set EERH */
+                    ptr_edmacc_shadow_regs->eesrh = (1UL << (lch-32));
+
+                    ptr_edmacc_shadow_regs->ecrh = (1UL << (lch-32));
+                }
+            } else {
+                /* Manual Triggered */
+                if (lch < 32)   {
+                    ptr_edmacc_shadow_regs->esr = (1UL << lch);
+                } else {
+                    ptr_edmacc_shadow_regs->esrh = (1UL << (lch-32));
+                }
+            }
+        } else {
+            /* QDMA Channel */
+            ptr_edmacc_shadow_regs->qeesr =
+                                    (1u << (lch - EDMA_QDMA_CHANNEL_0));
+        }
+    } else {
+        DMA_PRINTK ("EINVAL lch %d \n", lch);
+        ret_val = EINVAL;
+    }
+
+    DMA_FN_OUT;
+
+    return ret_val;
 }
+EXPORT_SYMBOL(davinci_start_dma);
+
 
 /******************************************************************************
  *
@@ -1348,96 +2214,95 @@ int davinci_start_dma(int lch)
  *****************************************************************************/
 void davinci_stop_dma(int lch)
 {
-	if (lch < DAVINCI_EDMA_NUM_DMACH) {
-		int flag = 0;
-		int i = 0;
-		/* If the dma stop request is for the unused events */
-		while (dma_chan_no_event[i] != -1) {
-			if (dma_chan_no_event[i] == lch) {
-				/* EDMA channels without event association */
-				/* if the requested channel is one of the
-				   unused channels then reset the coresponding
-				   bit of ESR-Event Set Register */
-				flag = 1;
-				break;
-			}
-			i++;
-		}
-		if (!flag) {
-			/* EDMA channel with event association */
-			(lch < 32) ? (ptr_edmacc_regs->shadow[0].eecr |=
-				      (1UL << lch)) :
-			    (ptr_edmacc_regs->shadow[0].eecrh |=
-			     (1UL << (lch - 32)));
-			if (lch < 32) {
-				if (ptr_edmacc_regs->shadow[0].er & (1 << lch)) {
-					dev_dbg(&edma_dev.dev, "ER=%x\n",
-						ptr_edmacc_regs->shadow[0].er);
-					ptr_edmacc_regs->shadow[0].ecr |=
-					    (1 << lch);
-				}
-			} else {
-				if (ptr_edmacc_regs->shadow[0].erh
-				    & (1 << (lch - 32))) {
-					dev_dbg(&edma_dev.dev, "ERH=%x\n",
-						ptr_edmacc_regs->shadow[0].erh);
-					ptr_edmacc_regs->shadow[0].ecrh |=
-					    (1 << (lch - 32));
-				}
-			}
-			if (lch < 32) {
-				if (ptr_edmacc_regs->shadow[0].ser & (1 << lch)) {
-					dev_dbg(&edma_dev.dev, "SER=%x\n",
-						ptr_edmacc_regs->shadow[0].ser);
-					ptr_edmacc_regs->shadow[0].secr |=
-					    (1 << lch);
-				} else {
-				}
-			} else {
-				if (ptr_edmacc_regs->
-				    shadow[0].serh & (1 << (lch - 32))) {
-					dev_dbg(&edma_dev.dev, "SERH=%x\n",
-						ptr_edmacc_regs->shadow[0].
-						serh);
-					ptr_edmacc_regs->shadow[0].secrh |=
-					    (1 << (lch - 32));
-				}
-			}
-			if (lch < 32) {
-				if (ptr_edmacc_regs->emr & (1 << lch)) {
-					dev_dbg(&edma_dev.dev, "EMR=%x\n",
-						ptr_edmacc_regs->emr);
-					ptr_edmacc_regs->emcr |= (1 << lch);
-				}
-			} else {
-				if (ptr_edmacc_regs->emrh & (1 << (lch - 32))) {
-					dev_dbg(&edma_dev.dev, "EMRH=%x\n",
-						ptr_edmacc_regs->emrh);
-					ptr_edmacc_regs->emcrh |=
-					    (1 << (lch - 32));
-				}
-			}
-			dev_dbg(&edma_dev.dev, "EER=%d\r\n",
-				ptr_edmacc_regs->shadow[0].eer);
-			/* if the requested channel is one of the event channels
-			   then just set the link field of the corresponding
-			   param entry to 0xffff */
-		}
-	} else if ((lch >= DAVINCI_EDMA_NUM_DMACH)
-		   &&
-		   (lch < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH))) {
-		/* for QDMA channels */
-		ptr_edmacc_regs->qeecr |= (1 << (lch - DAVINCI_EDMA_NUM_DMACH));
-		dev_dbg(&edma_dev.dev, "QER=%d\r\n", ptr_edmacc_regs->qer);
-		dev_dbg(&edma_dev.dev, "QEER=%d\r\n", ptr_edmacc_regs->qeer);
-	} else if ((lch >= (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH))
-		   && lch < DAVINCI_EDMA_NUM_PARAMENTRY) {
-		/* for slaveChannels */
-		ptr_edmacc_regs->paramentry[lch].link_bcntrld &= 0xffff0000;
-		ptr_edmacc_regs->paramentry[lch].link_bcntrld |= 0xffff;
-	} else {
-	}
+    DMA_FN_IN;
+
+    if (lch < edma_max_logical_ch)   {
+        if (lch >= 0 && lch < EDMA_NUM_DMACH)   {
+            /* DMA Channel */
+            if (((davinci_dma_ch_hw_event_map[lch/32u]) &
+                    (1u << (lch%32u))) != 0)    {
+                /* Event Mapped */
+                if (lch < 32)   {
+                    ptr_edmacc_shadow_regs->eecr = (1UL << lch);
+                    if (ptr_edmacc_shadow_regs->er & (1 << lch)) {
+                        DMA_PRINTK ("ER=%x\n", ptr_edmacc_shadow_regs->er);
+
+                        ptr_edmacc_shadow_regs->ecr = (1 << lch);
+                    }
+
+                    if (ptr_edmacc_shadow_regs->ser & (1 << lch)) {
+                        DMA_PRINTK ("SER=%x\n", ptr_edmacc_shadow_regs->ser);
+
+                        ptr_edmacc_shadow_regs->secr = (1 << lch);
+                    }
+
+                    if (ptr_edmacc_regs->emr & (1 << lch)) {
+                        DMA_PRINTK ("EMR=%x\n", ptr_edmacc_regs->emr);
+
+                        ptr_edmacc_regs->emcr = (1 << lch);
+                    }
+                } else {
+                    ptr_edmacc_shadow_regs->eecrh = (1UL << (lch-32));
+                    if (ptr_edmacc_shadow_regs->erh & (1 << (lch-32))) {
+                        DMA_PRINTK ("ERH=%x\n", ptr_edmacc_shadow_regs->erh);
+
+                        ptr_edmacc_shadow_regs->ecrh = (1 << (lch-32));
+                    }
+
+                    if (ptr_edmacc_shadow_regs->serh & (1 << (lch-32))) {
+                        DMA_PRINTK ("SERH=%x\n", ptr_edmacc_shadow_regs->serh);
+
+                        ptr_edmacc_shadow_regs->secrh = (1 << (lch-32));
+                    }
+
+                    if (ptr_edmacc_regs->emrh & (1 << (lch-32))) {
+                        DMA_PRINTK ("EMRH=%x\n", ptr_edmacc_regs->emrh);
+
+                        ptr_edmacc_regs->emcrh = (1 << (lch-32));
+                    }
+                }
+            } else {    /* Manual Triggered */
+                if (lch < 32)   {
+                    DMA_PRINTK ("ESR=%x\r\n", ptr_edmacc_shadow_regs->esr);
+
+                    if (ptr_edmacc_shadow_regs->ser & (1 << lch)) {
+                        DMA_PRINTK ("SER=%x\n", ptr_edmacc_shadow_regs->ser);
+
+                        ptr_edmacc_shadow_regs->secr = (1 << lch);
+                    }
+
+                    if (ptr_edmacc_regs->emr & (1 << lch)) {
+                        DMA_PRINTK ("EMR=%x\n", ptr_edmacc_regs->emr);
+
+                        ptr_edmacc_regs->emcr = (1 << lch);
+                    }
+                } else {
+                    DMA_PRINTK ("ESRH=%x\r\n", ptr_edmacc_shadow_regs->esrh);
+
+                    if (ptr_edmacc_shadow_regs->serh & (1 << (lch-32))) {
+                        DMA_PRINTK ("SERH=%x\n", ptr_edmacc_shadow_regs->serh);
+
+                        ptr_edmacc_shadow_regs->secrh = (1 << (lch-32));
+                    }
+
+                    if (ptr_edmacc_regs->emrh & (1 << (lch-32))) {
+                        DMA_PRINTK ("EMRH=%x\n", ptr_edmacc_regs->emrh);
+
+                        ptr_edmacc_regs->emcrh = (1 << (lch-32));
+                    }
+                }
+            }
+        } else if ((lch <= EDMA_QDMA_CHANNEL_7)&&(lch >= EDMA_QDMA_CHANNEL_0) ) {
+            /* QDMA Channel */
+            ptr_edmacc_shadow_regs->qeecr = (1 << (lch - EDMA_QDMA_CHANNEL_0));
+        }
+    }
+
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_stop_dma);
+
+
 
 /******************************************************************************
  *
@@ -1452,41 +2317,28 @@ void davinci_stop_dma(int lch)
  *****************************************************************************/
 void davinci_dma_link_lch(int lch_head, int lch_queue)
 {
-	unsigned long link;
-	int temp_ch = 0;
-	if (lch_head >=
-	    DAVINCI_EDMA_NUM_DMACH
-	    && lch_head < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch =
-		    qdam_to_param_mapping[lch_head - DAVINCI_EDMA_NUM_DMACH];
-		lch_head = temp_ch;
-	}
-	if (lch_queue >=
-	    DAVINCI_EDMA_NUM_DMACH
-	    && lch_queue < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch =
-		    qdam_to_param_mapping[lch_queue - DAVINCI_EDMA_NUM_DMACH];
-		lch_queue = temp_ch;
-	}
-	if ((lch_head >= 0 && lch_head < DAVINCI_EDMA_NUM_PARAMENTRY)
-	    && (lch_queue >= 0 && lch_queue < DAVINCI_EDMA_NUM_PARAMENTRY)) {
-		/* program LINK */
-		link =
-		    (unsigned
-		     long)(&
-			   (ptr_edmacc_regs->
-			    paramentry[dma_chan[lch_queue].param_no].opt));
-		ptr_edmacc_regs->
-		    paramentry[dma_chan
-			       [lch_head].param_no].link_bcntrld &= 0xffff0000;
-		ptr_edmacc_regs->
-		    paramentry[dma_chan
-			       [lch_head].
-			       param_no].link_bcntrld |= ((unsigned short)
-							  link);
-		dma_chan[lch_head].link_lch = lch_queue;
-	}
+    DMA_FN_IN;
+
+    if ((lch_head < edma_max_logical_ch)
+        || (lch_queue < edma_max_logical_ch))    {
+        unsigned int param1_id = 0;
+        unsigned int param2_id = 0;
+        unsigned long link;
+
+        param1_id = dma_ch_bound_res[lch_head].param_id;
+        param2_id = dma_ch_bound_res[lch_queue].param_id;
+
+        link = (unsigned long)(&(ptr_edmacc_regs->paramentry[param2_id].opt));
+
+        ptr_edmacc_regs->paramentry[param1_id].link_bcntrld &= 0xffff0000;
+        ptr_edmacc_regs->paramentry[param1_id].link_bcntrld |= ((unsigned short)link);
+    }
+
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_dma_link_lch);
+
+
 
 /******************************************************************************
  *
@@ -1500,70 +2352,55 @@ void davinci_dma_link_lch(int lch_head, 
  *****************************************************************************/
 void davinci_dma_unlink_lch(int lch_head, int lch_queue)
 {
-	int temp_ch = 0;
-	if (lch_head >=
-	    DAVINCI_EDMA_NUM_DMACH
-	    && lch_head < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch =
-		    qdam_to_param_mapping[lch_head - DAVINCI_EDMA_NUM_DMACH];
-		lch_head = temp_ch;
-	}
-	if (lch_queue >=
-	    DAVINCI_EDMA_NUM_DMACH
-	    && lch_queue < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch =
-		    qdam_to_param_mapping[lch_queue - DAVINCI_EDMA_NUM_DMACH];
-		lch_queue = temp_ch;
-	}
-	if ((lch_head >= 0 && lch_head < DAVINCI_EDMA_NUM_PARAMENTRY)
-	    && (lch_queue >= 0 && lch_queue < DAVINCI_EDMA_NUM_PARAMENTRY)) {
-		ptr_edmacc_regs->
-		    paramentry[dma_chan
-			       [lch_head].param_no].link_bcntrld |= 0xffff;
-		dma_chan[lch_head].link_lch = -1;
-	}
+    DMA_FN_IN;
+
+    if ((lch_head < edma_max_logical_ch)
+        || (lch_queue < edma_max_logical_ch))    {
+        unsigned int param_id = 0;
+
+        param_id = dma_ch_bound_res[lch_head].param_id;
+
+        ptr_edmacc_regs->paramentry[param_id].link_bcntrld |= 0xFFFF;
+    }
+
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_dma_unlink_lch);
+
+
 
 /******************************************************************************
  *
  * DMA channel chain - chains the two logical channels passed through by
  * ARGUMENTS:
- * lch_head - logical channel number, from which the link field is to be removed
- * lch_queue - logical channel number or the param entry number, which is to be
- *             unlinked from lch_head
+ * lch_head - logical channel number, which will trigger the chained channel
+ *              'lch_queue'
+ * lch_queue - logical channel number which will be triggered by 'lch_head'
  *
  *****************************************************************************/
 void davinci_dma_chain_lch(int lch_head, int lch_queue)
 {
-	int temp_ch = 0;
-	if (lch_head >=
-	    DAVINCI_EDMA_NUM_DMACH
-	    && lch_head < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch =
-		    qdam_to_param_mapping[lch_head - DAVINCI_EDMA_NUM_DMACH];
-		lch_head = temp_ch;
-	}
-	if (lch_queue >=
-	    DAVINCI_EDMA_NUM_DMACH
-	    && lch_queue < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch =
-		    qdam_to_param_mapping[lch_queue - DAVINCI_EDMA_NUM_DMACH];
-		lch_queue = temp_ch;
-	}
-	if ((lch_head >= 0
-	     && lch_head < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH))
-	    &&
-	    (lch_queue >= 0
-	     && lch_queue < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH))
-	    ) {			/* set TCCHEN */
-		/* set TCCHEN */
-		ptr_edmacc_regs->paramentry[lch_head].opt |= TCCHEN;
-		/* program tcc */
-		ptr_edmacc_regs->paramentry[lch_head].opt &= (~TCC);
-		ptr_edmacc_regs->
-		    paramentry[lch_head].opt |= (lch_queue & 0x3f) << 12;
-	}
+    DMA_FN_IN;
+
+    if ((lch_head < edma_max_logical_ch)
+        || (lch_queue < edma_max_logical_ch))    {
+        unsigned int param_id = 0;
+
+        param_id = dma_ch_bound_res[lch_head].param_id;
+
+        /* set TCCHEN */
+        ptr_edmacc_regs->paramentry[param_id].opt |= TCCHEN;
+
+        /* program tcc */
+        ptr_edmacc_regs->paramentry[param_id].opt &= (~TCC);
+        ptr_edmacc_regs->paramentry[param_id].opt |= (lch_queue & 0x3f) << 12;
+    }
+
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_dma_chain_lch);
+
+
 
 /******************************************************************************
  *
@@ -1576,30 +2413,25 @@ void davinci_dma_chain_lch(int lch_head,
  *****************************************************************************/
 void davinci_dma_unchain_lch(int lch_head, int lch_queue)
 {
-	int temp_ch = 0;
-	if (lch_head >=
-	    DAVINCI_EDMA_NUM_DMACH
-	    && lch_head < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch =
-		    qdam_to_param_mapping[lch_head - DAVINCI_EDMA_NUM_DMACH];
-		lch_head = temp_ch;
-	}
-	if (lch_queue >=
-	    DAVINCI_EDMA_NUM_DMACH
-	    && lch_queue < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH)) {
-		temp_ch =
-		    qdam_to_param_mapping[lch_queue - DAVINCI_EDMA_NUM_DMACH];
-		lch_queue = temp_ch;
-	}
-	if ((lch_head >= 0
-	     && lch_head < (DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH))
-	    && (lch_queue >= 0
-		&& lch_queue <
-		(DAVINCI_EDMA_NUM_DMACH + DAVINCI_EDMA_NUM_QDMACH))) {
-		/* reset TCCHEN */
-		ptr_edmacc_regs->paramentry[lch_head].opt &= ~TCCHEN;
-	}
+    DMA_FN_IN;
+
+    if ((lch_head < edma_max_logical_ch)
+        || (lch_queue < edma_max_logical_ch))    {
+        unsigned int param_id = 0;
+
+        param_id = dma_ch_bound_res[lch_head].param_id;
+
+        /* reset TCCHEN */
+        ptr_edmacc_regs->paramentry[param_id].opt &= ~TCCHEN;
+        /* reset ITCCHEN */
+        ptr_edmacc_regs->paramentry[param_id].opt &= ~ITCCHEN;
+    }
+
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_dma_unchain_lch);
+
+
 
 /******************************************************************************
  *
@@ -1614,123 +2446,270 @@ void davinci_dma_unchain_lch(int lch_hea
 
 void davinci_clean_channel(int ch_no)
 {
-	int i;
-	dev_dbg(&edma_dev.dev, "EMR =%d\r\n", ptr_edmacc_regs->emr);
-	if (ch_no < 32) {
-		for (i = 0; i < 32; i++) {
-			if (ch_no == i) {
-				ptr_edmacc_regs->shadow[0].ecr |= (1 << i);
-				/* Clear the corresponding EMR bits */
-				ptr_edmacc_regs->emcr |= (1 << i);
-				/* Clear any SER */
-				ptr_edmacc_regs->shadow[0].secr |= (1 << i);
-				ptr_edmacc_regs->ccerrclr |= ((1 << 16) | 0x3);
-			}
-		}
-	}
+    unsigned int count;
+    unsigned int value = 0;
 
-	if (ch_no > 32) {
-		dev_dbg(&edma_dev.dev, "EMRH =%d\r\n", ptr_edmacc_regs->emrh);
-		for (i = 0; i < 32; i++) {
-			if (ch_no == (i + 32)) {
-				ptr_edmacc_regs->shadow[0].ecrh |= (1 << i);
-				/* Clear the corresponding IPR bits */
-				ptr_edmacc_regs->emcrh |= (1 << i);
-				/* Clear any SER */
-				ptr_edmacc_regs->shadow[0].secrh |= (1 << i);
-				ptr_edmacc_regs->ccerrclr |= ((1 << 16) | 0x3);
-			}
-		}
-	}
+    DMA_FN_IN;
+
+    if (ch_no < 32) {
+        DMA_PRINTK ("EMR = %x\r\n", ptr_edmacc_regs->emr);
+
+        ptr_edmacc_shadow_regs->ecr = (1 << ch_no);
+
+        /* Clear the corresponding EMR bits */
+        ptr_edmacc_regs->emcr = (1 << ch_no);
+
+        /* Clear any SER */
+        ptr_edmacc_shadow_regs->secr = (1 << ch_no);
+
+        /* Clear any EER */
+        ptr_edmacc_shadow_regs->eecr = (1 << ch_no);
+    }
+
+    if (ch_no > 32) {
+        DMA_PRINTK ("EMRH = %x\r\n", ptr_edmacc_regs->emrh);
+
+        ptr_edmacc_shadow_regs->ecrh = (1 << (ch_no-32));
+
+        /* Clear the corresponding EMRH bits */
+        ptr_edmacc_regs->emcrh = (1 << (ch_no-32));
+
+        /* Clear any SER */
+        ptr_edmacc_shadow_regs->secrh = (1 << (ch_no-32));
+
+        /* Clear any EERH */
+        ptr_edmacc_shadow_regs->eecrh = (1 << (ch_no-32));
+    }
+
+    for (count = 0; count < davinci_edma_num_evtq; count++) {
+	value |= (1u << count);
+    }
+
+    ptr_edmacc_regs->ccerrclr = ((1 << 16) | value);
+
+    DMA_FN_OUT;
 }
+EXPORT_SYMBOL(davinci_clean_channel);
+
+
 
 /******************************************************************************
  *
- * DMA interrupt handlers
+ * EDMA3 Initialisation on DaVinci
  *
  *****************************************************************************/
-static int dma_irq_handler_l(int sound_curr_lch, void
-			     *ch_status, struct
-			     pt_regs
-			     *data)
+int __init arch_dma_init(void)
 {
-	dev_dbg(&edma_dev.dev, "dma_irq_handler\n");
-	(*cb[0]) ();
-	return IRQ_HANDLED;
-}
+    struct edma_map *q_pri, *q_wm, *q_tc;
+    unsigned int i = 0u;
 
-static int
-    dma_ccerr_handler_l
-    (int sound_curr_lch, void *ch_status, struct pt_regs *data) {
-	dev_dbg(&edma_dev.dev, "dma_ccerr_handler\n");
-	(*cb[1]) ();
-	return IRQ_HANDLED;
-}
+    ptr_edmacc_regs = (edmacc_regs *) IO_ADDRESS(EDMA_CC_BASE_ADDRESS);
+    DMA_PRINTK ("DMA CC REG BASE ADDR=%x\n", (unsigned int)ptr_edmacc_regs);
 
-static int
-    dma_tc1err_handler_l
-    (int sound_curr_lch, void *ch_status, struct pt_regs *data) {
-	dev_dbg(&edma_dev.dev, "dma_tc1err_handler\n");
-	(*cb[2]) ();
-	return IRQ_HANDLED;
-}
+    if(cpu_is_davinci_dm6467()) {
+	davinci_edma_num_evtq = EDMA_DM646X_NUM_EVQUE;
+	davinci_edma_chmap_exist = EDMA_DM646X_CHMAP_EXIST;
+	davinci_edma_num_tc = EDMA_DM646X_NUM_TC;
+	davinci_edmatc_base_addrs = dm646x_edmatc_base_addrs;
+	edma_max_logical_ch = EDMA_NUM_QDMACH + EDMA_DM646X_NUM_PARAMENTRY;
+	davinci_edma_num_param = EDMA_DM646X_NUM_PARAMENTRY;
+	davinci_dma_ch_hw_event_map = dm646x_dma_ch_hw_event_map;
+
+	edma_channels_arm = dm646x_edma_channels_arm;
+	qdma_channels_arm = dm646x_qdma_channels_arm;
+	param_entry_arm = dm646x_param_entry_arm;
+	tcc_arm = dm646x_tcc_arm;
+	param_entry_reserved = dm646x_param_entry_reserved;
+
+	q_pri = dm646x_queue_priority_mapping;
+	q_tc = dm646x_queue_tc_mapping;
+	q_wm = dm646x_queue_watermark_level;
+
+    } else if (cpu_is_davinci_dm355()) {
+	davinci_edma_num_evtq = EDMA_DM355_NUM_EVQUE;
+	davinci_edma_chmap_exist = EDMA_DM355_CHMAP_EXIST;
+	davinci_edma_num_tc = EDMA_DM355_NUM_TC;
+	davinci_edmatc_base_addrs = dm355_edmatc_base_addrs;
+	edma_max_logical_ch = EDMA_NUM_QDMACH + EDMA_DM355_NUM_PARAMENTRY;
+	davinci_edma_num_param = EDMA_DM355_NUM_PARAMENTRY;
+	davinci_dma_ch_hw_event_map = dm355_dma_ch_hw_event_map;
+
+	edma_channels_arm = dm355_edma_channels_arm;
+	qdma_channels_arm = dm355_qdma_channels_arm;
+	param_entry_arm = dm355_param_entry_arm;
+	tcc_arm = dm355_tcc_arm;
+	param_entry_reserved = dm355_param_entry_reserved;
+
+	q_pri = dm355_queue_priority_mapping;
+	q_tc = dm355_queue_tc_mapping;
+	q_wm = dm355_queue_watermark_level;
+    } else {
+	davinci_edma_num_evtq = EDMA_DM644X_NUM_EVQUE;
+	davinci_edma_chmap_exist = EDMA_DM644X_CHMAP_EXIST;
+	davinci_edma_num_tc = EDMA_DM644X_NUM_TC;
+	davinci_edmatc_base_addrs = dm644x_edmatc_base_addrs;
+	edma_max_logical_ch = EDMA_NUM_QDMACH + EDMA_DM644X_NUM_PARAMENTRY;
+	davinci_edma_num_param = EDMA_DM644X_NUM_PARAMENTRY;
+	davinci_dma_ch_hw_event_map = dm644x_dma_ch_hw_event_map;
+
+	edma_channels_arm = dm644x_edma_channels_arm;
+	qdma_channels_arm = dm644x_qdma_channels_arm;
+	param_entry_arm = dm644x_param_entry_arm;
+	tcc_arm = dm644x_tcc_arm;
+	param_entry_reserved = dm644x_param_entry_reserved;
+
+	q_pri = dm644x_queue_priority_mapping;
+	q_tc = dm644x_queue_tc_mapping;
+	q_wm = dm644x_queue_watermark_level;
+    }
+    dma_ch_bound_res =
+        kmalloc(sizeof(struct edma3_ch_bound_res) * edma_max_logical_ch,
+             GFP_KERNEL);
+
+    for (i = 0; i < davinci_edma_num_tc; i++) {
+        ptr_edmatc_regs[i] = (volatile edmatc_regs *)
+                IO_ADDRESS(davinci_edmatc_base_addrs[i]);
+        DMA_PRINTK ("DMA TC[%d] REG BASE ADDR=%x\n", i,
+                (unsigned int)ptr_edmatc_regs[i]);
+    }
+
+    /* Reset global data */
+    /* Reset the DCHMAP registers if they exist */
+    if (davinci_edma_chmap_exist == 1) {
+        memset((void *)&(ptr_edmacc_regs->dchmap[0u]), 0x00u,
+           sizeof(ptr_edmacc_regs->dchmap));
+    }
+
+    /* Reset book-keeping info */
+    memset(dma_ch_bound_res, 0x00u,  (sizeof(struct edma3_ch_bound_res) *
+		edma_max_logical_ch));
+    memset(dma_interrupt_param, 0x00u,  sizeof(dma_interrupt_param));
+    memset(edma_dma_ch_tcc_mapping, 0x00u,  sizeof(edma_dma_ch_tcc_mapping));
+    memset(edma_qdma_ch_tcc_mapping, 0x00u,  sizeof(edma_qdma_ch_tcc_mapping));
+
+    memset((void *)&(ptr_edmacc_regs->paramentry[0u]), 0x00u,
+           sizeof(ptr_edmacc_regs->paramentry));
+
+    /* Clear Error Registers */
+    ptr_edmacc_regs->emcr = 0xFFFFFFFFu;
+    ptr_edmacc_regs->emcrh = 0xFFFFFFFFu;
+    ptr_edmacc_regs->qemcr = 0xFFFFFFFFu;
+    ptr_edmacc_regs->ccerrclr = 0xFFFFFFFFu;
+
+    i = 0u;
+    while (i < davinci_edma_num_evtq) {
+        /* Event Queue to TC mapping, if it exists */
+        if (EDMA_EVENT_QUEUE_TC_MAPPING == 1u) {
+            ptr_edmacc_regs->quetcmap &= QUETCMAP_CLR_MASK(q_tc[i].param1);
+            ptr_edmacc_regs->quetcmap |= QUETCMAP_SET_MASK(q_tc[i].param1,
+                                                           q_tc[i].param2);
+        }
+
+        /* Event Queue Priority */
+        ptr_edmacc_regs->quepri &= QUEPRI_CLR_MASK(q_pri[i].param1);
+        ptr_edmacc_regs->quepri |= QUEPRI_SET_MASK(q_pri[i].param1,
+                                                   q_pri[i].param2);
+
+        /* Event Queue Watermark Level */
+        ptr_edmacc_regs->qwmthra &= QUEWMTHR_CLR_MASK(q_wm[i].param1);
+        ptr_edmacc_regs->qwmthra |= QUEWMTHR_SET_MASK(q_wm[i].param1,
+                                                      q_wm[i].param2);
+
+        i++;
+    }
+
+    /* Reset the Allocated TCCs Array first. */
+    allocated_tccs[0u] = 0x0u;
+    allocated_tccs[1u] = 0x0u;
+
+    /* Point to the Master Shadow Region for later use */
+    ptr_edmacc_shadow_regs = (edmacc_shadow_regs *)&(ptr_edmacc_regs->shadow
+                                        [EDMA_MASTER_SHADOW_REGION]);
+    DMA_PRINTK ("\n\nShadow REG BASE ADDR = %x\n\n",
+		(unsigned int)ptr_edmacc_shadow_regs);
+    if (!ptr_edmacc_shadow_regs) {
+        /* Bad address */
+        return -EFAULT;
+    }
+
+    /* Clear region specific Shadow Registers */
+    ptr_edmacc_shadow_regs->ecr = (edma_channels_arm[0] | tcc_arm[0]);
+    ptr_edmacc_shadow_regs->ecrh = (edma_channels_arm[1] | tcc_arm[1]);
+    ptr_edmacc_shadow_regs->eecr = (edma_channels_arm[0] | tcc_arm[0]);
+    ptr_edmacc_shadow_regs->eecrh = (edma_channels_arm[1] | tcc_arm[1]);
+    ptr_edmacc_shadow_regs->secr = (edma_channels_arm[0] | tcc_arm[0]);
+    ptr_edmacc_shadow_regs->secrh = (edma_channels_arm[1] | tcc_arm[1]);
+    ptr_edmacc_shadow_regs->iecr = (edma_channels_arm[0] | tcc_arm[0]);
+    ptr_edmacc_shadow_regs->iecrh = (edma_channels_arm[1] | tcc_arm[1]);
+    ptr_edmacc_shadow_regs->icr = (edma_channels_arm[0] | tcc_arm[0]);
+    ptr_edmacc_shadow_regs->icrh = (edma_channels_arm[1] | tcc_arm[1]);
+    ptr_edmacc_shadow_regs->qeecr = (qdma_channels_arm[0]);
+    ptr_edmacc_shadow_regs->qsecr = (qdma_channels_arm[0]);
+
+
+    /* Reset Region Access Enable Registers for the Master Shadow Region */
+    ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].drae = 0x0u;
+    ptr_edmacc_regs->dra[EDMA_MASTER_SHADOW_REGION].draeh = 0x0u;
+    ptr_edmacc_regs->qrae[EDMA_MASTER_SHADOW_REGION] = 0x0u;
+
+    if (register_dma_interrupts())
+        return -EINVAL;
+
+
+    spin_lock_init(&dma_chan_lock);
+
+    return 0;
+}
+
+
+/* Register different ISRs with the underlying OS */
+int register_dma_interrupts(void)
+{
+    int result = 0;
+    int i;
+    unsigned int *tc_error_int;
+
+    if (cpu_is_davinci_dm6467())
+        tc_error_int = dm646x_tc_error_int;
+    else if (cpu_is_davinci_dm355())
+        tc_error_int = dm355_tc_error_int;
+    else
+        tc_error_int = dm644x_tc_error_int;
+
+    result = request_irq (EDMA_XFER_COMPLETION_INT, dma_irq_handler, 0,
+                    "EDMA Completion", NULL);
+    if (result < 0) {
+        DMA_PRINTK ("request_irq failed for dma_irq_handler, error=%d\n",
+                                                        result);
+        return result;
+    }
+
+    result = request_irq (EDMA_CC_ERROR_INT, dma_ccerr_handler, 0,
+                    "EDMA CC Err", NULL);
+    if (result < 0) {
+        DMA_PRINTK ("request_irq failed for dma_ccerr_handler, error=%d\n",
+                                                        result);
+        return result;
+    }
+
+    for (i = 0; i < davinci_edma_num_tc; i++)      {
+            result = request_irq (tc_error_int[i], ptr_edmatc_isrs[i], 0,
+                            "EDMA TC Error", NULL);
+            if (result < 0) {
+                DMA_PRINTK ("request_irq failed for dma_tc%d err_handler\n", i);
+                DMA_PRINTK ("error = %d \n", result);
+                return result;
+            }
+    }
 
-static int
-    dma_tc2err_handler_l
-    (int sound_curr_lch, void *ch_status, struct pt_regs *data) {
-	dev_dbg(&edma_dev.dev, "dma_tc2err_handler\n");
-	(*cb[3]) ();
-	return IRQ_HANDLED;
+    return result;
 }
 
-int register_dma_interrupts
-    (intr_callback cb1,
-     intr_callback cb2, intr_callback cb3, intr_callback cb4) {
-	cb[0] = cb1;
-	cb[1] = cb2;
-	cb[2] = cb3;
-	cb[3] = cb4;
-	if (!cb1 || !cb2 || !cb3 || !cb4) {
-		dev_dbg(&edma_dev.dev, "NULL callback\n");
-		return -1;
-	}
 
-	if (request_irq(IRQ_CCINT0, dma_irq_handler_l, 0, "EDMA", NULL)) {
-		dev_dbg(&edma_dev.dev, "request_irq failed\n");
-		return -1;
-	}
-	if (request_irq
-	    (IRQ_CCERRINT, dma_ccerr_handler_l, 0, "EDMA CC Err", NULL)) {
-		dev_dbg(&edma_dev.dev, "request_irq failed\n");
-		return -1;
-	}
-	if (request_irq
-	    (IRQ_TCERRINT0, dma_tc1err_handler_l, 0, "EDMA TC1 Err", NULL)) {
-		dev_dbg(&edma_dev.dev, "request_irq failed\n");
-		return -1;
-	}
-	if (request_irq
-	    (IRQ_TCERRINT, dma_tc2err_handler_l, 0, "EDMA TC2 Err", NULL)) {
-		dev_dbg(&edma_dev.dev, "request_irq failed\n");
-		return -1;
-	}
-	return 0;
-}
 
 arch_initcall(arch_dma_init);
-EXPORT_SYMBOL(davinci_start_dma);
-EXPORT_SYMBOL(davinci_dma_link_lch);
-EXPORT_SYMBOL(davinci_set_dma_params);
-EXPORT_SYMBOL(davinci_get_dma_params);
-EXPORT_SYMBOL(davinci_set_dma_transfer_params);
-EXPORT_SYMBOL(davinci_set_dma_dest_index);
-EXPORT_SYMBOL(davinci_set_dma_src_index);
-EXPORT_SYMBOL(davinci_set_dma_dest_params);
-EXPORT_SYMBOL(davinci_set_dma_src_params);
-EXPORT_SYMBOL(davinci_request_dma);
-EXPORT_SYMBOL(davinci_stop_dma);
-EXPORT_SYMBOL(davinci_clean_channel);
-EXPORT_SYMBOL(davinci_free_dma);
-EXPORT_SYMBOL(davinci_dma_chain_lch);
-EXPORT_SYMBOL(davinci_dma_unchain_lch);
-EXPORT_SYMBOL(davinci_dma_unlink_lch);
+
+
+MODULE_AUTHOR("Texas Instruments");
+MODULE_LICENSE("GPL");
+
Index: linux-2.6.10/include/asm-arm/arch-davinci/edma.h
===================================================================
--- linux-2.6.10.orig/include/asm-arm/arch-davinci/edma.h
+++ linux-2.6.10/include/asm-arm/arch-davinci/edma.h
@@ -2,7 +2,7 @@
  *  linux/include/asm-arm/arch-davinci/edma.h
  *
  *  BRIEF MODULE DESCRIPTION
- *      TI DAVINCI dma definitions
+ *      TI EDMA3 definitions
  *
  *  Copyright (C) 2006 Texas Instruments.
  *
@@ -28,20 +28,20 @@
  *
  */
 /******************************************************************************
- * DMA driver for DaVinci
- * DMA driver for Davinci abstractes each ParamEntry as a Logical DMA channel
- * for the user.So on Davinci the user can request 128 DAM channels
+ * EDMA3 Driver
+ * EDMA3 Driver abstracts each ParamEntry as a Logical DMA channel
+ * for the user. So for eg on DM644x, the user can request 128 DMA channels
  *
- * Actual Physical DMA channels = 64 EDMA channels + 8 QDMA channels
+ * Actual Physical DMA channels (on DM644x) = 64 EDMA channels + 8 QDMA channels
  *
- * On davinci user can request for two kinds of Logical DMA channels
+ * User can request for two kinds of Logical DMA channels
  * DMA MasterChannel -> ParamEntry which is associated with a DMA channel.
- *                      On Davinci there are (64 + 8) MasterChanneles
+ *                      On DM644x, there are (64 + 8) MasterChanneles
  *                      MasterChannel can be triggered by an event or manually
  *
  * DMA SlaveChannel  -> ParamEntry which is not associated with DMA cahnnel but
  *                      which can be used to associate with MasterChannel.
- *                      On Davinci there are (128-(64 + 8)) SlaveChannels
+ *                      On DM644x, there are (128-(64 + 8)) SlaveChannels
  *                      SlaveChannel can only be triggered by a MasterChannel
  *
  */
@@ -49,8 +49,55 @@
 #ifndef EDMA_H_
 #define EDMA_H_
 
-/*Used by driver*/
 
+/* Generic defines for all the platforms */
+#define EDMA_NUM_DMACH				64
+#define EDMA_NUM_QDMACH				8
+#define EDMA_NUM_TCC				64
+#define EDMA_CC_BASE_ADDRESS			DAVINCI_DMA_3PCC_BASE
+#define EDMA_XFER_COMPLETION_INT		IRQ_CCINT0
+#define EDMA_CC_ERROR_INT			IRQ_CCERRINT
+#define EDMA_EVENT_QUEUE_TC_MAPPING		1
+#define EDMA_MASTER_SHADOW_REGION		0
+#define EDMA_NUM_DMA_CHAN_DWRDS			(EDMA_NUM_DMACH / 32)
+#define EDMA_NUM_QDMA_CHAN_DWRDS		1
+
+
+/* SoC specific EDMA3 hardware information, should be provided for a new SoC */
+/* DM644x specific EDMA3 information */
+#define EDMA_DM644X_NUM_PARAMENTRY		128
+#define EDMA_DM644X_NUM_EVQUE			2
+#define EDMA_DM644X_NUM_TC			2
+#define EDMA_DM644X_CHMAP_EXIST			0
+#define EDMA_DM644X_NUM_REGIONS			4
+#define EDMA_DM644X_CHANNEL_TO_EVENT_MAPPING_0	0x3DFF0FFCu
+#define EDMA_DM644X_CHANNEL_TO_EVENT_MAPPING_1	0x007F1FFFu
+/* end DM644x specific */
+
+/* DM646x specific EDMA3 information */
+#define EDMA_DM646X_NUM_PARAMENTRY		512
+#define EDMA_DM646X_NUM_EVQUE			4
+#define EDMA_DM646X_NUM_TC			4
+#define EDMA_DM646X_CHMAP_EXIST			1
+#define EDMA_DM646X_NUM_REGIONS			8
+#define EDMA_DM646X_CHANNEL_TO_EVENT_MAPPING_0	0x30FF1FF0u
+#define EDMA_DM646X_CHANNEL_TO_EVENT_MAPPING_1	0x003F07FFu
+/* end DM646X specific info */
+
+/* DM355 specific info */
+#define EDMA_DM355_NUM_PARAMENTRY		512
+#define EDMA_DM355_NUM_EVQUE			8
+#define EDMA_DM355_NUM_TC			2
+#define EDMA_DM355_CHMAP_EXIST			0
+#define EDMA_DM355_NUM_REGIONS			4
+#define EDMA_DM355_CHANNEL_TO_EVENT_MAPPING_0	0xFDFF0FFCu
+#define EDMA_DM355_CHANNEL_TO_EVENT_MAPPING_1	0x007F1FFFu
+
+/* end DM355 specific info */
+
+/**************************************************************************\
+* Register Overlay Structure for Channel Controller
+\**************************************************************************/
 /**************************************************************************\
 * Register Overlay Structure for DRA
 \**************************************************************************/
@@ -130,7 +177,9 @@ typedef struct {
 typedef struct {
 	unsigned int rev;
 	unsigned int cccfg;
-	unsigned char rsvd0[504];
+	unsigned char rsvd0[244];
+	unsigned int clkgdis;
+	unsigned int dchmap[64];
 	unsigned int qchmap[8];
 	unsigned char rsvd1[32];
 	unsigned int dmaqnum[8];
@@ -149,23 +198,26 @@ typedef struct {
 	unsigned int ccerrclr;
 	unsigned int eeval;
 	unsigned char rsvd4[28];
-	edmacc_dra_regs dra[4];
-	unsigned char rsvd5[32];
-	unsigned int qrae[4];
-	unsigned char rsvd6[112];
-	edmacc_que_evtentry_regs queevtentry[2][16];
-	unsigned char rsvd7[384];
-	unsigned int qstat[2];
-	unsigned char rsvd8[24];
+	edmacc_dra_regs dra[8];
+	unsigned int qrae[8];
+	unsigned char rsvd5[96];
+	edmacc_que_evtentry_regs queevtentry[8][16];
+	unsigned int qstat[8];
 	unsigned int qwmthra;
 	unsigned int qwmthrb;
-	unsigned char rsvd9[24];
+	unsigned char rsvd6[24];
 	unsigned int ccstat;
-	unsigned char rsvd10[188];
+	unsigned char rsvd7[188];
 	unsigned int aetctl;
 	unsigned int aetstat;
 	unsigned int aetcmd;
-	unsigned char rsvd11[2292];
+	unsigned char rsvd8[244];
+	unsigned int mpfar;
+	unsigned int mpfsr;
+	unsigned int mpfcr;
+	unsigned int mppag;
+	unsigned int mppa[8];
+	unsigned char rsvd9[2000];
 	unsigned int er;
 	unsigned int erh;
 	unsigned int ecr;
@@ -184,7 +236,7 @@ typedef struct {
 	unsigned int serh;
 	unsigned int secr;
 	unsigned int secrh;
-	unsigned char rsvd12[8];
+	unsigned char rsvd10[8];
 	unsigned int ier;
 	unsigned int ierh;
 	unsigned int iecr;
@@ -196,117 +248,302 @@ typedef struct {
 	unsigned int icr;
 	unsigned int icrh;
 	unsigned int ieval;
-	unsigned char rsvd13[4];
+	unsigned char rsvd11[4];
 	unsigned int qer;
 	unsigned int qeer;
 	unsigned int qeecr;
 	unsigned int qeesr;
 	unsigned int qser;
 	unsigned int qsecr;
-	unsigned char rsvd14[3944];
-	edmacc_shadow_regs shadow[4];
-	unsigned char rsvd15[6144];
-	edmacc_paramentry_regs paramentry[128];
+	unsigned char rsvd12[3944];
+	edmacc_shadow_regs shadow[8];
+	unsigned char rsvd13[4096];
+	edmacc_paramentry_regs paramentry[512];
 } edmacc_regs;
 
-#define CCINT0_INTERRUPT     16
-#define CCERRINT_INTERRUPT   17
-#define TCERRINT0_INTERRUPT   18
-#define TCERRINT1_INTERRUPT   19
-
-#define SAM (1)
-#define DAM (1<<1)
-#define SYNCDIM (1<<2)
-#define STATIC (1<<3)
-#define EDMA_FWID (0x7<<8)
-#define TCCMODE (0x1<<11)
-#define TCC (0x3f<<12)
-#define WIMODE (0x1<<19)
-#define TCINTEN (0x1<<20)
-#define ITCINTEN (0x1<<21)
-#define TCCHEN (0x1<<22)
-#define ITCCHEN (0x1<<23)
-#define SECURE (0x1<<30)
-#define PRIV (0x1<<31)
 
-#define TRWORD (0x7<<2)
-#define PAENTRY (0x1ff<<5)
-/*if changing the QDMA_TRWORD do appropriate change in davinci_start_dma */
-#define QDMA_TRWORD (7 & 0x7)
 
-/*Used by driver*/
+/**************************************************************************\
+* Register Overlay Structure for Transfer Controller
+\**************************************************************************/
+/**************************************************************************\
+* Register Overlay Structure for DFIREG
+\**************************************************************************/
+typedef struct  {
+	unsigned int dfopt;
+	unsigned int dfsrc;
+	unsigned int dfcnt;
+	unsigned int dfdst;
+	unsigned int dfbidx;
+	unsigned int dfmpprxy;
+	unsigned char rsvd0[40];
+} edmtc_dfiregregs;
 
-#define DAVINCI_EDMA_NUM_DMACH           64
-#define DAVINCI_EDMA_NUM_QDMACH           8
-#define DAVINCI_EDMA_NUM_PARAMENTRY     128
-#define DAVINCI_EDMA_NUM_EVQUE            2
-#define DAVINCI_EDMA_CHMAPEXIST           0
-#define DAVINCI_EDMA_NUM_REGIONS          4
-#define DAVINCI_EDMA_MEMPROTECT           0
-
-#define DAVINCI_NUM_UNUSEDCH             21
-
-#define TCC_ANY    -1
-
-#define DAVINCI_EDMA_PARAM_ANY            -2
-#define DAVINCI_DMA_CHANNEL_ANY           -1
-#define DAVINCI_DMA_MCBSP_TX              2
-#define DAVINCI_DMA_MCBSP_RX              3
-#define DAVINCI_DMA_VPSS_HIST             4
-#define DAVINCI_DMA_VPSS_H3A              5
-#define DAVINCI_DMA_VPSS_PRVU             6
-#define DAVINCI_DMA_VPSS_RSZ              7
-#define DAVINCI_DMA_IMCOP_IMXINT          8
-#define DAVINCI_DMA_IMCOP_VLCDINT         9
-#define DAVINCI_DMA_IMCO_PASQINT         10
-#define DAVINCI_DMA_IMCOP_DSQINT         11
-#define DAVINCI_DMA_SPI_SPIX             16
-#define DAVINCI_DMA_SPI_SPIR             17
-#define DAVINCI_DMA_UART0_URXEVT0        18
-#define DAVINCI_DMA_UART0_UTXEVT0        19
-#define DAVINCI_DMA_UART1_URXEVT1        20
-#define DAVINCI_DMA_UART1_UTXEVT1        21
-#define DAVINCI_DMA_UART2_URXEVT2        22
-#define DAVINCI_DMA_UART2_UTXEVT2        23
-#define DAVINCI_DMA_MEMSTK_MSEVT         24
-#define DAVINCI_DMA_MMCRXEVT             26
-#define DAVINCI_DMA_MMCTXEVT             27
-#define DAVINCI_DMA_I2C_ICREVT           28
-#define DAVINCI_DMA_I2C_ICXEVT           29
-#define DAVINCI_DMA_GPIO_GPINT0          32
-#define DAVINCI_DMA_GPIO_GPINT1          33
-#define DAVINCI_DMA_GPIO_GPINT2          34
-#define DAVINCI_DMA_GPIO_GPINT3          35
-#define DAVINCI_DMA_GPIO_GPINT4          36
-#define DAVINCI_DMA_GPIO_GPINT5          37
-#define DAVINCI_DMA_GPIO_GPINT6          38
-#define DAVINCI_DMA_GPIO_GPINT7          39
-#define DAVINCI_DMA_GPIO_GPBNKINT0       40
-#define DAVINCI_DMA_GPIO_GPBNKINT1       41
-#define DAVINCI_DMA_GPIO_GPBNKINT2       42
-#define DAVINCI_DMA_GPIO_GPBNKINT3       43
-#define DAVINCI_DMA_GPIO_GPBNKINT4       44
-#define DAVINCI_DMA_TIMER0_TINT0         48
-#define DAVINCI_DMA_TIMER1_TINT1         49
-#define DAVINCI_DMA_TIMER2_TINT2         50
-#define DAVINCI_DMA_TIMER3_TINT3         51
-#define DAVINCI_DMA_PWM0                 52
-#define DAVINCI_DMA_PWM1                 53
-#define DAVINCI_DMA_PWM2                 54
-#define DAVINCI_DMA_QDMA0                64
-#define DAVINCI_DMA_QDMA1                65
-#define DAVINCI_DMA_QDMA2                66
-#define DAVINCI_DMA_QDMA3                67
-#define DAVINCI_DMA_QDMA4                68
-#define DAVINCI_DMA_QDMA5                69
-#define DAVINCI_DMA_QDMA6                71
-#define DAVINCI_DMA_QDMA7                72
+
+/**************************************************************************\
+* Register Overlay Structure
+\**************************************************************************/
+typedef struct  {
+	unsigned int rev;
+	unsigned int tccfg;
+	unsigned char rsvd0[248];
+	unsigned int tcstat;
+	unsigned int intstat;
+	unsigned int inten;
+	unsigned int intclr;
+	unsigned int intcmd;
+	unsigned char rsvd1[12];
+	unsigned int errstat;
+	unsigned int erren;
+	unsigned int errclr;
+	unsigned int errdet;
+	unsigned int errcmd;
+	unsigned char rsvd2[12];
+	unsigned int rdrate;
+	unsigned char rsvd3[188];
+	unsigned int popt;
+	unsigned int psrc;
+	unsigned int pcnt;
+	unsigned int pdst;
+	unsigned int pbidx;
+	unsigned int pmpprxy;
+	unsigned char rsvd4[40];
+	unsigned int saopt;
+	unsigned int sasrc;
+	unsigned int sacnt;
+	unsigned int sadst;
+	unsigned int sabidx;
+	unsigned int sampprxy;
+	unsigned int sacntrld;
+	unsigned int sasrcbref;
+	unsigned int sadstbref;
+	unsigned char rsvd5[28];
+	unsigned int dfcntrld;
+	unsigned int dfsrcbref;
+	unsigned int dfdstbref;
+	unsigned char rsvd6[116];
+	edmtc_dfiregregs dfireg[4];
+} edmatc_regs;
+
+
+#define SAM					(1)
+#define DAM					(1<<1)
+#define SYNCDIM					(1<<2)
+#define STATIC					(1<<3)
+#define EDMA_FWID				(0x7<<8)
+#define TCCMODE					(0x1<<11)
+#define TCC					(0x3f<<12)
+#define WIMODE					(0x1<<19)
+#define TCINTEN					(0x1<<20)
+#define ITCINTEN				(0x1<<21)
+#define TCCHEN					(0x1<<22)
+#define ITCCHEN					(0x1<<23)
+#define SECURE					(0x1<<30)
+#define PRIV					(0x1<<31)
+#define CCRL_CCERR_TCCERR_SHIFT			(0x10u)
+
+/** DMAQNUM bits Clear */
+#define DMAQNUM_CLR_MASK(ch_num)		(~(0x7u<<(((ch_num)%8u)*4u)))
+/** DMAQNUM bits Set */
+#define DMAQNUM_SET_MASK(ch_num, que_num)	((0x7u & (que_num)) << \
+							(((ch_num)%8u)*4u))
+/** QDMAQNUM bits Clear */
+#define QDMAQNUM_CLR_MASK(ch_num)		(~(0x7u<<((ch_num)*4u)))
+/** QDMAQNUM bits Set */
+#define QDMAQNUM_SET_MASK(ch_num, que_num)	((0x7u & (que_num)) << \
+							((ch_num)*4u))
+
+#define TRWORD					(0x7<<2)
+#define PAENTRY					(0x1ff<<5)
+
+/*if changing the QDMA_TRWORD do appropriate change in davinci_start_dma */
+#define QDMA_DEF_TRIG_WORD			0x7u
+
+/* QUETCMAP bits Clear */
+#define QUETCMAP_CLR_MASK(que_num)		(~(0x7u << ((que_num) * 0x4u)))
+/* QUETCMAP bits Set */
+#define QUETCMAP_SET_MASK(que_num, tc_num)	((0x7u & (tc_num)) << \
+							((que_num) * 0x4u))
+/* QUEPRI bits Clear */
+#define QUEPRI_CLR_MASK(que_num)		(~(0x7u << ((que_num) * 0x4u)))
+/* QUEPRI bits Set */
+#define QUEPRI_SET_MASK(que_num, que_pri)	((0x7u & (que_pri)) << \
+							((que_num) * 0x4u))
+/* QUEWMTHR bits Clear */
+#define QUEWMTHR_CLR_MASK(que_num)		(~(0x1Fu << ((que_num) * 0x8u)))
+/* QUEWMTHR bits Set */
+#define QUEWMTHR_SET_MASK(que_num, que_thr)	((0x1Fu & (que_thr)) << \
+							((que_num) * 0x8u))
+
+
+/** DCHMAP-PaRAMEntry bitfield Clear */
+#define DMACH_PARAM_CLR_MASK			(~0x3FE0u)
+/** DCHMAP-PaRAMEntry bitfield Set */
+#define DMACH_PARAM_SET_MASK(param_id)		(((0x3FE0u >> 0x5u) & \
+							(param_id)) << 0x5u)
+
+/** QCHMAP-PaRAMEntry bitfield Clear */
+#define QDMACH_PARAM_CLR_MASK			(~0x3FE0u)
+/** QCHMAP-PaRAMEntry bitfield Set */
+#define QDMACH_PARAM_SET_MASK(param_id)		(((0x3FE0u >> 0x5u) & \
+							(param_id)) << 0x5u)
+/** QCHMAP-TrigWord bitfield Clear */
+#define QDMACH_TRWORD_CLR_MASK			(~0x1Cu)
+/** QCHMAP-TrigWord bitfield Set */
+#define QDMACH_TRWORD_SET_MASK(param_id)	(((0x1Cu >> 0x2u) & \
+							(param_id)) << 0x2u)
+
+
+/* Defines needed for TC error checking */
+#define EDMA_TC_ERRSTAT_BUSERR_SHIFT		(0x00000000u)
+#define EDMA_TC_ERRSTAT_TRERR_SHIFT		(0x00000002u)
+#define EDMA_TC_ERRSTAT_MMRAERR_SHIFT		(0x00000003u)
+
+/* Maximum number of TCs possible */
+#define EDMA_MAX_TC				(8u)
+/* Maximum number of PARAMs possible */
+#define EDMA_MAX_PARAM_SET			(512u)
+
+/* Defines for QDMA Channels */
+#define EDMA_MAX_CHANNEL			(7u)
+#define EDMA_QDMA_CHANNEL_0			davinci_get_qdma_channel(0)
+#define EDMA_QDMA_CHANNEL_1			davinci_get_qdma_channel(1)
+#define EDMA_QDMA_CHANNEL_2			davinci_get_qdma_channel(2)
+#define EDMA_QDMA_CHANNEL_3			davinci_get_qdma_channel(3)
+#define EDMA_QDMA_CHANNEL_4			davinci_get_qdma_channel(4)
+#define EDMA_QDMA_CHANNEL_5			davinci_get_qdma_channel(5)
+#define EDMA_QDMA_CHANNEL_6			davinci_get_qdma_channel(6)
+#define EDMA_QDMA_CHANNEL_7			davinci_get_qdma_channel(7)
+
+/* Used for any TCC (Interrupt Channel) */
+#define EDMA_TCC_ANY				1001
+/* Used for LINK Channels */
+#define DAVINCI_EDMA_PARAM_ANY			1002
+/* Used for any DMA Channel */
+#define EDMA_DMA_CHANNEL_ANY			1003
+/* Used for any QDMA Channel */
+#define EDMA_QDMA_CHANNEL_ANY			1004
+
+
+
+/* DaVinci specific EDMA3 Events Information */
+#define DAVINCI_DMA_MCBSP_TX			2
+#define DAVINCI_DMA_MCBSP_RX			3
+#define DAVINCI_DMA_VPSS_HIST			4
+#define DAVINCI_DMA_VPSS_H3A			5
+#define DAVINCI_DMA_VPSS_PRVU			6
+#define DAVINCI_DMA_VPSS_RSZ			7
+#define DAVINCI_DMA_IMCOP_IMXINT		8
+#define DAVINCI_DMA_IMCOP_VLCDINT		9
+#define DAVINCI_DMA_IMCO_PASQINT		10
+#define DAVINCI_DMA_IMCOP_DSQINT		11
+#define DAVINCI_DMA_SPI_SPIX			16
+#define DAVINCI_DMA_SPI_SPIR			17
+#define DAVINCI_DMA_UART0_URXEVT0		18
+#define DAVINCI_DMA_UART0_UTXEVT0		19
+#define DAVINCI_DMA_UART1_URXEVT1		20
+#define DAVINCI_DMA_UART1_UTXEVT1		21
+#define DAVINCI_DMA_UART2_URXEVT2		22
+#define DAVINCI_DMA_UART2_UTXEVT2		23
+#define DAVINCI_DMA_MEMSTK_MSEVT		24
+#define DAVINCI_DMA_MMCRXEVT			26
+#define DAVINCI_DMA_MMCTXEVT			27
+#define DAVINCI_DMA_I2C_ICREVT			28
+#define DAVINCI_DMA_I2C_ICXEVT			29
+#define DAVINCI_DMA_GPIO_GPINT0			32
+#define DAVINCI_DMA_GPIO_GPINT1			33
+#define DAVINCI_DMA_GPIO_GPINT2			34
+#define DAVINCI_DMA_GPIO_GPINT3			35
+#define DAVINCI_DMA_GPIO_GPINT4			36
+#define DAVINCI_DMA_GPIO_GPINT5			37
+#define DAVINCI_DMA_GPIO_GPINT6			38
+#define DAVINCI_DMA_GPIO_GPINT7			39
+#define DAVINCI_DMA_GPIO_GPBNKINT0		40
+#define DAVINCI_DMA_GPIO_GPBNKINT1		41
+#define DAVINCI_DMA_GPIO_GPBNKINT2		42
+#define DAVINCI_DMA_GPIO_GPBNKINT3		43
+#define DAVINCI_DMA_GPIO_GPBNKINT4		44
+#define DAVINCI_DMA_TIMER0_TINT0		48
+#define DAVINCI_DMA_TIMER1_TINT1		49
+#define DAVINCI_DMA_TIMER2_TINT2		50
+#define DAVINCI_DMA_TIMER3_TINT3		51
+#define DAVINCI_DMA_PWM0			52
+#define DAVINCI_DMA_PWM1			53
+#define DAVINCI_DMA_PWM2			54
+
+
+/* DaVinci-HD specific EDMA3 Events Information */
+#define DAVINCI_DM646X_DMA_MCASP0_AXEVTE0	4
+#define DAVINCI_DM646X_DMA_MCASP0_AXEVTO0	5
+#define DAVINCI_DM646X_DMA_MCASP0_AXEVT0	6
+#define DAVINCI_DM646X_DMA_MCASP0_AREVTE0	7
+#define DAVINCI_DM646X_DMA_MCASP0_AREVTO0	8
+#define DAVINCI_DM646X_DMA_MCASP0_AREVT0	9
+#define DAVINCI_DM646X_DMA_MCASP1_AXEVTE1	10
+#define DAVINCI_DM646X_DMA_MCASP1_AXEVTO1	11
+#define DAVINCI_DM646X_DMA_MCASP1_AXEVT1	12
+#define DAVINCI_DM646X_DMA_IMCOP1_CP_ECDCMP1	43
+#define DAVINCI_DM646X_DMA_IMCOP1_CP_MC1	44
+#define DAVINCI_DM646X_DMA_IMCOP1_CP_BS1	45
+#define DAVINCI_DM646X_DMA_IMCOP1_CP_CALC1	46
+#define DAVINCI_DM646X_DMA_IMCOP1_CP_LPF1	47
+#define DAVINCI_DM646X_DMA_IMCOP0_CP_ME0	57
+#define DAVINCI_DM646X_DMA_IMCOP0_CP_IPE0	58
+#define DAVINCI_DM646X_DMA_IMCOP0_CP_ECDCMP0	59
+#define DAVINCI_DM646X_DMA_IMCOP0_CP_MC0	60
+#define DAVINCI_DM646X_DMA_IMCOP0_CP_BS0	61
+#define DAVINCI_DM646X_DMA_IMCOP0_CP_CALC0	62
+#define DAVINCI_DM646X_DMA_IMCOP0_CP_LPF0	63
+
+/* DM355 specific EDMA3 Events Information */
+#define DM355_DMA_TIMER3_TINT6			0
+#define DM355_DMA_TIMER3_TINT7			1
+#define DM355_DMA_MCBSP0_TX			2
+#define DM355_DMA_MCBSP0_RX			3
+#define DM355_DMA_MCBSP1_TX			8
+#define DM355_DMA_TIMER2_TINT4			8
+#define DM355_DMA_MCBSP1_RX			9
+#define DM355_DMA_TIMER2_TINT5			9
+#define DM355_DMA_SPI2_SPI2XEVT			10
+#define DM355_DMA_SPI2_SPI2REVT			11
+#define DM355_DMA_IMCOP_IMXINT			12
+#define DM355_DMA_IMCOP_SEQINT			13
+#define DM355_DMA_SPI1_SPI1XEVT			14
+#define DM355_DMA_SPI1_SPI1REVT			15
+#define DM355_DMA_SPI0_SPIX0			16
+#define DM355_DMA_SPI0_SPIR0			17
+#define DM355_DMA_RTOINT			24
+#define DM355_DMA_GPIO_GPINT9			25
+#define DM355_DMA_MMC0RXEVT			26
+#define DM355_DMA_MEMSTK_MSEVT			26
+#define DM355_DMA_MMC0TXEVT			27
+#define DM355_DMA_MMC1RXEVT			30
+#define DM355_DMA_MMC1TXEVT			31
+#define DM355_DMA_GPIO_GPBNKINT5		45
+#define DM355_DMA_GPIO_GPBNKINT6		46
+#define DM355_DMA_GPIO_GPINT8			47
+#define DM355_DMA_TIMER0_TINT1			49
+#define DM355_DMA_TIMER1_TINT2			50
+#define DM355_DMA_TIMER1_TINT3			51
+#define DM355_DMA_PWM3				55
+#define DM355_DMA_IMCOP_VLCDINT			56
+#define DM355_DMA_IMCOP_BIMINT			57
+#define DM355_DMA_IMCOP_DCTINT			58
+#define DM355_DMA_IMCOP_QIQINT			59
+#define DM355_DMA_IMCOP_BPSINT			60
+#define DM355_DMA_IMCOP_VLCDERRINT		61
+#define DM355_DMA_IMCOP_RCNTINT			62
+#define DM355_DMA_IMCOP_COPCINT			63
 
 /*ch_status paramater of callback function possible values*/
-#define DMA_COMPLETE 1
-#define DMA_CC_ERROR 2
-#define DMA_TC1_ERROR 3
-#define DMA_TC2_ERROR 4
+#define DMA_COMPLETE				1
+#define DMA_EVT_MISS_ERROR			2
+#define QDMA_EVT_MISS_ERROR			3
+#define DMA_CC_ERROR				4
+#define DMA_TC0_ERROR				5
+#define DMA_TC1_ERROR				6
+#define DMA_TC2_ERROR				7
+#define DMA_TC3_ERROR				8
 
 enum address_mode {
 	INCR = 0,
@@ -325,7 +562,8 @@ enum fifo_width {
 enum dma_event_q {
 	EVENTQ_0 = 0,
 	EVENTQ_1 = 1,
-	EVENTQ_DEFAULT = -1
+	EVENTQ_2 = 2,
+	EVENTQ_3 = 3,
 };
 
 enum sync_dimension {
@@ -333,6 +571,25 @@ enum sync_dimension {
 	ABSYNC = 1
 };
 
+enum resource_type {
+	RES_DMA_CHANNEL = 0,
+	RES_QDMA_CHANNEL = 1,
+	RES_TCC = 2,
+	RES_PARAM_SET = 3
+};
+
+/******************************************************************************
+ *
+ * davinci_get_qdma_channel: Convert qdma channel to logical channel
+ * Arguments:
+ *      ch      - input qdma channel.
+ *
+ * Return: logical channel associated with qdma channel or logical channel
+ *      associated with qdam channel 0 for out of range channel input.
+ *
+ *****************************************************************************/
+int davinci_get_qdma_channel(int ch);
+
 /******************************************************************************
  * davinci_request_dma - request for the Davinci DMA channel
  *
@@ -341,10 +598,10 @@ enum sync_dimension {
  * EX: DAVINCI_DMA_MCBSP_TX - For requesting a DMA MasterChannel with MCBSP_TX
  *     event association
  *
- *     DAVINCI_DMA_ANY - For requesting a DMA Masterchannel which does not has
- *     event association
+ *     EDMA_DMA_CHANNEL_ANY - For requesting a DMA Master channel which does
+ *                              not has event association
  *
- *     DAVINCI_DMA_LINK - for requesting a DMA SlaveChannel
+ *     DAVINCI_EDMA_PARAM_ANY - for requesting a DMA Slave Channel
  *
  * dev_name   - name of the dma channel in human readable format
  * callback   - channel callback function (valied only if you are requesting
@@ -357,20 +614,26 @@ enum sync_dimension {
  * eventq_no  - Event Queue no to which the channel will be associated with
  *              (valied only if you are requesting for a DMA MasterChannel)
  *              Values : EVENTQ_0/EVENTQ_1 for event queue 0/1.
- *                       EVENTQ_DEFAULT for Default queue
  *
  * Return: zero on success,
  *         -EINVAL - if the requested channel is not supported on the ARM side events
- *         -EBUSY - if the requested channel is already in use
- *          EREQDMA - if failed to request the dma channel
  *
  *****************************************************************************/
 int davinci_request_dma(int dev_id,
 			const char *dev_name,
 			void (*callback) (int lch, unsigned short ch_status,
-					  void *data), void *data, int *lch,
-			int *tcc, enum dma_event_q
-    );
+				void *data), void *data, int *lch,
+			int *tcc, enum dma_event_q);
+
+/******************************************************************************
+ *
+ * Free DMA channel - Free the dma channel number passed
+ *
+ * ARGUMENTS:
+ * lch - dma channel number to get free
+ *
+ *****************************************************************************/
+void davinci_free_dma(int lch);
 
 /******************************************************************************
  * davinci_set_dma_src_params - DMA source parameters setup
@@ -542,14 +805,5 @@ void davinci_dma_chain_lch(int lch_head,
  *****************************************************************************/
 void davinci_dma_unchain_lch(int lch_head, int lch_queue);
 
-/******************************************************************************
- *
- * Free DMA channel - Free the dma channel number passed
- *
- * ARGUMENTS:
- * lch - dma channel number to get free
- *
- *****************************************************************************/
-void davinci_free_dma(int lch);
 
 #endif
Index: linux-2.6.10/mvl_patches/pro-1678.c
===================================================================
--- /dev/null
+++ linux-2.6.10/mvl_patches/pro-1678.c
@@ -0,0 +1,16 @@
+/*
+ * Author: MontaVista Software, Inc. <source@mvista.com>
+ *
+ * 2008 (c) MontaVista Software, Inc. This file is licensed under
+ * the terms of the GNU General Public License version 2. This program
+ * is licensed "as is" without any warranty of any kind, whether express
+ * or implied.
+ */
+#include <linux/init.h>
+#include <linux/mvl_patch.h>
+
+static __init int regpatch(void)
+{
+        return mvl_register_patch(1678);
+}
+module_init(regpatch);
EOF

    rv=0
    cat /tmp/mvl_patch_$$
    if [ "$?" != "0" ]; then
	# Patch had a hard error, return 2
	rv=2
    elif grep '^Hunk' ${TMPFILE}; then
	rv=1
    fi

    rm -f ${TMPFILE}
    return $rv
}

function options() {
    echo "Options are:"
    echo "  --force-unsupported - Force the patch to be applied even if the"
    echo "      patch is out of order or the current kernel is unsupported."
    echo "      Use of this option is strongly discouraged."
    echo "  --force-apply-fuzz - If the patch has fuzz, go ahead and apply"
    echo "      it anyway.  This can occur if the patch is applied to an"
    echo "      unsupported kernel or applied out of order or if you have"
    echo "      made your own modifications to the kernel.  Use with"
    echo "      caution."
    echo "  --remove - Remove the patch"
}


function checkpatchnum() {
    local level;

    if [ ! -e ${1} ]; then
	echo "${1} does not exist, make sure you are in the kernel" 1>&2
	echo "base directory" 1>&2
	exit 1;
    fi

    # Extract the current patch number from the lsp info file.
    level=`grep '#define LSP_.*PATCH_LEVEL' ${1} | sed 's/^.*\"\\(.*\\)\".*\$/\\1/'`
    if [ "a$level" = "a" ]; then
	echo "No patch level defined in ${1}, are you sure this is" 1>&2
	echo "a valid MVL kernel LSP?" 1>&2
	exit 1;
    fi

    expr $level + 0 >/dev/null 2>&1
    isnum=$?

    # Check if the kernel is supported
    if [ "$level" = "unsupported" ]; then
	echo "**Current kernel is unsupported by MontaVista due to patches"
	echo "  begin applied out of order."
	if [ $force_unsupported == 't' ]; then
	    echo "  Application is forced, applying patch anyway"
	    unsupported=t
	    fix_patch_level=f
	else
	    echo "  Patch application aborted.  Use --force-unsupported to"
	    echo "  force the patch to be applied, but the kernel will not"
	    echo "  be supported by MontaVista."
	    exit 1;
	fi

    # Check the patch number from the lspinfo file to make sure it is
    # a valid number
    elif [ $isnum = 2 ]; then
	echo "**Patch level from ${1} was not a valid number, " 1>&2
	echo "  are you sure this is a valid MVL kernel LSP?" 1>&2
	exit 1;

    # Check that this is the right patch number to be applied.
    elif [ `expr $level $3` ${4} ${2} ]; then
	echo "**Application of this patch is out of order and will cause the"
	echo "  kernel to be unsupported by MontaVista."
	if [ $force_unsupported == 't' ]; then
	    echo "  application is forced, applying patch anyway"
	    unsupported=t
	else
	    echo "  Patch application aborted.  Please get all the patches in"
	    echo "  proper order from MontaVista Zone and apply them in order"
	    echo "  If you really want to apply this patch, use"
	    echo "  --force-unsupported to force the patch to be applied, but"
	    echo "  the kernel will not be supported by MontaVista."
	    exit 1;
	fi
    fi
}

#
# Update the patch level in the file.  Note that we use patch to do
# this.  Certain weak version control systems don't take kindly to
# arbitrary changes directly to files, but do have a special version
# of "patch" that understands this.
#
function setpatchnum() {
    sed "s/^#define LSP_\(.*\)PATCH_LEVEL[ \t*]\"[0-9]*\".*$/#define LSP_\1PATCH_LEVEL \"${2}\"/" <${1} >/tmp/$$.tmp1
    diff -u ${1} /tmp/$$.tmp1 >/tmp/$$.tmp2
    rm /tmp/$$.tmp1
    sed "s/^+++ \/tmp\/$$.tmp1/+++ include\/linux\/lsppatchlevel.h/" </tmp/$$.tmp2 >/tmp/$$.tmp1
    rm /tmp/$$.tmp2
    patch -p0 </tmp/$$.tmp1
    rm /tmp/$$.tmp1
}

force_unsupported=f
force_apply_fuzz=""
unsupported=f
fix_patch_level=t
reverse=f
common_patchnum_diff='+ 1'
common_patchnum=$PATCHNUM
patch_extraopts=''

# Extract command line parameters.
while [ $# -gt 0 ]; do
    if [ "a$1" == 'a--force-unsupported' ]; then
	force_unsupported=t
    elif [ "a$1" == 'a--force-apply-fuzz' ]; then
	force_apply_fuzz=y
    elif [ "a$1" == 'a--remove' ]; then
	reverse=t
	common_patchnum_diff=''
	common_patchnum=`expr $PATCHNUM - 1`
	patch_extraopts='--reverse'
    else
	echo "'$1' is an invalid command line parameter."
	options
	exit 1
    fi
    shift
done

echo "Checking patch level"
checkpatchnum ${LSPINFO} ${PATCHNUM} "${common_patchnum_diff}" "-ne"

if ! dopatch -p1 --dry-run --force $patch_extraopts; then
    if [ $? = 2 ]; then
	echo -n "**Patch had errors, application aborted" 1>&2
	exit 1;
    fi

    # Patch has warnings
    clean_apply=${force_apply_fuzz}
    while [ "a$clean_apply" != 'ay' -a "a$clean_apply" != 'an' ]; do
	echo -n "**Patch did not apply cleanly.  Do you still want to apply? (y/n) > "
	read clean_apply
	clean_apply=`echo "$clean_apply" | tr '[:upper:]' '[:lower:]'`
    done
    if [ $clean_apply = 'n' ]; then
	exit 1;
    fi
fi

dopatch -p1 --force $patch_extraopts

if [ $fix_patch_level = 't' ]; then 
    if [ $unsupported = 't' ]; then
	common_patchnum="unsupported"
    fi

    setpatchnum ${LSPINFO} ${common_patchnum}
fi

# Move the patch file into the mvl_patches directory if we are not reversing
if [ $reverse != 't' ]; then 
    if echo $0 | grep '/' >/dev/null; then
	# Filename is a path, either absolute or from the current directory.
	srcfile=$0
    else
	# Filename is from the path
	for i in `echo $PATH | tr ':;' '  '`; do
	    if [ -e ${i}/$0 ]; then
		srcfile=${i}/$0
	    fi
	done
    fi

    fname=`basename ${srcfile}`
    diff -uN mvl_patches/${fname} ${srcfile} | (cd mvl_patches; patch)
fi

