#! /usr/bin/env bash
# Patch: -common_arm_compat_semaphore
# Date: Wed Dec 28 15:00:36 2005
# Source: MontaVista Software, Inc.
# MR: 13523 
# Type: Defect Fix
# Disposition: merged from Real-Time Preempt
# Signed-off-by: Daniel Walker <dwalker@mvista.com>
# Signed-off-by: Tom Rini <trini@mvista.com>
# Description:
# 
# 	This is the arm specific changes needed to expose the system semaphore.
# 

PATCHNUM=182
LSPINFO=include/linux/lsppatchlevel.h
TMPFILE=/tmp/mvl_patch_$$

function dopatch() {
    patch $* >${TMPFILE} 2>&1 <<"EOF"
Source: MontaVista Software, Inc.
MR: 13523 
Type: Defect Fix
Disposition: merged from Real-Time Preempt
Signed-off-by: Daniel Walker <dwalker@mvista.com>
Signed-off-by: Tom Rini <trini@mvista.com>
Description:

	This is the arm specific changes needed to expose the system semaphore.

Index: linux-2.6.10/arch/arm/kernel/semaphore.c
===================================================================
--- linux-2.6.10.orig/arch/arm/kernel/semaphore.c
+++ linux-2.6.10/arch/arm/kernel/semaphore.c
@@ -48,57 +48,60 @@
  *    that we're on the wakeup list before we synchronize so that
  *    we cannot lose wakeup events.
  */
-
-void __up(struct semaphore *sem)
+fastcall void __compat_up(struct compat_semaphore *sem)
 {
 	wake_up(&sem->wait);
 }
 
-static DEFINE_RAW_SPINLOCK(semaphore_lock);
-
-void __sched __down(struct semaphore * sem)
+fastcall void __sched __compat_down(struct compat_semaphore * sem)
 {
 	struct task_struct *tsk = current;
 	DECLARE_WAITQUEUE(wait, tsk);
+	unsigned long flags;
+
 	tsk->state = TASK_UNINTERRUPTIBLE;
-	add_wait_queue_exclusive(&sem->wait, &wait);
+	spin_lock_irqsave(&sem->wait.lock, flags);
+	add_wait_queue_exclusive_locked(&sem->wait, &wait);
 
-	spin_lock_irq(&semaphore_lock);
 	sem->sleepers++;
 	for (;;) {
 		int sleepers = sem->sleepers;
 
 		/*
 		 * Add "everybody else" into it. They aren't
-		 * playing, because we own the spinlock.
+		 * playing, because we own the spinlock in
+		 * the wait_queue_head.
 		 */
 		if (!atomic_add_negative(sleepers - 1, &sem->count)) {
 			sem->sleepers = 0;
 			break;
 		}
 		sem->sleepers = 1;	/* us - see -1 above */
-		spin_unlock_irq(&semaphore_lock);
+		spin_unlock_irqrestore(&sem->wait.lock, flags);
 
 		schedule();
+
+		spin_lock_irqsave(&sem->wait.lock, flags);
 		tsk->state = TASK_UNINTERRUPTIBLE;
-		spin_lock_irq(&semaphore_lock);
 	}
-	spin_unlock_irq(&semaphore_lock);
-	remove_wait_queue(&sem->wait, &wait);
+	remove_wait_queue_locked(&sem->wait, &wait);
+	wake_up_locked(&sem->wait);
+	spin_unlock_irqrestore(&sem->wait.lock, flags);
 	tsk->state = TASK_RUNNING;
-	wake_up(&sem->wait);
 }
 
-int __sched __down_interruptible(struct semaphore * sem)
+fastcall int __sched __compat_down_interruptible(struct compat_semaphore * sem)
 {
 	int retval = 0;
 	struct task_struct *tsk = current;
 	DECLARE_WAITQUEUE(wait, tsk);
+	unsigned long flags;
+
 	tsk->state = TASK_INTERRUPTIBLE;
-	add_wait_queue_exclusive(&sem->wait, &wait);
+	spin_lock_irqsave(&sem->wait.lock, flags);
+	add_wait_queue_exclusive_locked(&sem->wait, &wait);
 
-	spin_lock_irq(&semaphore_lock);
-	sem->sleepers ++;
+	sem->sleepers++;
 	for (;;) {
 		int sleepers = sem->sleepers;
 
@@ -118,25 +121,27 @@ int __sched __down_interruptible(struct 
 
 		/*
 		 * Add "everybody else" into it. They aren't
-		 * playing, because we own the spinlock. The
-		 * "-1" is because we're still hoping to get
-		 * the lock.
+		 * playing, because we own the spinlock in
+		 * wait_queue_head. The "-1" is because we're
+		 * still hoping to get the semaphore.
 		 */
 		if (!atomic_add_negative(sleepers - 1, &sem->count)) {
 			sem->sleepers = 0;
 			break;
 		}
 		sem->sleepers = 1;	/* us - see -1 above */
-		spin_unlock_irq(&semaphore_lock);
+		spin_unlock_irqrestore(&sem->wait.lock, flags);
 
 		schedule();
+
+		spin_lock_irqsave(&sem->wait.lock, flags);
 		tsk->state = TASK_INTERRUPTIBLE;
-		spin_lock_irq(&semaphore_lock);
 	}
-	spin_unlock_irq(&semaphore_lock);
+	remove_wait_queue_locked(&sem->wait, &wait);
+	wake_up_locked(&sem->wait);
+	spin_unlock_irqrestore(&sem->wait.lock, flags);
+
 	tsk->state = TASK_RUNNING;
-	remove_wait_queue(&sem->wait, &wait);
-	wake_up(&sem->wait);
 	return retval;
 }
 
@@ -148,26 +153,34 @@ int __sched __down_interruptible(struct 
  * single "cmpxchg" without failure cases,
  * but then it wouldn't work on a 386.
  */
-int __down_trylock(struct semaphore * sem)
+fastcall int __compat_down_trylock(struct compat_semaphore * sem)
 {
 	int sleepers;
 	unsigned long flags;
 
-	spin_lock_irqsave(&semaphore_lock, flags);
+	spin_lock_irqsave(&sem->wait.lock, flags);
 	sleepers = sem->sleepers + 1;
 	sem->sleepers = 0;
 
 	/*
 	 * Add "everybody else" and us into it. They aren't
-	 * playing, because we own the spinlock.
+	 * playing, because we own the spinlock in the
+	 * wait_queue_head.
 	 */
-	if (!atomic_add_negative(sleepers, &sem->count))
-		wake_up(&sem->wait);
+	if (!atomic_add_negative(sleepers, &sem->count)) {
+		wake_up_locked(&sem->wait);
+	}
 
-	spin_unlock_irqrestore(&semaphore_lock, flags);
+	spin_unlock_irqrestore(&sem->wait.lock, flags);
 	return 1;
 }
 
+fastcall int compat_sem_is_locked(struct compat_semaphore *sem)
+{
+        return (int) atomic_read(&sem->count) < 0;
+}
+EXPORT_SYMBOL(compat_sem_is_locked);
+
 /*
  * The semaphore operations have a special calling sequence that
  * allow us to do a simpler in-line version of them. These routines
@@ -180,41 +193,41 @@ int __down_trylock(struct semaphore * se
  */
 asm("	.section .sched.text,\"ax\"		\n\
 	.align	5				\n\
-	.globl	__down_failed			\n\
-__down_failed:					\n\
+	.globl	__compat_down_failed		\n\
+__compat_down_failed:				\n\
 	stmfd	sp!, {r0 - r3, lr}		\n\
 	mov	r0, ip				\n\
-	bl	__down				\n\
+	bl	__compat_down			\n\
 	ldmfd	sp!, {r0 - r3, pc}		\n\
 						\n\
 	.align	5				\n\
-	.globl	__down_interruptible_failed	\n\
-__down_interruptible_failed:			\n\
+	.globl	__compat_down_interruptible_failed \n\
+__compat_down_interruptible_failed:			\n\
 	stmfd	sp!, {r0 - r3, lr}		\n\
 	mov	r0, ip				\n\
-	bl	__down_interruptible		\n\
+	bl	__compat_down_interruptible	\n\
 	mov	ip, r0				\n\
 	ldmfd	sp!, {r0 - r3, pc}		\n\
 						\n\
 	.align	5				\n\
-	.globl	__down_trylock_failed		\n\
-__down_trylock_failed:				\n\
+	.globl	__compat_down_trylock_failed	\n\
+__compat_down_trylock_failed:			\n\
 	stmfd	sp!, {r0 - r3, lr}		\n\
 	mov	r0, ip				\n\
-	bl	__down_trylock			\n\
+	bl	__compat_down_trylock		\n\
 	mov	ip, r0				\n\
 	ldmfd	sp!, {r0 - r3, pc}		\n\
 						\n\
 	.align	5				\n\
-	.globl	__up_wakeup			\n\
-__up_wakeup:					\n\
+	.globl	__compat_up_wakeup		\n\
+__compat_up_wakeup:				\n\
 	stmfd	sp!, {r0 - r3, lr}		\n\
 	mov	r0, ip				\n\
-	bl	__up				\n\
+	bl	__compat_up			\n\
 	ldmfd	sp!, {r0 - r3, pc}		\n\
 	");
 
-EXPORT_SYMBOL(__down_failed);
-EXPORT_SYMBOL(__down_interruptible_failed);
-EXPORT_SYMBOL(__down_trylock_failed);
-EXPORT_SYMBOL(__up_wakeup);
+EXPORT_SYMBOL(__compat_down_failed);
+EXPORT_SYMBOL(__compat_down_interruptible_failed);
+EXPORT_SYMBOL(__compat_down_trylock_failed);
+EXPORT_SYMBOL(__compat_up_wakeup);
Index: linux-2.6.10/include/asm-arm/locks.h
===================================================================
--- linux-2.6.10.orig/include/asm-arm/locks.h
+++ linux-2.6.10/include/asm-arm/locks.h
@@ -12,13 +12,12 @@
 #ifndef __ASM_PROC_LOCKS_H
 #define __ASM_PROC_LOCKS_H
 
-#ifndef CONFIG_PREEMPT_RT
 #if __LINUX_ARM_ARCH__ >= 6
 
-#define __down_op(ptr,fail)			\
+#define __compat_down_op(ptr,fail)		\
 	({					\
 	__asm__ __volatile__(			\
-	"@ down_op\n"				\
+	"@ compat_down_op\n"			\
 "1:	ldrex	lr, [%0]\n"			\
 "	sub	lr, lr, %1\n"			\
 "	strex	ip, lr, [%0]\n"			\
@@ -32,11 +31,11 @@
 	: "ip", "lr", "cc", "memory");		\
 	})
 
-#define __down_op_ret(ptr,fail)			\
+#define __compat_down_op_ret(ptr,fail)		\
 	({					\
 		unsigned int ret;		\
 	__asm__ __volatile__(			\
-	"@ down_op_ret\n"			\
+	"@ compat_down_op_ret\n"		\
 "1:	ldrex	lr, [%1]\n"			\
 "	sub	lr, lr, %2\n"			\
 "	strex	ip, lr, [%1]\n"			\
@@ -53,10 +52,10 @@
 	ret;					\
 	})
 
-#define __up_op(ptr,wake)			\
+#define __compat_up_op(ptr,wake)		\
 	({					\
 	__asm__ __volatile__(			\
-	"@ up_op\n"				\
+	"@ compat_up_op\n"				\
 "1:	ldrex	lr, [%0]\n"			\
 "	add	lr, lr, %1\n"			\
 "	strex	ip, lr, [%0]\n"			\
@@ -79,10 +78,10 @@
 #define RW_LOCK_BIAS      0x01000000
 #define RW_LOCK_BIAS_STR "0x01000000"
 
-#define __down_op_write(ptr,fail)		\
+#define __compat_down_op_write(ptr,fail)	\
 	({					\
 	__asm__ __volatile__(			\
-	"@ down_op_write\n"			\
+	"@ compat_down_op_write\n"		\
 "1:	ldrex	lr, [%0]\n"			\
 "	sub	lr, lr, %1\n"			\
 "	strex	ip, lr, [%0]\n"			\
@@ -96,10 +95,10 @@
 	: "ip", "lr", "cc", "memory");		\
 	})
 
-#define __up_op_write(ptr,wake)			\
+#define __comapat_up_op_write(ptr,wake)		\
 	({					\
 	__asm__ __volatile__(			\
-	"@ up_op_read\n"			\
+	"@ compat_up_op_read\n"			\
 "1:	ldrex	lr, [%0]\n"			\
 "	add	lr, lr, %1\n"			\
 "	strex	ip, lr, [%0]\n"			\
@@ -112,13 +111,13 @@
 	: "ip", "lr", "cc", "memory");		\
 	})
 
-#define __down_op_read(ptr,fail)		\
-	__down_op(ptr, fail)
+#define __compat_down_op_read(ptr,fail)		\
+	__compat_down_op(ptr, fail)
 
-#define __up_op_read(ptr,wake)			\
+#define __compat_up_op_read(ptr,wake)		\
 	({					\
 	__asm__ __volatile__(			\
-	"@ up_op_read\n"			\
+	"@ compat_up_op_read\n"			\
 "1:	ldrex	lr, [%0]\n"			\
 "	add	lr, lr, %1\n"			\
 "	strex	ip, lr, [%0]\n"			\
@@ -134,10 +133,10 @@
 
 #else
 
-#define __down_op(ptr,fail)			\
+#define __compat_down_op(ptr,fail)			\
 	({					\
 	__asm__ __volatile__(			\
-	"@ down_op\n"				\
+	"@ compat_down_op\n"				\
 "	mrs	ip, cpsr\n"			\
 "	orr	lr, ip, #128\n"			\
 "	msr	cpsr_c, lr\n"			\
@@ -152,11 +151,11 @@
 	: "ip", "lr", "cc", "memory");		\
 	})
 
-#define __down_op_ret(ptr,fail)			\
+#define __compat_down_op_ret(ptr,fail)		\
 	({					\
 		unsigned int ret;		\
 	__asm__ __volatile__(			\
-	"@ down_op_ret\n"			\
+	"@ compat_down_op_ret\n"		\
 "	mrs	ip, cpsr\n"			\
 "	orr	lr, ip, #128\n"			\
 "	msr	cpsr_c, lr\n"			\
@@ -174,10 +173,10 @@
 	ret;					\
 	})
 
-#define __up_op(ptr,wake)			\
+#define __compat_up_op(ptr,wake)		\
 	({					\
 	__asm__ __volatile__(			\
-	"@ up_op\n"				\
+	"@ compat_compat_up_op\n"		\
 "	mrs	ip, cpsr\n"			\
 "	orr	lr, ip, #128\n"			\
 "	msr	cpsr_c, lr\n"			\
@@ -201,10 +200,10 @@
 #define RW_LOCK_BIAS      0x01000000
 #define RW_LOCK_BIAS_STR "0x01000000"
 
-#define __down_op_write(ptr,fail)		\
+#define __compat_down_op_write(ptr,fail)	\
 	({					\
 	__asm__ __volatile__(			\
-	"@ down_op_write\n"			\
+	"@ compat_down_op_write\n"			\
 "	mrs	ip, cpsr\n"			\
 "	orr	lr, ip, #128\n"			\
 "	msr	cpsr_c, lr\n"			\
@@ -219,10 +218,10 @@
 	: "ip", "lr", "cc", "memory");		\
 	})
 
-#define __up_op_write(ptr,wake)			\
+#define __compat_up_op_write(ptr,wake)		\
 	({					\
 	__asm__ __volatile__(			\
-	"@ up_op_read\n"			\
+	"@ compat_up_op_read\n"			\
 "	mrs	ip, cpsr\n"			\
 "	orr	lr, ip, #128\n"			\
 "	msr	cpsr_c, lr\n"			\
@@ -237,13 +236,13 @@
 	: "ip", "lr", "cc", "memory");		\
 	})
 
-#define __down_op_read(ptr,fail)		\
-	__down_op(ptr, fail)
+#define __compat_down_op_read(ptr,fail)		\
+	__compat_down_op(ptr, fail)
 
-#define __up_op_read(ptr,wake)			\
+#define __compat_up_op_read(ptr,wake)			\
 	({					\
 	__asm__ __volatile__(			\
-	"@ up_op_read\n"			\
+	"@ compat_up_op_read\n"			\
 "	mrs	ip, cpsr\n"			\
 "	orr	lr, ip, #128\n"			\
 "	msr	cpsr_c, lr\n"			\
@@ -258,7 +257,6 @@
 	: "ip", "lr", "cc", "memory");		\
 	})
 
-#endif
 
 #endif
 #endif
Index: linux-2.6.10/include/asm-arm/semaphore.h
===================================================================
--- linux-2.6.10.orig/include/asm-arm/semaphore.h
+++ linux-2.6.10/include/asm-arm/semaphore.h
@@ -9,86 +9,90 @@
 #include <linux/wait.h>
 #include <linux/rwsem.h>
 
-#ifdef CONFIG_PREEMPT_RT
-# include <linux/rt_lock.h>
-#else
+/*
+ * On !PREEMPT_RT all semaphores are compat:
+ */
+#ifndef CONFIG_PREEMPT_RT
+# define compat_semaphore semaphore
+#endif
 
 #include <asm/atomic.h>
 #include <asm/locks.h>
 
-struct semaphore {
+struct compat_semaphore {
 	atomic_t count;
 	int sleepers;
 	wait_queue_head_t wait;
 };
 
-#define __SEMAPHORE_INIT(name, cnt)				\
+#define __COMPAT_SEMAPHORE_INIT(name, cnt)				\
 {								\
 	.count	= ATOMIC_INIT(cnt),				\
 	.wait	= __WAIT_QUEUE_HEAD_INITIALIZER((name).wait),	\
 }
 
-#define __MUTEX_INITIALIZER(name) __SEMAPHORE_INIT(name,1)
+#define __COMPAT_MUTEX_INITIALIZER(name) __COMPAT_SEMAPHORE_INIT(name,1)
 
-#define __DECLARE_SEMAPHORE_GENERIC(name,count)	\
-	struct semaphore name = __SEMAPHORE_INIT(name,count)
+#define __COMPAT_DECLARE_SEMAPHORE_GENERIC(name,count)	\
+	struct compat_semaphore name = __COMPAT_SEMAPHORE_INIT(name,count)
 
-#define DECLARE_MUTEX(name)		__DECLARE_SEMAPHORE_GENERIC(name,1)
-#define DECLARE_MUTEX_LOCKED(name)	__DECLARE_SEMAPHORE_GENERIC(name,0)
+#define COMPAT_DECLARE_MUTEX(name)	__COMPAT_DECLARE_SEMAPHORE_GENERIC(name,1)
+#define COMPAT_DECLARE_MUTEX_LOCKED(name) __COMPAT_DECLARE_SEMAPHORE_GENERIC(name,0)
 
-static inline void sema_init(struct semaphore *sem, int val)
+static inline void compat_sema_init(struct compat_semaphore *sem, int val)
 {
 	atomic_set(&sem->count, val);
 	sem->sleepers = 0;
 	init_waitqueue_head(&sem->wait);
 }
 
-static inline void init_MUTEX(struct semaphore *sem)
+static inline void compat_init_MUTEX(struct compat_semaphore *sem)
 {
-	sema_init(sem, 1);
+	compat_sema_init(sem, 1);
 }
 
-static inline void init_MUTEX_LOCKED(struct semaphore *sem)
+static inline void compat_init_MUTEX_LOCKED(struct compat_semaphore *sem)
 {
-	sema_init(sem, 0);
+	compat_sema_init(sem, 0);
 }
 
 /*
  * special register calling convention
  */
-asmlinkage void __down_failed(void);
-asmlinkage int  __down_interruptible_failed(void);
-asmlinkage int  __down_trylock_failed(void);
-asmlinkage void __up_wakeup(void);
-
-extern void __down(struct semaphore * sem);
-extern int  __down_interruptible(struct semaphore * sem);
-extern int  __down_trylock(struct semaphore * sem);
-extern void __up(struct semaphore * sem);
+asmlinkage void __compat_down_failed(void);
+asmlinkage int  __compat_down_interruptible_failed(void);
+asmlinkage int  __compat_down_trylock_failed(void);
+asmlinkage void __compat_up_wakeup(void);
+
+extern void __compat_down(struct compat_semaphore * sem);
+extern int  __compat_down_interruptible(struct compat_semaphore * sem);
+extern int  __compat_down_trylock(struct compat_semaphore * sem);
+extern void __compat_up(struct compat_semaphore * sem);
+extern int compat_sem_is_locked(struct compat_semaphore *sem);
 
 /*
  * This is ugly, but we want the default case to fall through.
  * "__down" is the actual routine that waits...
  */
-static inline void down(struct semaphore * sem)
+static inline void compat_down(struct compat_semaphore * sem)
 {
 	might_sleep();
-	__down_op(sem, __down_failed);
+	__compat_down_op(sem, __compat_down_failed);
 }
 
 /*
  * This is ugly, but we want the default case to fall through.
  * "__down_interruptible" is the actual routine that waits...
  */
-static inline int down_interruptible (struct semaphore * sem)
+static inline int compat_down_interruptible (struct compat_semaphore * sem)
 {
 	might_sleep();
-	return __down_op_ret(sem, __down_interruptible_failed);
+	return __compat_down_op_ret(sem, __compat_down_interruptible_failed);
 }
 
-static inline int down_trylock(struct semaphore *sem)
+static inline int compat_down_trylock(struct compat_semaphore *sem)
 {
-	return __down_op_ret(sem, __down_trylock_failed);
+	return __compat_down_op_ret(sem, __compat_down_trylock_failed);
 }
 
 /*
@@ -97,9 +101,13 @@ static inline int down_trylock(struct se
  * The default case (no contention) will result in NO
  * jumps for both down() and up().
  */
-static inline void up(struct semaphore * sem)
+static inline void compat_up(struct compat_semaphore * sem)
 {
-	__up_op(sem, __up_wakeup);
+	__compat_up_op(sem, __compat_up_wakeup);
 }
-#endif
+
+#define compat_sema_count(sem) atomic_read(&(sem)->count)
+
+#include <linux/semaphore.h>
+
 #endif
Index: linux-2.6.10/mvl_patches/pro-0182.c
===================================================================
--- /dev/null
+++ linux-2.6.10/mvl_patches/pro-0182.c
@@ -0,0 +1,16 @@
+/*
+ * Author: MontaVista Software, Inc. <source@mvista.com>
+ *
+ * 2005 (c) MontaVista Software, Inc. This file is licensed under
+ * the terms of the GNU General Public License version 2. This program
+ * is licensed "as is" without any warranty of any kind, whether express
+ * or implied.
+ */
+#include <linux/init.h>
+#include <linux/mvl_patch.h>
+
+static __init int regpatch(void)
+{
+        return mvl_register_patch(182);
+}
+module_init(regpatch);
EOF

    rv=0
    cat /tmp/mvl_patch_$$
    if [ "$?" != "0" ]; then
	# Patch had a hard error, return 2
	rv=2
    elif grep '^Hunk' ${TMPFILE}; then
	rv=1
    fi

    rm -f ${TMPFILE}
    return $rv
}

function options() {
    echo "Options are:"
    echo "  --force-unsupported - Force the patch to be applied even if the"
    echo "      patch is out of order or the current kernel is unsupported."
    echo "      Use of this option is strongly discouraged."
    echo "  --force-apply-fuzz - If the patch has fuzz, go ahead and apply"
    echo "      it anyway.  This can occur if the patch is applied to an"
    echo "      unsupported kernel or applied out of order or if you have"
    echo "      made your own modifications to the kernel.  Use with"
    echo "      caution."
    echo "  --remove - Remove the patch"
}


function checkpatchnum() {
    local level;

    if [ ! -e ${1} ]; then
	echo "${1} does not exist, make sure you are in the kernel" 1>&2
	echo "base directory" 1>&2
	exit 1;
    fi

    # Extract the current patch number from the lsp info file.
    level=`grep '#define LSP_.*PATCH_LEVEL' ${1} | sed 's/^.*\"\\(.*\\)\".*\$/\\1/'`
    if [ "a$level" = "a" ]; then
	echo "No patch level defined in ${1}, are you sure this is" 1>&2
	echo "a valid MVL kernel LSP?" 1>&2
	exit 1;
    fi

    expr $level + 0 >/dev/null 2>&1
    isnum=$?

    # Check if the kernel is supported
    if [ "$level" = "unsupported" ]; then
	echo "**Current kernel is unsupported by MontaVista due to patches"
	echo "  begin applied out of order."
	if [ $force_unsupported == 't' ]; then
	    echo "  Application is forced, applying patch anyway"
	    unsupported=t
	    fix_patch_level=f
	else
	    echo "  Patch application aborted.  Use --force-unsupported to"
	    echo "  force the patch to be applied, but the kernel will not"
	    echo "  be supported by MontaVista."
	    exit 1;
	fi

    # Check the patch number from the lspinfo file to make sure it is
    # a valid number
    elif [ $isnum = 2 ]; then
	echo "**Patch level from ${1} was not a valid number, " 1>&2
	echo "  are you sure this is a valid MVL kernel LSP?" 1>&2
	exit 1;

    # Check that this is the right patch number to be applied.
    elif [ `expr $level $3` ${4} ${2} ]; then
	echo "**Application of this patch is out of order and will cause the"
	echo "  kernel to be unsupported by MontaVista."
	if [ $force_unsupported == 't' ]; then
	    echo "  application is forced, applying patch anyway"
	    unsupported=t
	else
	    echo "  Patch application aborted.  Please get all the patches in"
	    echo "  proper order from MontaVista Zone and apply them in order"
	    echo "  If you really want to apply this patch, use"
	    echo "  --force-unsupported to force the patch to be applied, but"
	    echo "  the kernel will not be supported by MontaVista."
	    exit 1;
	fi
    fi
}

#
# Update the patch level in the file.  Note that we use patch to do
# this.  Certain weak version control systems don't take kindly to
# arbitrary changes directly to files, but do have a special version
# of "patch" that understands this.
#
function setpatchnum() {
    sed "s/^#define LSP_\(.*\)PATCH_LEVEL[ \t*]\"[0-9]*\".*$/#define LSP_\1PATCH_LEVEL \"${2}\"/" <${1} >/tmp/$$.tmp1
    diff -u ${1} /tmp/$$.tmp1 >/tmp/$$.tmp2
    rm /tmp/$$.tmp1
    sed "s/^+++ \/tmp\/$$.tmp1/+++ include\/linux\/lsppatchlevel.h/" </tmp/$$.tmp2 >/tmp/$$.tmp1
    rm /tmp/$$.tmp2
    patch -p0 </tmp/$$.tmp1
    rm /tmp/$$.tmp1
}

force_unsupported=f
force_apply_fuzz=""
unsupported=f
fix_patch_level=t
reverse=f
common_patchnum_diff='+ 1'
common_patchnum=$PATCHNUM
patch_extraopts=''

# Extract command line parameters.
while [ $# -gt 0 ]; do
    if [ "a$1" == 'a--force-unsupported' ]; then
	force_unsupported=t
    elif [ "a$1" == 'a--force-apply-fuzz' ]; then
	force_apply_fuzz=y
    elif [ "a$1" == 'a--remove' ]; then
	reverse=t
	common_patchnum_diff=''
	common_patchnum=`expr $PATCHNUM - 1`
	patch_extraopts='--reverse'
    else
	echo "'$1' is an invalid command line parameter."
	options
	exit 1
    fi
    shift
done

echo "Checking patch level"
checkpatchnum ${LSPINFO} ${PATCHNUM} "${common_patchnum_diff}" "-ne"

if ! dopatch -p1 --dry-run --force $patch_extraopts; then
    if [ $? = 2 ]; then
	echo -n "**Patch had errors, application aborted" 1>&2
	exit 1;
    fi

    # Patch has warnings
    clean_apply=${force_apply_fuzz}
    while [ "a$clean_apply" != 'ay' -a "a$clean_apply" != 'an' ]; do
	echo -n "**Patch did not apply cleanly.  Do you still want to apply? (y/n) > "
	read clean_apply
	clean_apply=`echo "$clean_apply" | tr '[:upper:]' '[:lower:]'`
    done
    if [ $clean_apply = 'n' ]; then
	exit 1;
    fi
fi

dopatch -p1 --force $patch_extraopts

if [ $fix_patch_level = 't' ]; then 
    if [ $unsupported = 't' ]; then
	common_patchnum="unsupported"
    fi

    setpatchnum ${LSPINFO} ${common_patchnum}
fi

# Move the patch file into the mvl_patches directory if we are not reversing
if [ $reverse != 't' ]; then 
    if echo $0 | grep '/' >/dev/null; then
	# Filename is a path, either absolute or from the current directory.
	srcfile=$0
    else
	# Filename is from the path
	for i in `echo $PATH | tr ':;' '  '`; do
	    if [ -e ${i}/$0 ]; then
		srcfile=${i}/$0
	    fi
	done
    fi

    fname=`basename ${srcfile}`
    diff -uN mvl_patches/${fname} ${srcfile} | (cd mvl_patches; patch)
fi

